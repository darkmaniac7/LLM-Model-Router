/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 10-14 00:35:41 [__init__.py:216] Automatically detected platform cuda.
[2025-10-14 00:35:42] server_args=ServerArgs(model_path='/home/ivan/models/KAT-Dev-AWQ-8bit', tokenizer_path='/home/ivan/models/KAT-Dev-AWQ-8bit', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, context_length=32768, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=192449356, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, crash_on_nan=False, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='kat-dev-awq-8bit', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.2, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-14 00:35:43] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 10-14 00:35:52 [__init__.py:216] Automatically detected platform cuda.
INFO 10-14 00:35:53 [__init__.py:216] Automatically detected platform cuda.
INFO 10-14 00:35:53 [__init__.py:216] Automatically detected platform cuda.
INFO 10-14 00:35:53 [__init__.py:216] Automatically detected platform cuda.
INFO 10-14 00:35:54 [__init__.py:216] Automatically detected platform cuda.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-14 00:35:54 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-14 00:35:54 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-14 00:35:55 TP0] sglang is using nccl==2.27.3
[2025-10-14 00:35:55 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-14 00:35:55 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-14 00:35:55 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-14 00:35:55 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-14 00:35:55 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-14 00:35:55 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-14 00:35:55 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-14 00:35:55 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-14 00:35:55 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-14 00:35:56 TP0] Load weight begin. avail mem=23.11 GB
INFO 10-14 00:35:56 [compressed_tensors_wNa16.py:95] Using MarlinLinearKernel for CompressedTensorsWNA16
INFO 10-14 00:35:56 [compressed_tensors_wNa16.py:95] Using MarlinLinearKernel for CompressedTensorsWNA16
INFO 10-14 00:35:56 [compressed_tensors_wNa16.py:95] Using MarlinLinearKernel for CompressedTensorsWNA16
INFO 10-14 00:35:56 [compressed_tensors_wNa16.py:95] Using MarlinLinearKernel for CompressedTensorsWNA16
Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:01,  3.76it/s]
Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:00<00:01,  3.22it/s]
Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:00<00:01,  4.41it/s]
Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:01<00:01,  3.81it/s]
Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:01<00:00,  3.40it/s]
Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:01<00:00,  3.22it/s]
Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:02<00:00,  3.11it/s]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:02<00:00,  3.05it/s]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:02<00:00,  3.29it/s]

[2025-10-14 00:35:59 TP0] Load weight end. type=Qwen3ForCausalLM, dtype=torch.bfloat16, avail mem=14.28 GB, mem usage=8.83 GB.
[2025-10-14 00:35:59 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-14 00:35:59 TP0] KV Cache is allocated. #tokens: 158234, K size: 4.83 GB, V size: 4.83 GB
[2025-10-14 00:35:59 TP1] KV Cache is allocated. #tokens: 158234, K size: 4.83 GB, V size: 4.83 GB
[2025-10-14 00:35:59 TP0] Memory pool end. avail mem=4.23 GB
[2025-10-14 00:35:59 TP2] KV Cache is allocated. #tokens: 158234, K size: 4.83 GB, V size: 4.83 GB
[2025-10-14 00:35:59 TP3] KV Cache is allocated. #tokens: 158234, K size: 4.83 GB, V size: 4.83 GB
[2025-10-14 00:36:00 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-14 00:36:00 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.58 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.58 GB):   7%|▋         | 1/14 [00:02<00:31,  2.41s/it]Capturing batches (bs=72 avail_mem=3.45 GB):   7%|▋         | 1/14 [00:02<00:31,  2.41s/it]Capturing batches (bs=72 avail_mem=3.45 GB):  14%|█▍        | 2/14 [00:02<00:13,  1.15s/it]Capturing batches (bs=64 avail_mem=3.44 GB):  14%|█▍        | 2/14 [00:02<00:13,  1.15s/it]Capturing batches (bs=64 avail_mem=3.44 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.43it/s]Capturing batches (bs=56 avail_mem=3.43 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.43it/s]Capturing batches (bs=56 avail_mem=3.43 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.06it/s]Capturing batches (bs=48 avail_mem=3.40 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.06it/s]Capturing batches (bs=48 avail_mem=3.40 GB):  36%|███▌      | 5/14 [00:03<00:03,  2.74it/s]Capturing batches (bs=40 avail_mem=3.39 GB):  36%|███▌      | 5/14 [00:03<00:03,  2.74it/s]Capturing batches (bs=40 avail_mem=3.39 GB):  43%|████▎     | 6/14 [00:03<00:02,  3.42it/s]Capturing batches (bs=32 avail_mem=3.37 GB):  43%|████▎     | 6/14 [00:03<00:02,  3.42it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
[rank2]:E1014 00:37:05.992000 3547235 torch/_inductor/select_algorithm.py:2555] [19/0] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/m4/cm4v2cjxdqqsxoughm6bjfmtwtiiqxx2sx66jfgepbga2jtbhm67.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank0]:E1014 00:37:06.257000 3547233 torch/_inductor/select_algorithm.py:2555] [19/0] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/o3/co37weiwo26z22dtudj3maz5mprwdbhkhj65qjcdcojdo4edqocn.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank1]:E1014 00:37:06.565000 3547234 torch/_inductor/select_algorithm.py:2555] [19/0] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/2n/c2nvltwyjeqpzccs7loaqxeaux6nk6oix5hrrzuwjdncox4lsaix.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank3]:E1014 00:37:06.737000 3547236 torch/_inductor/select_algorithm.py:2555] [19/0] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/5n/c5nzcg3rzkowjteh47a33dn2gw2blnzmcpxrr2physj4vzvke2ns.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank2]:E1014 00:37:07.065000 3547235 torch/_inductor/select_algorithm.py:2691] [19/0] Runtime error during autotuning: 
[rank2]:E1014 00:37:07.065000 3547235 torch/_inductor/select_algorithm.py:2691] [19/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1014 00:37:07.065000 3547235 torch/_inductor/select_algorithm.py:2691] [19/0] Ignoring this choice.
AUTOTUNE mm(32x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.4588 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_4 0.4628 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_11 0.4628 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_1 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_16 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_8 0.4762 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.4820 ms 95.2% 
  triton_mm_3 0.7916 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_9 0.8131 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_14 0.8161 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5534 seconds and 3.2794 seconds precompiling for 18 choices
[rank0]:E1014 00:37:07.403000 3547233 torch/_inductor/select_algorithm.py:2691] [19/0] Runtime error during autotuning: 
[rank0]:E1014 00:37:07.403000 3547233 torch/_inductor/select_algorithm.py:2691] [19/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1014 00:37:07.403000 3547233 torch/_inductor/select_algorithm.py:2691] [19/0] Ignoring this choice.
AUTOTUNE mm(32x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.4596 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_11 0.4639 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_16 0.4649 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_4 0.4659 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_1 0.4700 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_8 0.4721 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.4741 ms 96.9% 
  triton_mm_3 0.7905 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_14 0.8151 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_9 0.8161 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5457 seconds and 3.5384 seconds precompiling for 18 choices
[rank1]:E1014 00:37:07.647000 3547234 torch/_inductor/select_algorithm.py:2691] [19/0] Runtime error during autotuning: 
[rank1]:E1014 00:37:07.647000 3547234 torch/_inductor/select_algorithm.py:2691] [19/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1014 00:37:07.647000 3547234 torch/_inductor/select_algorithm.py:2691] [19/0] Ignoring this choice.
AUTOTUNE mm(32x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.4589 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_11 0.4627 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_4 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_16 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_1 0.4659 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_8 0.4700 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.4731 ms 97.0% 
  triton_mm_3 0.7926 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_9 0.8131 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_14 0.8202 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5430 seconds and 3.3237 seconds precompiling for 18 choices
[rank3]:E1014 00:37:07.830000 3547236 torch/_inductor/select_algorithm.py:2691] [19/0] Runtime error during autotuning: 
[rank3]:E1014 00:37:07.830000 3547236 torch/_inductor/select_algorithm.py:2691] [19/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1014 00:37:07.830000 3547236 torch/_inductor/select_algorithm.py:2691] [19/0] Ignoring this choice.
AUTOTUNE mm(32x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.4597 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_4 0.4639 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_11 0.4639 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_16 0.4649 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_8 0.4721 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.4813 ms 95.5% 
  triton_mm_1 0.4854 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_3 0.7916 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_6 0.8141 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_9 0.8141 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5454 seconds and 3.4468 seconds precompiling for 18 choices
Capturing batches (bs=32 avail_mem=3.37 GB):  50%|█████     | 7/14 [01:08<02:31, 21.63s/it]Capturing batches (bs=24 avail_mem=3.35 GB):  50%|█████     | 7/14 [01:08<02:31, 21.63s/it][rank1]:E1014 00:37:40.392000 3547234 torch/_inductor/select_algorithm.py:2555] [19/1] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/po/cpo52fpcoapxztuhfz7upxb7exvjpw5j5ke26bzfrv44s5wydyzt.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank0]:E1014 00:37:40.447000 3547233 torch/_inductor/select_algorithm.py:2555] [19/1] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/4o/c4otk34zsvh6wgdlerv6m3jaaihpkr32idscwpmccmafhfbxltc3.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank1]:E1014 00:37:41.526000 3547234 torch/_inductor/select_algorithm.py:2691] [19/1] Runtime error during autotuning: 
[rank1]:E1014 00:37:41.526000 3547234 torch/_inductor/select_algorithm.py:2691] [19/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1014 00:37:41.526000 3547234 torch/_inductor/select_algorithm.py:2691] [19/1] Ignoring this choice.
[rank0]:E1014 00:37:41.571000 3547233 torch/_inductor/select_algorithm.py:2691] [19/1] Runtime error during autotuning: 
[rank0]:E1014 00:37:41.571000 3547233 torch/_inductor/select_algorithm.py:2691] [19/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1014 00:37:41.571000 3547233 torch/_inductor/select_algorithm.py:2691] [19/1] Ignoring this choice.
AUTOTUNE mm(24x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_24 0.4588 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_33 0.4637 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_21 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_28 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_25 0.4669 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_18 0.4690 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  mm 0.4721 ms 97.2% 
  triton_mm_20 0.7854 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_31 0.8100 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_27 0.8131 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5425 seconds and 3.3650 seconds precompiling for 18 choices
AUTOTUNE mm(24x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_24 0.4588 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_28 0.4639 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_33 0.4639 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_21 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_25 0.4680 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.4721 ms 97.2% 
  triton_mm_18 0.4905 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_20 0.7875 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_31 0.8090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_26 0.8182 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5460 seconds and 3.4249 seconds precompiling for 18 choices
[rank2]:E1014 00:37:41.735000 3547235 torch/_inductor/select_algorithm.py:2555] [19/1] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/ty/ctyyhsmrcpaacmyeuk46lyull6o2uyinuqdhl7xeedfa3pnphsmm.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank3]:E1014 00:37:42.274000 3547236 torch/_inductor/select_algorithm.py:2555] [19/1] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/gt/cgtubmmsignt36kojmuusydi4u4puiubj25r6dosjnpwwwk7wwhs.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank2]:E1014 00:37:42.806000 3547235 torch/_inductor/select_algorithm.py:2691] [19/1] Runtime error during autotuning: 
[rank2]:E1014 00:37:42.806000 3547235 torch/_inductor/select_algorithm.py:2691] [19/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1014 00:37:42.806000 3547235 torch/_inductor/select_algorithm.py:2691] [19/1] Ignoring this choice.
AUTOTUNE mm(24x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_24 0.4588 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_28 0.4639 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_21 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_33 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_25 0.4690 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.4762 ms 96.3% 
  triton_mm_18 0.4772 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_20 0.7916 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_31 0.8049 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_26 0.8120 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5436 seconds and 3.3065 seconds precompiling for 18 choices
[rank3]:E1014 00:37:43.502000 3547236 torch/_inductor/select_algorithm.py:2691] [19/1] Runtime error during autotuning: 
[rank3]:E1014 00:37:43.502000 3547236 torch/_inductor/select_algorithm.py:2691] [19/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1014 00:37:43.502000 3547236 torch/_inductor/select_algorithm.py:2691] [19/1] Ignoring this choice.
AUTOTUNE mm(24x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_24 0.4588 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_33 0.4647 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_28 0.4649 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_21 0.4669 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.4762 ms 96.3% 
  triton_mm_25 0.4823 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_18 0.4864 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_20 0.7875 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_31 0.8059 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_26 0.8120 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5431 seconds and 3.5312 seconds precompiling for 18 choices
Capturing batches (bs=24 avail_mem=3.35 GB):  57%|█████▋    | 8/14 [01:44<02:36, 26.08s/it]Capturing batches (bs=16 avail_mem=3.33 GB):  57%|█████▋    | 8/14 [01:44<02:36, 26.08s/it][rank2]:E1014 00:38:17.262000 3547235 torch/_inductor/select_algorithm.py:2555] [19/2] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/na/cna6hd5wcyymn2sfwkmbnrxm6rwh4vzmojcmzfgp45sjgmqgvzif.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank0]:E1014 00:38:17.306000 3547233 torch/_inductor/select_algorithm.py:2555] [19/2] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/bc/cbcuxzrtahhhpx3yd4ty2wpcjyfhn4di65yz3oyslee4tyufkusm.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank1]:E1014 00:38:17.408000 3547234 torch/_inductor/select_algorithm.py:2555] [19/2] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/kb/ckbhyft7l4q55ed7ay5egt2hmnkoehfcnnqschz7wnzws4y4suuy.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank3]:E1014 00:38:18.182000 3547236 torch/_inductor/select_algorithm.py:2555] [19/2] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/nm/cnm27u6pmirqd3dgorxe3ei42s5kwemg276qpc2ppcrvvue6vj46.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank2]:E1014 00:38:18.338000 3547235 torch/_inductor/select_algorithm.py:2691] [19/2] Runtime error during autotuning: 
[rank2]:E1014 00:38:18.338000 3547235 torch/_inductor/select_algorithm.py:2691] [19/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1014 00:38:18.338000 3547235 torch/_inductor/select_algorithm.py:2691] [19/2] Ignoring this choice.
[rank0]:E1014 00:38:18.369000 3547233 torch/_inductor/select_algorithm.py:2691] [19/2] Runtime error during autotuning: 
[rank0]:E1014 00:38:18.369000 3547233 torch/_inductor/select_algorithm.py:2691] [19/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1014 00:38:18.369000 3547233 torch/_inductor/select_algorithm.py:2691] [19/2] Ignoring this choice.
AUTOTUNE mm(16x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_41 0.4516 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_38 0.4536 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_50 0.4554 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.4566 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_42 0.4567 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_35 0.4710 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4854 ms 93.0% 
  triton_mm_36 0.7844 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_48 0.7987 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_43 0.8028 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5459 seconds and 3.5001 seconds precompiling for 18 choices
AUTOTUNE mm(16x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_41 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_38 0.4547 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_50 0.4549 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_42 0.4557 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_45 0.4567 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_35 0.4772 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4813 ms 94.0% 
  triton_mm_36 0.7793 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_48 0.7977 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_44 0.8018 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5410 seconds and 3.4724 seconds precompiling for 18 choices
[rank1]:E1014 00:38:18.517000 3547234 torch/_inductor/select_algorithm.py:2691] [19/2] Runtime error during autotuning: 
[rank1]:E1014 00:38:18.517000 3547234 torch/_inductor/select_algorithm.py:2691] [19/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1014 00:38:18.517000 3547234 torch/_inductor/select_algorithm.py:2691] [19/2] Ignoring this choice.
AUTOTUNE mm(16x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_41 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_38 0.4547 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_50 0.4547 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_42 0.4566 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_45 0.4566 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_35 0.4710 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4813 ms 94.0% 
  triton_mm_36 0.7823 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_48 0.7997 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_44 0.8049 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5412 seconds and 3.5997 seconds precompiling for 18 choices
[rank3]:E1014 00:38:19.235000 3547236 torch/_inductor/select_algorithm.py:2691] [19/2] Runtime error during autotuning: 
[rank3]:E1014 00:38:19.235000 3547236 torch/_inductor/select_algorithm.py:2691] [19/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1014 00:38:19.235000 3547236 torch/_inductor/select_algorithm.py:2691] [19/2] Ignoring this choice.
AUTOTUNE mm(16x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_41 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_38 0.4536 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_50 0.4547 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.4567 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_42 0.4577 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_35 0.4721 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4864 ms 93.1% 
  triton_mm_36 0.7823 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_48 0.7967 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_44 0.8008 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5408 seconds and 3.5633 seconds precompiling for 18 choices
Capturing batches (bs=16 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [02:20<02:26, 29.27s/it]Capturing batches (bs=12 avail_mem=3.32 GB):  64%|██████▍   | 9/14 [02:20<02:26, 29.27s/it][rank3]:E1014 00:38:55.173000 3547236 torch/_inductor/select_algorithm.py:2555] [19/3] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/sg/csgtcbcit7lt3tr4e7g7t2dhtgdgncsmksfytxcx6zokuzsqzsbf.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank1]:E1014 00:38:55.254000 3547234 torch/_inductor/select_algorithm.py:2555] [19/3] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/pl/cplqszruhxlulgw4vqmtmxtgvaitlxsvzuugnttt47ub3djhbrtr.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank2]:E1014 00:38:55.267000 3547235 torch/_inductor/select_algorithm.py:2555] [19/3] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/u4/cu4tjcnh6suurchq44mute3pwrxzxedghfyu6pl4tw43tdnd4u3f.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank0]:E1014 00:38:55.890000 3547233 torch/_inductor/select_algorithm.py:2555] [19/3] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/uc/cucee6v5hv67gdhmes4kj5245poupmpdbxgm6cjjm5vmhadi5l5g.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank3]:E1014 00:38:56.286000 3547236 torch/_inductor/select_algorithm.py:2691] [19/3] Runtime error during autotuning: 
[rank3]:E1014 00:38:56.286000 3547236 torch/_inductor/select_algorithm.py:2691] [19/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1014 00:38:56.286000 3547236 torch/_inductor/select_algorithm.py:2691] [19/3] Ignoring this choice.
[rank1]:E1014 00:38:56.390000 3547234 torch/_inductor/select_algorithm.py:2691] [19/3] Runtime error during autotuning: 
[rank1]:E1014 00:38:56.390000 3547234 torch/_inductor/select_algorithm.py:2691] [19/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1014 00:38:56.390000 3547234 torch/_inductor/select_algorithm.py:2691] [19/3] Ignoring this choice.
[rank2]:E1014 00:38:56.397000 3547235 torch/_inductor/select_algorithm.py:2691] [19/3] Runtime error during autotuning: 
[rank2]:E1014 00:38:56.397000 3547235 torch/_inductor/select_algorithm.py:2691] [19/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1014 00:38:56.397000 3547235 torch/_inductor/select_algorithm.py:2691] [19/3] Ignoring this choice.
AUTOTUNE mm(12x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_55 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_67 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_58 0.4536 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_62 0.4566 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_59 0.4588 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_52 0.4710 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4864 ms 93.1% 
  triton_mm_53 0.7752 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_65 0.7926 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_61 0.7977 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5450 seconds and 3.1565 seconds precompiling for 18 choices
AUTOTUNE mm(12x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_67 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_58 0.4536 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_55 0.4547 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_59 0.4557 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_62 0.4567 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_52 0.4668 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4823 ms 93.8% 
  triton_mm_53 0.7813 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_54 0.7967 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_65 0.8008 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5440 seconds and 3.2825 seconds precompiling for 18 choices
AUTOTUNE mm(12x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_55 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_67 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_58 0.4536 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_62 0.4557 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_59 0.4577 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_52 0.4700 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4854 ms 93.2% 
  triton_mm_53 0.7721 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_65 0.7936 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_61 0.7987 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5404 seconds and 3.2510 seconds precompiling for 18 choices
[rank0]:E1014 00:38:56.986000 3547233 torch/_inductor/select_algorithm.py:2691] [19/3] Runtime error during autotuning: 
[rank0]:E1014 00:38:56.986000 3547233 torch/_inductor/select_algorithm.py:2691] [19/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1014 00:38:56.986000 3547233 torch/_inductor/select_algorithm.py:2691] [19/3] Ignoring this choice.
AUTOTUNE mm(12x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_55 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_67 0.4526 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_58 0.4536 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_59 0.4557 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_62 0.4567 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.4812 ms 94.1% 
  triton_mm_52 0.4844 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_53 0.7762 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_65 0.7944 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_61 0.7998 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5423 seconds and 3.1197 seconds precompiling for 18 choices
Capturing batches (bs=12 avail_mem=3.32 GB):  71%|███████▏  | 10/14 [02:58<02:07, 31.88s/it]Capturing batches (bs=8 avail_mem=3.29 GB):  71%|███████▏  | 10/14 [02:58<02:07, 31.88s/it] [rank2]:E1014 00:39:32.814000 3547235 torch/_inductor/select_algorithm.py:2555] [19/4] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/u5/cu5d4v53rqxvcloxt43kpslcujqfjdgchvar3q57z5ndarucjq76.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank0]:E1014 00:39:32.842000 3547233 torch/_inductor/select_algorithm.py:2555] [19/4] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/nu/cnu6q74fvqlz5rvsq7frrk6i23bektndrkmzdmadji5ozq77zfvq.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank1]:E1014 00:39:32.956000 3547234 torch/_inductor/select_algorithm.py:2555] [19/4] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/el/celix7tqehhquwaadtneg2w5uf4picugfgtfwyj2brfvu4357qt5.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank3]:E1014 00:39:33.839000 3547236 torch/_inductor/select_algorithm.py:2555] [19/4] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/py/cpypv5yazod2jmrzxnd5q5ti3ww4dapum5cyaxti6b6yrcizm7sh.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank2]:E1014 00:39:33.870000 3547235 torch/_inductor/select_algorithm.py:2691] [19/4] Runtime error during autotuning: 
[rank2]:E1014 00:39:33.870000 3547235 torch/_inductor/select_algorithm.py:2691] [19/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1014 00:39:33.870000 3547235 torch/_inductor/select_algorithm.py:2691] [19/4] Ignoring this choice.
[rank0]:E1014 00:39:33.930000 3547233 torch/_inductor/select_algorithm.py:2691] [19/4] Runtime error during autotuning: 
[rank0]:E1014 00:39:33.930000 3547233 torch/_inductor/select_algorithm.py:2691] [19/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1014 00:39:33.930000 3547233 torch/_inductor/select_algorithm.py:2691] [19/4] Ignoring this choice.
AUTOTUNE mm(8x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_75 0.4506 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_72 0.4516 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.4526 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_84 0.4526 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_79 0.4557 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_69 0.4690 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4854 ms 92.8% 
  triton_mm_70 0.7721 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_82 0.7895 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_71 0.7946 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5405 seconds and 3.4922 seconds precompiling for 18 choices
[rank1]:E1014 00:39:34.045000 3547234 torch/_inductor/select_algorithm.py:2691] [19/4] Runtime error during autotuning: 
[rank1]:E1014 00:39:34.045000 3547234 torch/_inductor/select_algorithm.py:2691] [19/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1014 00:39:34.045000 3547234 torch/_inductor/select_algorithm.py:2691] [19/4] Ignoring this choice.
AUTOTUNE mm(8x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_72 0.4495 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.4506 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_75 0.4516 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.4516 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_79 0.4557 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_69 0.4700 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4792 ms 93.8% 
  triton_mm_70 0.7741 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_82 0.7946 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_71 0.7977 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5496 seconds and 3.5894 seconds precompiling for 18 choices
AUTOTUNE mm(8x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_72 0.4506 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.4506 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_75 0.4516 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.4526 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_79 0.4556 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_69 0.4700 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4813 ms 93.6% 
  triton_mm_70 0.7772 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_82 0.7977 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_78 0.7987 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5405 seconds and 3.7006 seconds precompiling for 18 choices
[rank3]:E1014 00:39:34.891000 3547236 torch/_inductor/select_algorithm.py:2691] [19/4] Runtime error during autotuning: 
[rank3]:E1014 00:39:34.891000 3547236 torch/_inductor/select_algorithm.py:2691] [19/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1014 00:39:34.891000 3547236 torch/_inductor/select_algorithm.py:2691] [19/4] Ignoring this choice.
AUTOTUNE mm(8x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_72 0.4506 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_75 0.4516 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_76 0.4526 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_84 0.4536 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_79 0.4557 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_69 0.4690 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4854 ms 92.8% 
  triton_mm_70 0.7711 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_82 0.7906 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_78 0.7936 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5409 seconds and 3.5482 seconds precompiling for 18 choices
Capturing batches (bs=8 avail_mem=3.29 GB):  79%|███████▊  | 11/14 [03:36<01:41, 33.72s/it]Capturing batches (bs=4 avail_mem=3.28 GB):  79%|███████▊  | 11/14 [03:36<01:41, 33.72s/it][rank3]:E1014 00:40:19.522000 3547236 torch/_inductor/select_algorithm.py:2555] [19/5] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/hu/chupiusbqkqlfyevjeg656ve27hfdmmhjeahdiudtn2upgr7sloa.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank1]:E1014 00:40:19.567000 3547234 torch/_inductor/select_algorithm.py:2555] [19/5] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/2f/c2fct46mallwdpyzgogvxqufv3uab2a7cr7iqxxwqkpcnutvbpbg.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank0]:E1014 00:40:19.629000 3547233 torch/_inductor/select_algorithm.py:2555] [19/5] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/35/c35q3dkeurpvn6iq7wzrs3claepxr4swibh24gmsu645nnwpfiri.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank2]:E1014 00:40:19.886000 3547235 torch/_inductor/select_algorithm.py:2555] [19/5] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/7f/c7fvwzuo7vrjgdkb7m7dhucgflsqgxy34wc6ej5aw4hdewvhun4o.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank3]:E1014 00:40:20.605000 3547236 torch/_inductor/select_algorithm.py:2691] [19/5] Runtime error during autotuning: 
[rank3]:E1014 00:40:20.605000 3547236 torch/_inductor/select_algorithm.py:2691] [19/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1014 00:40:20.605000 3547236 torch/_inductor/select_algorithm.py:2691] [19/5] Ignoring this choice.
[rank1]:E1014 00:40:20.695000 3547234 torch/_inductor/select_algorithm.py:2691] [19/5] Runtime error during autotuning: 
[rank1]:E1014 00:40:20.695000 3547234 torch/_inductor/select_algorithm.py:2691] [19/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1014 00:40:20.695000 3547234 torch/_inductor/select_algorithm.py:2691] [19/5] Ignoring this choice.
AUTOTUNE mm(4x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_89 0.4485 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.4495 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_92 0.4526 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.4526 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_96 0.4547 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_86 0.4833 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4854 ms 92.4% 
  triton_mm_87 0.7649 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_99 0.7864 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_88 0.7977 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5401 seconds and 3.7372 seconds precompiling for 18 choices
[rank0]:E1014 00:40:20.744000 3547233 torch/_inductor/select_algorithm.py:2691] [19/5] Runtime error during autotuning: 
[rank0]:E1014 00:40:20.744000 3547233 torch/_inductor/select_algorithm.py:2691] [19/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1014 00:40:20.744000 3547233 torch/_inductor/select_algorithm.py:2691] [19/5] Ignoring this choice.
AUTOTUNE mm(4x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_93 0.4485 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_89 0.4506 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_92 0.4507 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.4516 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_96 0.4554 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.4803 ms 93.4% 
  triton_mm_86 0.4803 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_87 0.7700 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_99 0.7946 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_88 0.7996 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5412 seconds and 3.8118 seconds precompiling for 18 choices
AUTOTUNE mm(4x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_93 0.4485 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_89 0.4516 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_101 0.4516 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_92 0.4526 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_96 0.4547 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.4803 ms 93.4% 
  triton_mm_86 0.4874 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_87 0.7670 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_99 0.7926 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_88 0.7997 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5389 seconds and 3.8514 seconds precompiling for 18 choices
[rank2]:E1014 00:40:20.992000 3547235 torch/_inductor/select_algorithm.py:2691] [19/5] Runtime error during autotuning: 
[rank2]:E1014 00:40:20.992000 3547235 torch/_inductor/select_algorithm.py:2691] [19/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1014 00:40:20.992000 3547235 torch/_inductor/select_algorithm.py:2691] [19/5] Ignoring this choice.
AUTOTUNE mm(4x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_89 0.4475 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.4485 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_92 0.4506 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.4516 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_96 0.4547 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_86 0.4833 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.4844 ms 92.4% 
  triton_mm_87 0.7639 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_99 0.7936 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_88 0.7956 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5401 seconds and 3.7976 seconds precompiling for 18 choices
Capturing batches (bs=4 avail_mem=3.28 GB):  86%|████████▌ | 12/14 [04:22<01:14, 37.49s/it]Capturing batches (bs=2 avail_mem=3.25 GB):  86%|████████▌ | 12/14 [04:22<01:14, 37.49s/it][rank2]:E1014 00:40:58.636000 3547235 torch/_inductor/select_algorithm.py:2555] [19/6] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/ip/cipovl722dhgvfpswl4q4g64ft6ytqlnou7tt74e34iq3j6mjoyr.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank1]:E1014 00:40:58.821000 3547234 torch/_inductor/select_algorithm.py:2555] [19/6] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/nq/cnqzeebgqiplzg3ih6hjvt5h4div4pyy22mjhv3s7wglj34w2kcf.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank3]:E1014 00:40:58.837000 3547236 torch/_inductor/select_algorithm.py:2555] [19/6] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/3q/c3qyyvhdeq2am4e4d563r3hgegb7lsktsmnwxhet26ki6ec5jebw.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank0]:E1014 00:40:58.838000 3547233 torch/_inductor/select_algorithm.py:2555] [19/6] Exception No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_root/d7/cd7g3ztoofirtg5kcf4meoocicfgh5k7h7wb7eztwlrfseglooi2.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4)
[rank2]:E1014 00:40:59.686000 3547235 torch/_inductor/select_algorithm.py:2691] [19/6] Runtime error during autotuning: 
[rank2]:E1014 00:40:59.686000 3547235 torch/_inductor/select_algorithm.py:2691] [19/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1014 00:40:59.686000 3547235 torch/_inductor/select_algorithm.py:2691] [19/6] Ignoring this choice.
AUTOTUNE mm(2x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_110 0.4485 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_118 0.4516 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_113 0.4547 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_106 0.4557 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_109 0.4588 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.4854 ms 92.4% 
  triton_mm_103 0.5335 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 0.7649 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_116 0.7916 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_112 0.7997 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5393 seconds and 3.6425 seconds precompiling for 18 choices
[rank0]:E1014 00:40:59.904000 3547233 torch/_inductor/select_algorithm.py:2691] [19/6] Runtime error during autotuning: 
[rank0]:E1014 00:40:59.904000 3547233 torch/_inductor/select_algorithm.py:2691] [19/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1014 00:40:59.904000 3547233 torch/_inductor/select_algorithm.py:2691] [19/6] Ignoring this choice.
[rank1]:E1014 00:40:59.906000 3547234 torch/_inductor/select_algorithm.py:2691] [19/6] Runtime error during autotuning: 
[rank1]:E1014 00:40:59.906000 3547234 torch/_inductor/select_algorithm.py:2691] [19/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1014 00:40:59.906000 3547234 torch/_inductor/select_algorithm.py:2691] [19/6] Ignoring this choice.
[rank3]:E1014 00:40:59.912000 3547236 torch/_inductor/select_algorithm.py:2691] [19/6] Runtime error during autotuning: 
[rank3]:E1014 00:40:59.912000 3547236 torch/_inductor/select_algorithm.py:2691] [19/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1014 00:40:59.912000 3547236 torch/_inductor/select_algorithm.py:2691] [19/6] Ignoring this choice.
AUTOTUNE mm(2x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_110 0.4475 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_118 0.4516 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_106 0.4536 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_113 0.4547 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_109 0.4577 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.4803 ms 93.2% 
  triton_mm_103 0.5111 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 0.7680 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_105 0.7813 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_116 0.7926 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5456 seconds and 3.7979 seconds precompiling for 18 choices
AUTOTUNE mm(2x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_110 0.4475 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_118 0.4515 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_113 0.4536 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_106 0.4538 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_109 0.4567 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.4812 ms 93.0% 
  triton_mm_103 0.5110 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 0.7700 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_116 0.7936 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_112 0.8008 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5413 seconds and 3.8321 seconds precompiling for 18 choices
AUTOTUNE mm(2x5120, 5120x37984)
strides: [5120, 1], [1, 5120]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_110 0.4485 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_118 0.4506 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_113 0.4536 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_106 0.4557 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_109 0.4588 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.4833 ms 92.8% 
  triton_mm_103 0.5386 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 0.7690 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_116 0.7895 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
  triton_mm_112 0.7926 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.5409 seconds and 3.8100 seconds precompiling for 18 choices
Capturing batches (bs=2 avail_mem=3.25 GB):  93%|█████████▎| 13/14 [05:01<00:37, 37.91s/it]Capturing batches (bs=1 avail_mem=3.23 GB):  93%|█████████▎| 13/14 [05:01<00:37, 37.91s/it]Capturing batches (bs=1 avail_mem=3.23 GB): 100%|██████████| 14/14 [05:35<00:00, 36.70s/it]Capturing batches (bs=1 avail_mem=3.23 GB): 100%|██████████| 14/14 [05:35<00:00, 23.95s/it]
[2025-10-14 00:41:36 TP0] Capture cuda graph end. Time elapsed: 336.44 s. mem usage=0.43 GB. avail mem=3.21 GB.
[2025-10-14 00:41:36 TP0] max_total_num_tokens=158234, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2472, context_len=32768, available_gpu_mem=3.21 GB
[2025-10-14 00:41:37] INFO:     Started server process [3547114]
[2025-10-14 00:41:37] INFO:     Waiting for application startup.
[2025-10-14 00:41:37] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2025-10-14 00:41:37] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2025-10-14 00:41:37] INFO:     Application startup complete.
[2025-10-14 00:41:37] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-14 00:41:38] INFO:     127.0.0.1:60320 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-14 00:41:38] Receive: obj=GenerateReqInput(rid='100cbc73ae1e40e28e45eec2f9182ec6', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:41:38 TP0] Prefill batch. #new-seq: 1, #new-token: 6, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:41:39] Finish: obj=GenerateReqInput(rid='100cbc73ae1e40e28e45eec2f9182ec6', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' Paris. This is a fact that is', 'output_ids': [6722, 3283, 315, 9625, 374, 12095, 13, 1096, 374, 264, 2097, 429, 374], 'meta_info': {'id': '100cbc73ae1e40e28e45eec2f9182ec6', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 6, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.5554287433624268}}
[2025-10-14 00:41:39] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:41:39] The server is fired up and ready to roll!
[2025-10-14 00:42:01] INFO:     127.0.0.1:48588 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-14 00:43:15] Receive: obj=GenerateReqInput(rid='09b51876c9ee432aa458eca7034c8a84', text='Write a detailed explanation of how neural networks work, covering architecture, training, and backpropagation:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 500, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'stop_regex': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:43:15 TP0] Prefill batch. #new-seq: 1, #new-token: 20, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:43:16 TP0] Decode batch. #running-req: 1, #token: 53, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.40, #queue-req: 0, 
[2025-10-14 00:43:17 TP0] Decode batch. #running-req: 1, #token: 93, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.39, #queue-req: 0, 
[2025-10-14 00:43:18 TP0] Decode batch. #running-req: 1, #token: 133, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.75, #queue-req: 0, 
[2025-10-14 00:43:18 TP0] Decode batch. #running-req: 1, #token: 173, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.13, #queue-req: 0, 
[2025-10-14 00:43:19 TP0] Decode batch. #running-req: 1, #token: 213, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.24, #queue-req: 0, 
[2025-10-14 00:43:19 TP0] Decode batch. #running-req: 1, #token: 253, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.36, #queue-req: 0, 
[2025-10-14 00:43:20 TP0] Decode batch. #running-req: 1, #token: 293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.16, #queue-req: 0, 
[2025-10-14 00:43:21 TP0] Decode batch. #running-req: 1, #token: 333, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.36, #queue-req: 0, 
[2025-10-14 00:43:21 TP0] Decode batch. #running-req: 1, #token: 373, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.21, #queue-req: 0, 
[2025-10-14 00:43:22 TP0] Decode batch. #running-req: 1, #token: 413, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.25, #queue-req: 0, 
[2025-10-14 00:43:23 TP0] Decode batch. #running-req: 1, #token: 453, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.25, #queue-req: 0, 
[2025-10-14 00:43:23 TP0] Decode batch. #running-req: 1, #token: 493, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.22, #queue-req: 0, 
[2025-10-14 00:43:24] Finish: obj=GenerateReqInput(rid='09b51876c9ee432aa458eca7034c8a84', text='Write a detailed explanation of how neural networks work, covering architecture, training, and backpropagation:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 500, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'stop_regex': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': " explain technical concepts without jargon and use an example to illustrate the process. ### How Neural Networks Work: A Detailed Explanation\n\nNeural networks are computational models inspired by the human brain. They are used to recognize patterns, make predictions, and solve complex problems in fields like image recognition, natural language processing, and game playing. Let's explore how they work by breaking down their **architecture**, **training process**, and the key algorithm **backpropagation**.\n\n---\n\n#### 1. Neural Network Architecture: The Building Blocks\n\nThink of a neural network as a layered system of simple processing units called **neurons** (also called nodes or units). Each neuron receives input, processes it, and sends output to the next layer.\n\nA typical neural network has three types of layers:\n\n- **Input Layer**: Receives the raw data (e.g., pixel values of an image).\n- **Hidden Layers**: Process the data through transformations (there can be one or multiple hidden layers).\n- **Output La" ... "ch connection between neurons has a **weight**—a number that determines how much influence one neuron has on another. Weights start as random values and are adjusted during training.\n\nHere's a simple example:\n- **Task**: Predict whether someone will like a movie based on two features:  \n  - How much they like action movies (0 to 10)  \n  - How much they like romance movies (0 to 10)  \n\n- **Network structure**:\n  - Input layer: 2 neurons (action score, romance score)\n  - 1 hidden layer: 3 neurons\n  - Output layer: 1 neuron (likes movie = 1, doesn't like = 0)\n\nEach neuron in the hidden layer receives weighted inputs from the input layer, performs a calculation, and passes the result to the output layer.\n\n---\n\n#### 2. How a Neuron Works: The Math Made Simple\n\nLet’s say we have a neuron in the hidden layer. It receives two inputs:\n- Action score: 8\n- Romance score: 3\n\nThese are multiplied by weights:\n- Weight for action: 0.5\n- Weight for romance: 0.2\n\nThe neuron adds them up:\n```\n(8 × 0.5) + (3 × 0.2) = 4 + 0.6 = ", 'output_ids': [323, 1182, 2674, 27137, 25, 10339, 10916, 18940, 2041, 502, 70821, 323, 990, 458, 3110, 311, 40368, 279, 1882, 13, 16600, 2585, 60477, 38710, 5547, 25, 362, 62665, 71287, 271, 8813, 4176, 14155, 525, 54480, 4119, 14606, 553, 279, 3738, 8109, 13, 2379, 525, 1483, 311, 15282, 12624, 11, 1281, 19898, 11, 323, 11625, 6351, 5322, 304, 5043, 1075, 2168, 17843, 11, 5810, 4128, 8692, 11, 323, 1809, 5619, 13, 6771, 594, 13186, 1246, 807, 975, 553, 14719, 1495, 862, 3070, 80419, 97219, 3070, 30654, 1882, 97219, 323, 279, 1376, 12111, 3070, 1419, 2674, 27137, 334, 382, 44364, 820, 220, 16, 13, 60477, 8141, 37843, 25, 576, 16858, 45678, 271, 38687, 315, 264, 29728, 3922, 438, 264, 63141, 1849, 315, 4285, 8692, 8153, 2598, 3070, 811, 53669, 334, 320, 18654, 2598, 7798, 476, 8153, 568, 8886, 48284, 21189, 1946, 11, 11364, 432, 11, 323, 21308, 2550, 311, 279, 1790, 6193, 382, 32, 14260, 29728, 3922, 702, 2326, 4494, 315, 13617, 1447, 12, 3070, 2505, 22735, 95518, 60296, 1886, 279, 7112, 821, 320, 68, 1302, 2572, 12955, 2750, 315, 458, 2168, 4292, 12, 3070, 17506, 83822, 95518, 8603, 279, 821, 1526, 52970, 320, 18532, 646, 387, 825, 476, 5248, 8177, 13617, 4292, 12, 3070, 5097, 22735, 95518, 12042, 1603, 279, 1590, 1102, 320, 68, 1302, 2572, 330, 4616, 1, 476, 330, 18457, 1, 369, 2168, 23850, 3593, 4854, 3633, 1948, 33213, 702, 264, 3070, 4765, 334, 2293, 64, 1372, 429, 26569, 1246, 1753, 10173, 825, 48284, 702, 389, 2441, 13, 1205, 2796, 1191, 438, 4194, 2750, 323, 525, 23368, 2337, 4862, 382, 8420, 594, 264, 4285, 3110, 510, 12, 3070, 6262, 95518, 32710, 3425, 4325, 686, 1075, 264, 5700, 3118, 389, 1378, 4419, 25, 2303, 220, 481, 2585, 1753, 807, 1075, 1917, 9508, 320, 15, 311, 220, 16, 15, 8, 2303, 220, 481, 2585, 1753, 807, 1075, 29263, 9508, 320, 15, 311, 220, 16, 15, 8, 18611, 12, 3070, 12320, 5944, 334, 510, 220, 481, 5571, 6193, 25, 220, 17, 33213, 320, 1311, 5456, 11, 29263, 5456, 340, 220, 481, 220, 16, 8177, 6193, 25, 220, 18, 33213, 198, 220, 481, 9258, 6193, 25, 220, 16, 48284, 320, 25039, 5700, 284, 220, 16, 11, 3171, 944, 1075, 284, 220, 15, 692, 4854, 48284, 304, 279, 8177, 6193, 21189, 36824, 11127, 504, 279, 1946, 6193, 11, 26674, 264, 21937, 11, 323, 16211, 279, 1102, 311, 279, 2550, 6193, 382, 44364, 820, 220, 17, 13, 2585, 264, 4182, 36090, 21104, 25, 576, 4149, 18802, 8993, 271, 10061, 748, 1977, 582, 614, 264, 48284, 304, 279, 8177, 6193, 13, 1084, 21189, 1378, 11127, 510, 12, 5586, 5456, 25, 220, 23, 198, 12, 51156, 5456, 25, 220, 18, 271, 9485, 525, 54916, 553, 14324, 510, 12, 16516, 369, 1917, 25, 220, 15, 13, 20, 198, 12, 16516, 369, 29263, 25, 220, 15, 13, 17, 271, 785, 48284, 11367, 1105, 705, 510, 13874, 3989, 7, 23, 24768, 220, 15, 13, 20, 8, 488, 320, 18, 24768, 220, 15, 13, 17, 8, 284, 220, 19, 488, 220, 15, 13, 21, 284, 220], 'meta_info': {'id': '09b51876c9ee432aa458eca7034c8a84', 'finish_reason': {'type': 'length', 'length': 500}, 'prompt_tokens': 20, 'weight_version': 'default', 'completion_tokens': 500, 'cached_tokens': 0, 'e2e_latency': 8.44343113899231}}
[2025-10-14 00:43:24] INFO:     127.0.0.1:36588 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-14 00:43:28] Receive: obj=GenerateReqInput(rid='3c214ddc5445424183553eadf0aed958', text='Write a detailed explanation of how neural networks work:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'max_new_tokens': 400, 'temperature': 0.7}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:43:28 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:43:29 TP0] Decode batch. #running-req: 1, #token: 23, token usage: 0.00, cuda graph: True, gen throughput (token/s): 7.35, #queue-req: 0, 
[2025-10-14 00:43:29 TP0] Decode batch. #running-req: 1, #token: 63, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.26, #queue-req: 0, 
[2025-10-14 00:43:30 TP0] Decode batch. #running-req: 1, #token: 103, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.17, #queue-req: 0, 
[2025-10-14 00:43:31 TP0] Decode batch. #running-req: 1, #token: 143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.27, #queue-req: 0, 
[2025-10-14 00:43:31 TP0] Decode batch. #running-req: 1, #token: 183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.41, #queue-req: 0, 
[2025-10-14 00:43:32 TP0] Decode batch. #running-req: 1, #token: 223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.10, #queue-req: 0, 
[2025-10-14 00:43:33 TP0] Decode batch. #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.22, #queue-req: 0, 
[2025-10-14 00:43:33 TP0] Decode batch. #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.31, #queue-req: 0, 
[2025-10-14 00:43:34 TP0] Decode batch. #running-req: 1, #token: 343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.13, #queue-req: 0, 
[2025-10-14 00:43:35 TP0] Decode batch. #running-req: 1, #token: 383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.22, #queue-req: 0, 
[2025-10-14 00:43:35] Finish: obj=GenerateReqInput(rid='3c214ddc5445424183553eadf0aed958', text='Write a detailed explanation of how neural networks work:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'max_new_tokens': 400, 'temperature': 0.7}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' their structure, function, and training process, with emphasis on backpropagation and gradient descent. Use clear examples to illustrate key concepts.\n\nNeural networks are computational models inspired by the human brain\'s structure and function. They are fundamental to modern artificial intelligence and machine learning, enabling computers to recognize patterns, make decisions, and solve complex problems. Let\'s explore their structure, function, and training process in detail.\n\n---\n\n### **1. Structure of Neural Networks**\n\nA neural network is composed of layers of interconnected "neurons" (artificial neurons), which are mathematical functions that process input data. The basic structure includes:\n\n#### **a. Layers**\n- **Input Layer**: Receives raw data (e.g., pixel values of an image, word embeddings). Each neuron corresponds to a feature.\n- **Hidden Layers**: Intermediate layers that transform inputs into more abstract representations. A network can have one or multiple hidden layers.\n- **Output Layer**: Produces the final prediction (e.g., class probabilities in classification, numerical value in regression).\n\n#### **b. Neurons (Nodes)**\nEach neuron:\n1. **Receives inputs** (from previous layer or directly from data).\n2. **Multiplies each input by a weight** (learnable parameter).\n3. **Adds a bias** (another learnable parameter).\n4. **Applies an activation function** to produce an output.\n\nMathematically, for a neuron:\n```\noutput = activation(∑(weight_i × input_i) + bias)\n```\n\n#### **c. Activation Functions**\nThese introduce non-linearity, allowing networks to model complex relationships. Common examples:\n- **Sigmoid**: σ(x) = 1/(1 + e^(-x)) → Output: (0,1)\n- **ReLU (Rectified Linear Unit)**: f(x) = max(0, x) → Simple, widely used\n- **Tanh**: tanh(x) = (e^x - e', 'output_ids': [1246, 29728, 14155, 975, 25, 862, 5944, 11, 729, 11, 323, 4862, 1882, 11, 448, 24654, 389, 1182, 2674, 27137, 323, 20169, 36952, 13, 5443, 2797, 10295, 311, 40368, 1376, 18940, 382, 8813, 4176, 14155, 525, 54480, 4119, 14606, 553, 279, 3738, 8109, 594, 5944, 323, 729, 13, 2379, 525, 15811, 311, 6481, 20443, 11229, 323, 5662, 6832, 11, 27362, 18495, 311, 15282, 12624, 11, 1281, 11181, 11, 323, 11625, 6351, 5322, 13, 6771, 594, 13186, 862, 5944, 11, 729, 11, 323, 4862, 1882, 304, 7716, 382, 44364, 14374, 3070, 16, 13, 28596, 315, 60477, 38710, 56177, 32, 29728, 3922, 374, 23415, 315, 13617, 315, 82316, 330, 811, 53669, 1, 320, 471, 16488, 33213, 701, 892, 525, 35972, 5746, 429, 1882, 1946, 821, 13, 576, 6770, 5944, 5646, 1447, 820, 3070, 64, 13, 83822, 1019, 12, 3070, 2505, 22735, 95518, 60296, 1886, 7112, 821, 320, 68, 1302, 2572, 12955, 2750, 315, 458, 2168, 11, 3409, 70547, 568, 8886, 48284, 33210, 311, 264, 4565, 624, 12, 3070, 17506, 83822, 95518, 60648, 13617, 429, 5165, 11127, 1119, 803, 8115, 43613, 13, 362, 3922, 646, 614, 825, 476, 5248, 8177, 13617, 624, 12, 3070, 5097, 22735, 95518, 12042, 1603, 279, 1590, 19639, 320, 68, 1302, 2572, 536, 48216, 304, 23850, 11, 34776, 897, 304, 30549, 3593, 820, 3070, 65, 13, 4182, 53669, 320, 12288, 8, 1019, 4854, 48284, 510, 16, 13, 3070, 693, 346, 1886, 11127, 334, 320, 1499, 3681, 6193, 476, 5961, 504, 821, 4292, 17, 13, 3070, 20358, 7202, 1817, 1946, 553, 264, 4680, 334, 320, 12675, 480, 5733, 4292, 18, 13, 3070, 72111, 264, 15470, 334, 320, 41963, 3960, 480, 5733, 4292, 19, 13, 3070, 10611, 7202, 458, 15099, 729, 334, 311, 8193, 458, 2550, 382, 8815, 336, 7022, 11, 369, 264, 48284, 510, 13874, 3989, 3006, 284, 15099, 7, 145127, 7, 4765, 5318, 24768, 1946, 5318, 8, 488, 15470, 340, 13874, 19324, 820, 3070, 66, 13, 49647, 23550, 1019, 9485, 19131, 2477, 8447, 10748, 11, 10693, 14155, 311, 1614, 6351, 11871, 13, 7718, 10295, 510, 12, 3070, 50, 52111, 95518, 47723, 2075, 8, 284, 220, 16, 11884, 16, 488, 384, 61, 4080, 87, 593, 11397, 9258, 25, 320, 15, 11, 16, 340, 12, 3070, 79652, 320, 4415, 1870, 28263, 7954, 32295, 25, 282, 2075, 8, 284, 1932, 7, 15, 11, 856, 8, 11397, 8993, 11, 13570, 1483, 198, 12, 3070, 51, 27924, 95518, 14197, 71, 2075, 8, 284, 320, 68, 61, 87, 481, 384], 'meta_info': {'id': '3c214ddc5445424183553eadf0aed958', 'finish_reason': {'type': 'length', 'length': 400}, 'prompt_tokens': 10, 'weight_version': 'default', 'completion_tokens': 400, 'cached_tokens': 9, 'e2e_latency': 6.814141511917114}}
[2025-10-14 00:43:35] INFO:     127.0.0.1:37730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:45:18] Receive: obj=GenerateReqInput(rid='3d35a8fa6f96414b98213d449617294e', text='Write a detailed explanation of how neural networks work:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'max_new_tokens': 400, 'temperature': 0.7}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:45:18 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 9, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:45:18 TP0] Decode batch. #running-req: 1, #token: 23, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.39, #queue-req: 0, 
[2025-10-14 00:45:19 TP0] Decode batch. #running-req: 1, #token: 63, token usage: 0.00, cuda graph: True, gen throughput (token/s): 64.22, #queue-req: 0, 
[2025-10-14 00:45:19 TP0] Decode batch. #running-req: 1, #token: 103, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.66, #queue-req: 0, 
[2025-10-14 00:45:20 TP0] Decode batch. #running-req: 1, #token: 143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.66, #queue-req: 0, 
[2025-10-14 00:45:21 TP0] Decode batch. #running-req: 1, #token: 183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.38, #queue-req: 0, 
[2025-10-14 00:45:21 TP0] Decode batch. #running-req: 1, #token: 223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.36, #queue-req: 0, 
[2025-10-14 00:45:22 TP0] Decode batch. #running-req: 1, #token: 263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.49, #queue-req: 0, 
[2025-10-14 00:45:23 TP0] Decode batch. #running-req: 1, #token: 303, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.30, #queue-req: 0, 
[2025-10-14 00:45:23 TP0] Decode batch. #running-req: 1, #token: 343, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.36, #queue-req: 0, 
[2025-10-14 00:45:24 TP0] Decode batch. #running-req: 1, #token: 383, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.34, #queue-req: 0, 
[2025-10-14 00:45:24] Finish: obj=GenerateReqInput(rid='3d35a8fa6f96414b98213d449617294e', text='Write a detailed explanation of how neural networks work:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'max_new_tokens': 400, 'temperature': 0.7}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': " Their structure, how they process information, how they learn, and their applications.illustrate your explaination with examples of real-world applications.\nNeural networks are computational models inspired by the structure and function of the biological brain. They are a fundamental component of machine learning and artificial intelligence, enabling computers to recognize patterns, make decisions, and solve complex problems. This explanation will cover their structure, information processing, learning mechanisms, and real-world applications.\n\n1. Structure of Neural Networks\n\nA neural network consists of interconnected layers of artificial neurons (nodes). These layers include:\n\n- Input Layer: The first layer that receives the raw input data (e.g., pixel values from an image, sound waves from audio).\n- Hidden Layers: One or more intermediate layers between input and output that perform computations and extract features from the data.\n- Output Layer: The final layer that produces the network's predictions or " ... 'yer, applies a mathematical transformation, and passes the result to neurons in the next layer. The connections between neurons have associated weights, which determine the strength of the signal passed between them.\n\n2. How Neural Networks Process Information\n\nInformation flows through a neural network in a process called forward propagation:\n\n1. Input data is fed into the input layer.\n2. Each neuron in the subsequent layer computes a weighted sum of its inputs: z = Σ(wi * xi) + b, where wi are weights, xi are inputs, and b is a bias term.\n3. The weighted sum is passed through an activation function (e.g., ReLU, sigmoid, tanh) to introduce non-linearity: a = f(z).\n4. The activated outputs are passed to the next layer, and this process continues until the output layer produces the final result.\n\nFor example, in image classification, pixel values enter the input layer, hidden layers progressively extract features like edges, shapes, and textures, and the output layer provides probabilities for different object', 'output_ids': [1246, 29728, 14155, 975, 25, 10964, 5944, 11, 1246, 807, 1882, 1995, 11, 1246, 807, 3960, 11, 323, 862, 8357, 13, 82618, 349, 697, 10339, 367, 448, 10295, 315, 1931, 30084, 8357, 624, 8813, 4176, 14155, 525, 54480, 4119, 14606, 553, 279, 5944, 323, 729, 315, 279, 23275, 8109, 13, 2379, 525, 264, 15811, 3692, 315, 5662, 6832, 323, 20443, 11229, 11, 27362, 18495, 311, 15282, 12624, 11, 1281, 11181, 11, 323, 11625, 6351, 5322, 13, 1096, 16148, 686, 3421, 862, 5944, 11, 1995, 8692, 11, 6832, 23783, 11, 323, 1931, 30084, 8357, 382, 16, 13, 28596, 315, 60477, 38710, 271, 32, 29728, 3922, 17167, 315, 82316, 13617, 315, 20443, 33213, 320, 20008, 568, 4220, 13617, 2924, 1447, 12, 5571, 22735, 25, 576, 1156, 6193, 429, 21189, 279, 7112, 1946, 821, 320, 68, 1302, 2572, 12955, 2750, 504, 458, 2168, 11, 5112, 16876, 504, 7699, 4292, 12, 34242, 83822, 25, 3776, 476, 803, 28439, 13617, 1948, 1946, 323, 2550, 429, 2736, 82599, 323, 8649, 4419, 504, 279, 821, 624, 12, 9258, 22735, 25, 576, 1590, 6193, 429, 18644, 279, 3922, 594, 19898, 476, 95671, 382, 4854, 48284, 304, 264, 6193, 21189, 11127, 504, 33213, 304, 279, 3681, 6193, 11, 16790, 264, 35972, 17991, 11, 323, 16211, 279, 1102, 311, 33213, 304, 279, 1790, 6193, 13, 576, 13234, 1948, 33213, 614, 5815, 14324, 11, 892, 8253, 279, 8170, 315, 279, 8286, 5823, 1948, 1105, 382, 17, 13, 2585, 60477, 38710, 8603, 8085, 271, 14873, 27455, 1526, 264, 29728, 3922, 304, 264, 1882, 2598, 4637, 53643, 1447, 16, 13, 5571, 821, 374, 22313, 1119, 279, 1946, 6193, 624, 17, 13, 8886, 48284, 304, 279, 17420, 6193, 57203, 264, 36824, 2629, 315, 1181, 11127, 25, 1147, 284, 7851, 96, 3622, 72, 353, 37590, 8, 488, 293, 11, 1380, 23250, 525, 14324, 11, 37590, 525, 11127, 11, 323, 293, 374, 264, 15470, 4647, 624, 18, 13, 576, 36824, 2629, 374, 5823, 1526, 458, 15099, 729, 320, 68, 1302, 2572, 1032, 23236, 11, 64890, 11, 14197, 71, 8, 311, 19131, 2477, 8447, 10748, 25, 264, 284, 282, 13174, 4292, 19, 13, 576, 21985, 16275, 525, 5823, 311, 279, 1790, 6193, 11, 323, 419, 1882, 9539, 3080, 279, 2550, 6193, 18644, 279, 1590, 1102, 382, 2461, 3110, 11, 304, 2168, 23850, 11, 12955, 2750, 3725, 279, 1946, 6193, 11, 8177, 13617, 71759, 8649, 4419, 1075, 12822, 11, 20816, 11, 323, 29853, 11, 323, 279, 2550, 6193, 5707, 48216, 369, 2155, 1633], 'meta_info': {'id': '3d35a8fa6f96414b98213d449617294e', 'finish_reason': {'type': 'length', 'length': 400}, 'prompt_tokens': 10, 'weight_version': 'default', 'completion_tokens': 400, 'cached_tokens': 9, 'e2e_latency': 6.37157678604126}}
[2025-10-14 00:45:24] INFO:     127.0.0.1:56760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:50:58] INFO:     127.0.0.1:56684 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-14 00:50:58] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:50:58] Receive: obj=GenerateReqInput(rid='864ffb625f4e4ba0a83831fc6c9356d1', text='User Context:\n\n\n\nUser: Tell me a random fun fact about the Roman Empire\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=True, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:50:58 TP0] Prefill batch. #new-seq: 1, #new-token: 18, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:50:58 TP0] Decode batch. #running-req: 1, #token: 31, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.12, #queue-req: 0, 
[2025-10-14 00:50:59 TP0] Decode batch. #running-req: 1, #token: 71, token usage: 0.00, cuda graph: True, gen throughput (token/s): 63.45, #queue-req: 0, 
[2025-10-14 00:50:59 TP0] Decode batch. #running-req: 1, #token: 111, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.54, #queue-req: 0, 
[2025-10-14 00:51:00] INFO:     127.0.0.1:56704 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-14 00:51:00 TP0] Decode batch. #running-req: 1, #token: 151, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.58, #queue-req: 0, 
[2025-10-14 00:51:00] Finish: obj=GenerateReqInput(rid='864ffb625f4e4ba0a83831fc6c9356d1', text='User Context:\n\n\n\nUser: Tell me a random fun fact about the Roman Empire\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=True, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': " Did you know that the Romans had a special day dedicated to laughter? Every year on March 25th, they celebrated *Ridiculus Dies* (the Day of Laughter)! On this day, Romans would tell jokes, wear silly costumes, and even mock their teachers and bosses—temporarily reversing social roles for some good, clean fun. It was their way of relieving stress and keeping society light-hearted! 😄\n\nWhile the exact historical accuracy of this festival is debated by scholars, it's a fun example of how ancient Romans balanced their serious military and political life with humor and celebration. (Bonus: The Romans also had a god of laughter—*Gelotus*—though he was more of a mythical figure than a widely worshipped deity!)", 'output_ids': [151645], 'meta_info': {'id': '864ffb625f4e4ba0a83831fc6c9356d1', 'finish_reason': {'type': 'stop', 'matched': 151645}, 'prompt_tokens': 18, 'weight_version': 'default', 'completion_tokens': 158, 'cached_tokens': 0, 'e2e_latency': 2.621061086654663}}
[2025-10-14 00:51:01] Receive: obj=GenerateReqInput(rid='ab5ad910a48e406997f8a0607255e67e', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to kat-dev-awq-8bit...\n\n✅ Model ready! (0s)\n\n Did you know that the Romans had a special day dedicated to laughter? Every year on March 25th, they celebrated *Ridiculus Dies* (the Day of Laughter)! On this day, Romans would tell jokes, wear silly costumes, and even mock their teachers and bosses—temporarily reversing social roles for some good, clean fun. It was their way of relieving stress and keeping society light-hearted! 😄\n\nWhile the exact historical accuracy of this festival is debated by scholars, it\'s a fun example of how ancient Romans balanced their serious military and political life with humor and celebration. (Bonus: The Romans also had a god of laughter—*Gelotus*—though he was more of a mythical figure than a widely worshipped deity!)\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:01 TP0] Prefill batch. #new-seq: 1, #new-token: 399, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:01 TP0] Decode batch. #running-req: 1, #token: 415, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-10-14 00:51:02 TP0] Decode batch. #running-req: 1, #token: 455, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.14, #queue-req: 0, 
[2025-10-14 00:51:02] Finish: obj=GenerateReqInput(rid='ab5ad910a48e406997f8a0607255e67e', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to kat-dev-awq-8bit...\n\n✅ Model ready! (0s)\n\n Did you know that the Romans had a special day dedicated to laughter? Every year on March 25th, they celebrated *Ridiculus Dies* (the Day of Laughter)! On this day, Romans would tell jokes, wear silly costumes, and even mock their teachers and bosses—temporarily reversing social roles for some good, clean fun. It was their way of relieving stress and keeping society light-hearted! 😄\n\nWhile the exact historical accuracy of this festival is debated by scholars, it\'s a fun example of how ancient Romans balanced their serious military and political life with humor and celebration. (Bonus: The Romans also had a god of laughter—*Gelotus*—though he was more of a mythical figure than a widely worshipped deity!)\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "follow_ups": ["What other unique festivals did the Romans celebrate?", "Who was Gelotus and how was he worshiped?", "Did other ancient civilizations have similar humor-based holidays?", "How accurate is the historical record about Ridiculus Dies?", "What role did humor play in Roman daily life beyond festivals?"] }', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 18559, 8237, 82, 788, 4383, 3838, 1008, 4911, 44417, 1521, 279, 48717, 18383, 31011, 330, 15191, 572, 44382, 354, 355, 323, 1246, 572, 566, 23916, 291, 31011, 330, 6986, 1008, 13833, 85469, 614, 4428, 27385, 5980, 24421, 31011, 330, 4340, 13382, 374, 279, 13656, 3255, 911, 44895, 292, 19425, 23980, 31011, 330, 3838, 3476, 1521, 27385, 1486, 304, 12751, 7298, 2272, 7797, 44417, 30, 1341, 335, 151643], 'meta_info': {'id': 'ab5ad910a48e406997f8a0607255e67e', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 400, 'weight_version': 'default', 'completion_tokens': 69, 'cached_tokens': 1, 'e2e_latency': 1.370288610458374}}
[2025-10-14 00:51:02] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:02] Receive: obj=GenerateReqInput(rid='37d1a2b322e84bf88c2795d77bf7443a', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music S' ... 'oductivity Tips" },\n- { "title": "Artificial Intelligence in Healthcare" },\n- { "title": "🎮 Video Game Development Insights" }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to kat-dev-awq-8bit...\n\n✅ Model ready! (0s)\n\n Did you know that the Romans had a special day dedicated to laughter? Every year on March 25th, they celebrated *Ridiculus Dies* (the Day of Laughter)! On this day, Romans would tell jokes, wear silly costumes, and even mock their teachers and bosses—temporarily reversing social roles for some good, clean fun. It was their way of relieving stress and keeping society light-hearted! 😄\n\nWhile the exact historical accuracy of this festival is debated by scholars, it\'s a fun example of how ancient Romans balanced their serious military and political life with humor and celebration. (Bonus: The Romans also had a god of laughter—*Gelotus*—though he was more of a mythical figure than a widely worshipped deity!)\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:02 TP0] Prefill batch. #new-seq: 1, #new-token: 465, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:02] Finish: obj=GenerateReqInput(rid='37d1a2b322e84bf88c2795d77bf7443a', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music S' ... 'oductivity Tips" },\n- { "title": "Artificial Intelligence in Healthcare" },\n- { "title": "🎮 Video Game Development Insights" }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to kat-dev-awq-8bit...\n\n✅ Model ready! (0s)\n\n Did you know that the Romans had a special day dedicated to laughter? Every year on March 25th, they celebrated *Ridiculus Dies* (the Day of Laughter)! On this day, Romans would tell jokes, wear silly costumes, and even mock their teachers and bosses—temporarily reversing social roles for some good, clean fun. It was their way of relieving stress and keeping society light-hearted! 😄\n\nWhile the exact historical accuracy of this festival is debated by scholars, it\'s a fun example of how ancient Romans balanced their serious military and political life with humor and celebration. (Bonus: The Romans also had a god of laughter—*Gelotus*—though he was more of a mythical figure than a widely worshipped deity!)\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "title": "😄 Roman Empire Laughter Day" }', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 2102, 788, 330, 144741, 12751, 20448, 4929, 7340, 6059, 1, 335, 151643], 'meta_info': {'id': '37d1a2b322e84bf88c2795d77bf7443a', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 470, 'weight_version': 'default', 'completion_tokens': 14, 'cached_tokens': 5, 'e2e_latency': 0.5339312553405762}}
[2025-10-14 00:51:02] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:02] Receive: obj=GenerateReqInput(rid='9fd85827dd3f4fbaaa4e37b7a32c3ff7', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to kat-dev-awq-8bit...\n\n✅ Model ready! (0s)\n\n Did you know that the Romans had a special day dedicated to laughter? Every year on March 25th, they celebrated *Ridiculus Dies* (the Day of Laughter)! On this day, Romans would tell jokes, wear silly costumes, and even mock their teachers and bosses—temporarily reversing social roles for some good, clean fun. It was their way of relieving stress and keeping society light-hearted! 😄\n\nWhile the exact historical accuracy of this festival is debated by scholars, it\'s a fun example of how ancient Romans balanced their serious military and political life with humor and celebration. (Bonus: The Romans also had a god of laughter—*Gelotus*—though he was more of a mythical figure than a widely worshipped deity!)\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:02 TP0] Prefill batch. #new-seq: 1, #new-token: 355, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:03 TP0] Decode batch. #running-req: 1, #token: 373, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.22, #queue-req: 0, 
[2025-10-14 00:51:03] Finish: obj=GenerateReqInput(rid='9fd85827dd3f4fbaaa4e37b7a32c3ff7', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to kat-dev-awq-8bit...\n\n✅ Model ready! (0s)\n\n Did you know that the Romans had a special day dedicated to laughter? Every year on March 25th, they celebrated *Ridiculus Dies* (the Day of Laughter)! On this day, Romans would tell jokes, wear silly costumes, and even mock their teachers and bosses—temporarily reversing social roles for some good, clean fun. It was their way of relieving stress and keeping society light-hearted! 😄\n\nWhile the exact historical accuracy of this festival is debated by scholars, it\'s a fun example of how ancient Romans balanced their serious military and political life with humor and celebration. (Bonus: The Romans also had a god of laughter—*Gelotus*—though he was more of a mythical figure than a widely worshipped deity!)\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "tags": ["History", "Ancient Rome", "Culture"] }', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 14082, 788, 4383, 13424, 497, 330, 88067, 1167, 21718, 497, 330, 25811, 1341, 335, 151643], 'meta_info': {'id': '9fd85827dd3f4fbaaa4e37b7a32c3ff7', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 361, 'weight_version': 'default', 'completion_tokens': 17, 'cached_tokens': 6, 'e2e_latency': 0.505521297454834}}
[2025-10-14 00:51:03] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:16] INFO:     127.0.0.1:45130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:16] Receive: obj=GenerateReqInput(rid='c4477d6d214b4bd1a53ffb2b81de0410', text='User Context:\n\n\n\nUser: Tell me a random fun fact about the Roman Empire\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=True, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 17, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:16 TP0] Decode batch. #running-req: 1, #token: 53, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.98, #queue-req: 0, 
[2025-10-14 00:51:17 TP0] Decode batch. #running-req: 1, #token: 93, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.32, #queue-req: 0, 
[2025-10-14 00:51:17] Finish: obj=GenerateReqInput(rid='c4477d6d214b4bd1a53ffb2b81de0410', text='User Context:\n\n\n\nUser: Tell me a random fun fact about the Roman Empire\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=True, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' Did you know that the Roman Empire had a surprisingly advanced postal system? The Romans established a network of relay stations called *cursus publicus* (public course) that allowed for the rapid transmission of official messages across the empire. Messengers, often mounted on horses, would travel from station to station, changing horses to maintain speed. This system was so efficient that messages could travel from Rome to the edge of the empire in a matter of days—a pretty impressive feat for the ancient world! 🐎📜', 'output_ids': [151645], 'meta_info': {'id': 'c4477d6d214b4bd1a53ffb2b81de0410', 'finish_reason': {'type': 'stop', 'matched': 151645}, 'prompt_tokens': 18, 'weight_version': 'default', 'completion_tokens': 104, 'cached_tokens': 17, 'e2e_latency': 1.7827682495117188}}
[2025-10-14 00:51:18] Receive: obj=GenerateReqInput(rid='3cc152b7f975413b9555b50ffa8b6bca', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: Did you know that the Roman Empire had a surprisingly advanced postal system? The Romans established a network of relay stations called *cursus publicus* (public course) that allowed for the rapid transmission of official messages across the empire. Messengers, often mounted on horses, would travel from station to station, changing horses to maintain speed. This system was so efficient that messages could travel from Rome to the edge of the empire in a matter of days—a pretty impressive feat for the ancient world! 🐎📜\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:18 TP0] Prefill batch. #new-seq: 1, #new-token: 110, #cached-token: 215, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:18 TP0] Decode batch. #running-req: 1, #token: 336, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.32, #queue-req: 0, 
[2025-10-14 00:51:18 TP0] Decode batch. #running-req: 1, #token: 376, token usage: 0.00, cuda graph: True, gen throughput (token/s): 58.51, #queue-req: 0, 
[2025-10-14 00:51:19] Finish: obj=GenerateReqInput(rid='3cc152b7f975413b9555b50ffa8b6bca', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: Did you know that the Roman Empire had a surprisingly advanced postal system? The Romans established a network of relay stations called *cursus publicus* (public course) that allowed for the rapid transmission of official messages across the empire. Messengers, often mounted on horses, would travel from station to station, changing horses to maintain speed. This system was so efficient that messages could travel from Rome to the edge of the empire in a matter of days—a pretty impressive feat for the ancient world! 🐎📜\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "follow_ups": ["How did the Roman postal system compare to other ancient empires?", "What happened to the cursus publicus after the fall of the Roman Empire?", "Were there any security measures to protect messages in the Roman postal system?", "Did common citizens have access to the Roman postal service, or was it only for officials?", "How many relay stations were there in the Roman Empire\'s postal network?"] }', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 18559, 8237, 82, 788, 4383, 4340, 1521, 279, 12751, 39754, 1849, 9429, 311, 1008, 13833, 976, 18968, 31011, 330, 3838, 6932, 311, 279, 23143, 355, 584, 355, 1283, 279, 4399, 315, 279, 12751, 20448, 31011, 330, 88453, 1052, 894, 4763, 10953, 311, 6016, 6605, 304, 279, 12751, 39754, 1849, 31011, 330, 6986, 4185, 10283, 614, 2615, 311, 279, 12751, 39754, 2473, 11, 476, 572, 432, 1172, 369, 7359, 31011, 330, 4340, 1657, 31851, 17337, 1033, 1052, 304, 279, 12751, 20448, 594, 39754, 3922, 30, 1341, 335, 151643], 'meta_info': {'id': '3cc152b7f975413b9555b50ffa8b6bca', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 325, 'weight_version': 'default', 'completion_tokens': 89, 'cached_tokens': 215, 'e2e_latency': 1.5958924293518066}}
[2025-10-14 00:51:19] INFO:     127.0.0.1:45144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:19] Receive: obj=GenerateReqInput(rid='8b33f84964694b108952eb02a8f4d060', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music Streaming" },\n- { "title": "Remote Work Productivity Tips" },\n- { "title": "Artificial Intelligence in Healthcare" },\n- { "title": "🎮 Video Game Development Insights" }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: Did you know that the Roman Empire had a surprisingly advanced postal system? The Romans established a network of relay stations called *cursus publicus* (public course) that allowed for the rapid transmission of official messages across the empire. Messengers, often mounted on horses, would travel from station to station, changing horses to maintain speed. This system was so efficient that messages could travel from Rome to the edge of the empire in a matter of days—a pretty impressive feat for the ancient world! 🐎📜\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:19 TP0] Prefill batch. #new-seq: 1, #new-token: 110, #cached-token: 285, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:19 TP0] Decode batch. #running-req: 1, #token: 397, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.99, #queue-req: 0, 
[2025-10-14 00:51:19] Finish: obj=GenerateReqInput(rid='8b33f84964694b108952eb02a8f4d060', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music Streaming" },\n- { "title": "Remote Work Productivity Tips" },\n- { "title": "Artificial Intelligence in Healthcare" },\n- { "title": "🎮 Video Game Development Insights" }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: Did you know that the Roman Empire had a surprisingly advanced postal system? The Romans established a network of relay stations called *cursus publicus* (public course) that allowed for the rapid transmission of official messages across the empire. Messengers, often mounted on horses, would travel from station to station, changing horses to maintain speed. This system was so efficient that messages could travel from Rome to the edge of the empire in a matter of days—a pretty impressive feat for the ancient world! 🐎📜\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "title": "🐎📜 Roman Postal System" }', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 2102, 788, 330, 148145, 147316, 12751, 54217, 739, 1, 335, 151643], 'meta_info': {'id': '8b33f84964694b108952eb02a8f4d060', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 395, 'weight_version': 'default', 'completion_tokens': 13, 'cached_tokens': 285, 'e2e_latency': 0.2918429374694824}}
[2025-10-14 00:51:19] INFO:     127.0.0.1:45146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:19] Receive: obj=GenerateReqInput(rid='02619e7b9fec4f69a238812de2b98722', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: Did you know that the Roman Empire had a surprisingly advanced postal system? The Romans established a network of relay stations called *cursus publicus* (public course) that allowed for the rapid transmission of official messages across the empire. Messengers, often mounted on horses, would travel from station to station, changing horses to maintain speed. This system was so efficient that messages could travel from Rome to the edge of the empire in a matter of days—a pretty impressive feat for the ancient world! 🐎📜\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:19 TP0] Prefill batch. #new-seq: 1, #new-token: 110, #cached-token: 176, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:20] Finish: obj=GenerateReqInput(rid='02619e7b9fec4f69a238812de2b98722', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: Did you know that the Roman Empire had a surprisingly advanced postal system? The Romans established a network of relay stations called *cursus publicus* (public course) that allowed for the rapid transmission of official messages across the empire. Messengers, often mounted on horses, would travel from station to station, changing horses to maintain speed. This system was so efficient that messages could travel from Rome to the edge of the empire in a matter of days—a pretty impressive feat for the ancient world! 🐎📜\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' ```json\n{\n  "tags": ["History", "Ancient Rome", "Communication"]\n}\n```', 'output_ids': [9686, 19802, 1339, 71703, 25, 54275, 2236, 198, 515, 220, 330, 14082, 788, 4383, 13424, 497, 330, 88067, 1167, 21718, 497, 330, 65411, 7026, 532, 73594, 151643], 'meta_info': {'id': '02619e7b9fec4f69a238812de2b98722', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 286, 'weight_version': 'default', 'completion_tokens': 22, 'cached_tokens': 176, 'e2e_latency': 0.4499626159667969}}
[2025-10-14 00:51:20] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:34] INFO:     127.0.0.1:42320 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-14 00:51:34] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:34] Receive: obj=GenerateReqInput(rid='947265aacfbf43cfaf9efa4d440d9b1b', text='User Context:\n\n\n\nUser: Tell me a random fun fact about the Roman Empire\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=True, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 17, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:34 TP0] Decode batch. #running-req: 1, #token: 25, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.65, #queue-req: 0, 
[2025-10-14 00:51:35 TP0] Decode batch. #running-req: 1, #token: 65, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.37, #queue-req: 0, 
[2025-10-14 00:51:36 TP0] Decode batch. #running-req: 1, #token: 105, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.20, #queue-req: 0, 
[2025-10-14 00:51:36] INFO:     127.0.0.1:42326 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-14 00:51:36 TP0] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.47, #queue-req: 0, 
[2025-10-14 00:51:37] Finish: obj=GenerateReqInput(rid='947265aacfbf43cfaf9efa4d440d9b1b', text='User Context:\n\n\n\nUser: Tell me a random fun fact about the Roman Empire\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=True, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': " Here's a random fun fact about the Roman Empire:\n\n**Romans were obsessed with mirrors!** They used polished bronze and silver mirrors in their homes and baths. These weren't just for vanity—wealthy Romans would sometimes carry small mirrors to check their appearance before social events, much like we might use a phone screen today. Some mirrors even had handles shaped like gods or mythological figures!\n\nBonus fact: Since bronze tarnishes, these mirrors were actually a bit dark and more like reflective bronze plates rather than the clear glass mirrors we know today. But still, it shows just how much importance Romans placed on personal appearance and luxury gadgets of their time! ✨\n\nWant another random tidbit, or are you curious about a specific aspect of Roman life? 🔥🏛️", 'output_ids': [151645], 'meta_info': {'id': '947265aacfbf43cfaf9efa4d440d9b1b', 'finish_reason': {'type': 'stop', 'matched': 151645}, 'prompt_tokens': 18, 'weight_version': 'default', 'completion_tokens': 157, 'cached_tokens': 17, 'e2e_latency': 2.6129581928253174}}
[2025-10-14 00:51:37] Receive: obj=GenerateReqInput(rid='0015720e16054dfebb0a638988392fa8', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to magnum-v4-123b-awq...\n\n✅ Model ready! (0s)\n\n Here\'s a random fun fact about the Roman Empire:\n\n**Romans were obsessed with mirrors!** They used polished bronze and silver mirrors in their homes and baths. These weren\'t just for vanity—wealthy Romans would sometimes carry small mirrors to check their appearance before social events, much like we might use a phone screen today. Some mirrors even had handles shaped like gods or mythological figures!\n\nBonus fact: Since bronze tarnishes, these mirrors were actually a bit dark and more like reflective bronze plates rather than the clear glass mirrors we know today. But still, it shows just how much importance Romans placed on personal appearance and luxury gadgets of their time! ✨\n\nWant another random tidbit, or are you curious about a specific aspect of Roman life? 🔥🏛️\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:37 TP0] Prefill batch. #new-seq: 1, #new-token: 184, #cached-token: 220, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:37 TP0] Decode batch. #running-req: 1, #token: 414, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.19, #queue-req: 0, 
[2025-10-14 00:51:38 TP0] Decode batch. #running-req: 1, #token: 454, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.23, #queue-req: 0, 
[2025-10-14 00:51:38] Finish: obj=GenerateReqInput(rid='0015720e16054dfebb0a638988392fa8', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to magnum-v4-123b-awq...\n\n✅ Model ready! (0s)\n\n Here\'s a random fun fact about the Roman Empire:\n\n**Romans were obsessed with mirrors!** They used polished bronze and silver mirrors in their homes and baths. These weren\'t just for vanity—wealthy Romans would sometimes carry small mirrors to check their appearance before social events, much like we might use a phone screen today. Some mirrors even had handles shaped like gods or mythological figures!\n\nBonus fact: Since bronze tarnishes, these mirrors were actually a bit dark and more like reflective bronze plates rather than the clear glass mirrors we know today. But still, it shows just how much importance Romans placed on personal appearance and luxury gadgets of their time! ✨\n\nWant another random tidbit, or are you curious about a specific aspect of Roman life? 🔥🏛️\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "follow_ups": ["What other personal grooming habits did the Romans have?", "How were bronze mirrors made in ancient Rome?", "Did Roman mirrors have any symbolic or religious meanings?", "What materials did Romans use for their luxury goods besides bronze?"] }', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 18559, 8237, 82, 788, 4383, 3838, 1008, 4345, 74061, 25785, 1521, 279, 48717, 614, 31011, 330, 4340, 1033, 39807, 40485, 1865, 304, 13833, 21718, 31011, 330, 6986, 12751, 40485, 614, 894, 35296, 476, 10381, 49700, 31011, 330, 3838, 7236, 1521, 48717, 990, 369, 862, 19352, 11561, 27758, 39807, 30, 1341, 335, 151643], 'meta_info': {'id': '0015720e16054dfebb0a638988392fa8', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 404, 'weight_version': 'default', 'completion_tokens': 54, 'cached_tokens': 220, 'e2e_latency': 1.0054659843444824}}
[2025-10-14 00:51:38] INFO:     127.0.0.1:42336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:38] Receive: obj=GenerateReqInput(rid='e707c36c8e6a48f39980cbd1413fa690', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music S' ... 'e" },\n- { "title": "🎮 Video Game Development Insights" }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to magnum-v4-123b-awq...\n\n✅ Model ready! (0s)\n\n Here\'s a random fun fact about the Roman Empire:\n\n**Romans were obsessed with mirrors!** They used polished bronze and silver mirrors in their homes and baths. These weren\'t just for vanity—wealthy Romans would sometimes carry small mirrors to check their appearance before social events, much like we might use a phone screen today. Some mirrors even had handles shaped like gods or mythological figures!\n\nBonus fact: Since bronze tarnishes, these mirrors were actually a bit dark and more like reflective bronze plates rather than the clear glass mirrors we know today. But still, it shows just how much importance Romans placed on personal appearance and luxury gadgets of their time! ✨\n\nWant another random tidbit, or are you curious about a specific aspect of Roman life? 🔥🏛️\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:38 TP0] Prefill batch. #new-seq: 1, #new-token: 184, #cached-token: 290, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:39 TP0] Decode batch. #running-req: 1, #token: 510, token usage: 0.00, cuda graph: True, gen throughput (token/s): 46.16, #queue-req: 0, 
[2025-10-14 00:51:39 TP0] Decode batch. #running-req: 1, #token: 550, token usage: 0.00, cuda graph: True, gen throughput (token/s): 60.49, #queue-req: 0, 
[2025-10-14 00:51:40 TP0] Decode batch. #running-req: 1, #token: 590, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.28, #queue-req: 0, 
[2025-10-14 00:51:41 TP0] Decode batch. #running-req: 1, #token: 630, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.28, #queue-req: 0, 
[2025-10-14 00:51:41] Finish: obj=GenerateReqInput(rid='e707c36c8e6a48f39980cbd1413fa690', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music S' ... 'e" },\n- { "title": "🎮 Video Game Development Insights" }\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to magnum-v4-123b-awq...\n\n✅ Model ready! (0s)\n\n Here\'s a random fun fact about the Roman Empire:\n\n**Romans were obsessed with mirrors!** They used polished bronze and silver mirrors in their homes and baths. These weren\'t just for vanity—wealthy Romans would sometimes carry small mirrors to check their appearance before social events, much like we might use a phone screen today. Some mirrors even had handles shaped like gods or mythological figures!\n\nBonus fact: Since bronze tarnishes, these mirrors were actually a bit dark and more like reflective bronze plates rather than the clear glass mirrors we know today. But still, it shows just how much importance Romans placed on personal appearance and luxury gadgets of their time! ✨\n\nWant another random tidbit, or are you curious about a specific aspect of Roman life? 🔥🏛️\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "title": "_ROMAN EMPIRE FUN FACT" }  \nNote: The title includes an underscore at the beginning to meet the 3-5 word requirement while still clearly representing the chat\'s main theme. The fun fact about Roman mirrors is the central topic. If the underscore is not desired, the title could also be: "Roman Empire Mirror Obsession" but that exceeds the word limit. The emoji would be better placed as part of the title format if allowed, but per the guidelines, no special formatting is permitted. Since the example titles don\'t include emojis within the title text itself (they seem to be added in the examples for display purposes), I\'ve kept the title clean and focused on the core topic. The title accurately represents the chat\'s main subject: a fun fact about the Roman Empire.', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 2102, 788, 330, 71844, 1093, 16902, 1893, 787, 35449, 58543, 1, 335, 2303, 9112, 25, 576, 2265, 5646, 458, 52109, 518, 279, 7167, 311, 3367, 279, 220, 18, 12, 20, 3409, 16287, 1393, 2058, 9355, 14064, 279, 6236, 594, 1887, 6912, 13, 576, 2464, 2097, 911, 12751, 40485, 374, 279, 8622, 8544, 13, 1416, 279, 52109, 374, 537, 12685, 11, 279, 2265, 1410, 1083, 387, 25, 330, 60980, 20448, 33854, 50441, 1338, 1, 714, 429, 35275, 279, 3409, 3930, 13, 576, 42365, 1035, 387, 2664, 9099, 438, 949, 315, 279, 2265, 3561, 421, 5420, 11, 714, 817, 279, 17501, 11, 902, 3281, 36566, 374, 15129, 13, 8704, 279, 3110, 15311, 1513, 944, 2924, 99066, 2878, 279, 2265, 1467, 5086, 320, 20069, 2803, 311, 387, 3694, 304, 279, 10295, 369, 3037, 9895, 701, 358, 3003, 8604, 279, 2265, 4240, 323, 10735, 389, 279, 6200, 8544, 13, 576, 2265, 29257, 10868, 279, 6236, 594, 1887, 3832, 25, 264, 2464, 2097, 911, 279, 12751, 20448, 13, 151645], 'meta_info': {'id': 'e707c36c8e6a48f39980cbd1413fa690', 'finish_reason': {'type': 'stop', 'matched': 151645}, 'prompt_tokens': 474, 'weight_version': 'default', 'completion_tokens': 166, 'cached_tokens': 290, 'e2e_latency': 2.8343255519866943}}
[2025-10-14 00:51:41] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:51:41] Receive: obj=GenerateReqInput(rid='0ae9757546874f53896da6b22e248a4b', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to magnum-v4-123b-awq...\n\n✅ Model ready! (0s)\n\n Here\'s a random fun fact about the Roman Empire:\n\n**Romans were obsessed with mirrors!** They used polished bronze and silver mirrors in their homes and baths. These weren\'t just for vanity—wealthy Romans would sometimes carry small mirrors to check their appearance before social events, much like we might use a phone screen today. Some mirrors even had handles shaped like gods or mythological figures!\n\nBonus fact: Since bronze tarnishes, these mirrors were actually a bit dark and more like reflective bronze plates rather than the clear glass mirrors we know today. But still, it shows just how much importance Romans placed on personal appearance and luxury gadgets of their time! ✨\n\nWant another random tidbit, or are you curious about a specific aspect of Roman life? 🔥🏛️\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False)
[2025-10-14 00:51:41 TP0] Prefill batch. #new-seq: 1, #new-token: 184, #cached-token: 181, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-14 00:51:41] Finish: obj=GenerateReqInput(rid='0ae9757546874f53896da6b22e248a4b', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Tell me a random fun fact about the Roman Empire\nASSISTANT: 🔄 Switching to magnum-v4-123b-awq...\n\n✅ Model ready! (0s)\n\n Here\'s a random fun fact about the Roman Empire:\n\n**Romans were obsessed with mirrors!** They used polished bronze and silver mirrors in their homes and baths. These weren\'t just for vanity—wealthy Romans would sometimes carry small mirrors to check their appearance before social events, much like we might use a phone screen today. Some mirrors even had handles shaped like gods or mythological figures!\n\nBonus fact: Since bronze tarnishes, these mirrors were actually a bit dark and more like reflective bronze plates rather than the clear glass mirrors we know today. But still, it shows just how much importance Romans placed on personal appearance and luxury gadgets of their time! ✨\n\nWant another random tidbit, or are you curious about a specific aspect of Roman life? 🔥🏛️\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False, return_entropy=False), out={'text': ' { "tags": ["History", "Ancient Rome", "Daily Life"]}', 'output_ids': [9686, 19802, 1339, 71703, 25, 314, 330, 14082, 788, 4383, 13424, 497, 330, 88067, 1167, 21718, 497, 330, 43553, 9414, 92446, 151643], 'meta_info': {'id': '0ae9757546874f53896da6b22e248a4b', 'finish_reason': {'type': 'stop', 'matched': 151643}, 'prompt_tokens': 365, 'weight_version': 'default', 'completion_tokens': 17, 'cached_tokens': 181, 'e2e_latency': 0.39624834060668945}}
[2025-10-14 00:51:41] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-14 00:56:41] INFO:     127.0.0.1:60388 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-14 00:56:50] INFO:     127.0.0.1:51316 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-14 00:58:13] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-14 00:58:18] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/start_sglang_katdev_tp4_compiled.sh: line 22: 3547114 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/KAT-Dev-AWQ-8bit --host 0.0.0.0 --port 8001 --tp 4 --context-length 32768 --served-model-name kat-dev-awq-8bit --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
