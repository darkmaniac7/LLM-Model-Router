/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
usage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,remote,remote_instance}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal] [--revision REVISION]
                        [--model-impl MODEL_IMPL] [--host HOST] [--port PORT]
                        [--grpc-mode] [--skip-server-warmup]
                        [--warmups WARMUPS] [--nccl-port NCCL_PORT]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--modelopt-checkpoint-restore-path MODELOPT_CHECKPOINT_RESTORE_PATH]
                        [--modelopt-checkpoint-save-path MODELOPT_CHECKPOINT_SAVE_PATH]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3}]
                        [--enable-fp32-lm-head]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--constrained-json-disable-any-whitespace]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--crash-on-nan CRASH_ON_NAN] [--show-time-cost]
                        [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--oltp-traces-endpoint OLTP_TRACES_ENDPOINT]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,qwen3,qwen3-thinking,step3}]
                        [--tool-call-parser {deepseekv3,deepseekv31,glm,glm45,gpt-oss,kimi_k2,llama3,mistral,pythonic,qwen,qwen25,qwen3_coder,step3}]
                        [--sampling-defaults {openai,model}]
                        [--tool-server TOOL_SERVER]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-eviction-policy {lru,fifo}]
                        [--lora-backend {triton,csgmv}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--sampling-backend {flashinfer,pytorch}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn}]
                        [--nsa-prefill {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--nsa-decode {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--enable-beta-spec]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--mamba-full-memory-ratio MAMBA_FULL_MEMORY_RATIO]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--radix-eviction-policy {lru,lfu}]
                        [--hicache-io-backend {direct,kernel}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE] [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-nccl-nvls] [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--enable-piecewise-cuda-graph]
                        [--piecewise-cuda-graph-tokens PIECEWISE_CUDA_GRAPH_TOKENS]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--piecewise-cuda-graph-max-tokens PIECEWISE_CUDA_GRAPH_MAX_TOKENS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup] [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--enable-pdmux]
                        [--pdmux-config-path PDMUX_CONFIG_PATH]
                        [--sm-group-num SM_GROUP_NUM]
                        [--enable-deterministic-inference] [--enable-ep-moe]
                        [--enable-deepep-moe]
                        [--enable-flashinfer-cutlass-moe]
                        [--enable-flashinfer-cutedsl-moe]
                        [--enable-flashinfer-trtllm-moe]
                        [--enable-triton-kernel-moe]
                        [--enable-flashinfer-mxfp4-moe] [--config CONFIG]
launch_server.py: error: argument --quantization: invalid choice: 'compressed-tensors' (choose from 'awq', 'fp8', 'gptq', 'marlin', 'gptq_marlin', 'awq_marlin', 'bitsandbytes', 'gguf', 'modelopt', 'modelopt_fp4', 'petit_nvfp4', 'w8a8_int8', 'w8a8_fp8', 'moe_wna16', 'qoq', 'w4afp8', 'mxfp4')
