nohup: ignoring input
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 10-13 17:19:45 [__init__.py:216] Automatically detected platform cuda.
[2025-10-13 17:19:47] server_args=ServerArgs(model_path='/home/ivan/models/Magnum-v4-123B-AWQ', tokenizer_path='/home/ivan/models/Magnum-v4-123B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=16384, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8003, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='float16', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.78, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=33432487, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='magnum-v4-123b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 17:19:47] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 17:19:47] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
INFO 10-13 17:19:58 [__init__.py:216] Automatically detected platform cuda.
INFO 10-13 17:19:58 [__init__.py:216] Automatically detected platform cuda.
INFO 10-13 17:19:58 [__init__.py:216] Automatically detected platform cuda.
INFO 10-13 17:19:58 [__init__.py:216] Automatically detected platform cuda.
INFO 10-13 17:19:58 [__init__.py:216] Automatically detected platform cuda.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 17:19:58 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 17:19:58 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 17:19:58 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 17:19:58 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 17:19:58 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 17:19:58 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 17:19:58 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 17:19:59 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 17:19:59 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 17:19:59 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 17:19:59 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 17:20:00 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 17:20:00 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 17:20:00 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 17:20:00 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 17:20:00 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 17:20:00 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 17:20:00 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 17:20:00 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 17:20:00 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 17:20:01 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:17<03:53, 17.93s/it]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:36<03:42, 18.57s/it]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:56<03:29, 19.07s/it]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [01:14<03:05, 18.57s/it]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [01:33<02:48, 18.69s/it]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [01:53<02:34, 19.31s/it]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [02:12<02:14, 19.18s/it]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [02:15<01:24, 14.05s/it]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [02:35<01:19, 15.95s/it]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [02:54<01:06, 16.74s/it]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [03:13<00:52, 17.35s/it]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [03:32<00:36, 18.01s/it]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [03:52<00:18, 18.61s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [04:12<00:00, 18.96s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [04:12<00:00, 18.03s/it]

[2025-10-13 17:24:16 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.91 GB, mem usage=15.20 GB.
[2025-10-13 17:24:16 TP0] Using KV cache dtype: torch.float16
[2025-10-13 17:24:16 TP2] KV Cache is allocated. #tokens: 33633, K size: 1.41 GB, V size: 1.41 GB
[2025-10-13 17:24:16 TP1] KV Cache is allocated. #tokens: 33633, K size: 1.41 GB, V size: 1.41 GB
[2025-10-13 17:24:16 TP0] KV Cache is allocated. #tokens: 33633, K size: 1.41 GB, V size: 1.41 GB
[2025-10-13 17:24:16 TP0] Memory pool end. avail mem=4.96 GB
[2025-10-13 17:24:16 TP3] KV Cache is allocated. #tokens: 33633, K size: 1.41 GB, V size: 1.41 GB
[2025-10-13 17:24:16 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=4.48 GB
[2025-10-13 17:24:16 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=4.48 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=4.48 GB):   7%|▋         | 1/14 [00:02<00:28,  2.21s/it]Capturing batches (bs=72 avail_mem=4.40 GB):   7%|▋         | 1/14 [00:02<00:28,  2.21s/it]Capturing batches (bs=72 avail_mem=4.40 GB):  14%|█▍        | 2/14 [00:02<00:15,  1.28s/it]Capturing batches (bs=64 avail_mem=4.39 GB):  14%|█▍        | 2/14 [00:02<00:15,  1.28s/it]Capturing batches (bs=64 avail_mem=4.39 GB):  21%|██▏       | 3/14 [00:03<00:09,  1.16it/s]Capturing batches (bs=56 avail_mem=4.38 GB):  21%|██▏       | 3/14 [00:03<00:09,  1.16it/s]Capturing batches (bs=56 avail_mem=4.38 GB):  29%|██▊       | 4/14 [00:03<00:06,  1.53it/s]Capturing batches (bs=48 avail_mem=4.35 GB):  29%|██▊       | 4/14 [00:03<00:06,  1.53it/s]Capturing batches (bs=48 avail_mem=4.35 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.91it/s]Capturing batches (bs=40 avail_mem=4.34 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.91it/s]Capturing batches (bs=40 avail_mem=4.34 GB):  43%|████▎     | 6/14 [00:04<00:03,  2.27it/s]Capturing batches (bs=32 avail_mem=4.33 GB):  43%|████▎     | 6/14 [00:04<00:03,  2.27it/s]Capturing batches (bs=32 avail_mem=4.33 GB):  50%|█████     | 7/14 [00:04<00:02,  2.66it/s]Capturing batches (bs=24 avail_mem=4.30 GB):  50%|█████     | 7/14 [00:04<00:02,  2.66it/s]Capturing batches (bs=24 avail_mem=4.30 GB):  57%|█████▋    | 8/14 [00:04<00:01,  3.05it/s]Capturing batches (bs=16 avail_mem=4.29 GB):  57%|█████▋    | 8/14 [00:04<00:01,  3.05it/s]Capturing batches (bs=16 avail_mem=4.29 GB):  64%|██████▍   | 9/14 [00:04<00:01,  3.42it/s]Capturing batches (bs=12 avail_mem=4.26 GB):  64%|██████▍   | 9/14 [00:04<00:01,  3.42it/s]Capturing batches (bs=12 avail_mem=4.26 GB):  71%|███████▏  | 10/14 [00:04<00:01,  3.84it/s]Capturing batches (bs=8 avail_mem=4.25 GB):  71%|███████▏  | 10/14 [00:04<00:01,  3.84it/s] Capturing batches (bs=8 avail_mem=4.25 GB):  79%|███████▊  | 11/14 [00:05<00:00,  4.28it/s]Capturing batches (bs=4 avail_mem=4.24 GB):  79%|███████▊  | 11/14 [00:05<00:00,  4.28it/s]Capturing batches (bs=4 avail_mem=4.24 GB):  86%|████████▌ | 12/14 [00:05<00:00,  4.63it/s]Capturing batches (bs=2 avail_mem=4.20 GB):  86%|████████▌ | 12/14 [00:05<00:00,  4.63it/s]Capturing batches (bs=2 avail_mem=4.20 GB):  93%|█████████▎| 13/14 [00:05<00:00,  4.91it/s]Capturing batches (bs=1 avail_mem=4.19 GB):  93%|█████████▎| 13/14 [00:05<00:00,  4.91it/s]Capturing batches (bs=1 avail_mem=4.19 GB): 100%|██████████| 14/14 [00:05<00:00,  3.56it/s]Capturing batches (bs=1 avail_mem=4.19 GB): 100%|██████████| 14/14 [00:05<00:00,  2.35it/s]
[2025-10-13 17:24:23 TP0] Capture cuda graph end. Time elapsed: 6.56 s. mem usage=0.32 GB. avail mem=4.16 GB.
[2025-10-13 17:24:23 TP0] max_total_num_tokens=33633, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=16384, available_gpu_mem=4.16 GB
[2025-10-13 17:24:23] INFO:     Started server process [8894]
[2025-10-13 17:24:23] INFO:     Waiting for application startup.
[2025-10-13 17:24:23] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 17:24:23] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 17:24:23] INFO:     Application startup complete.
[2025-10-13 17:24:23] INFO:     Uvicorn running on http://0.0.0.0:8003 (Press CTRL+C to quit)
[2025-10-13 17:24:24] INFO:     127.0.0.1:48628 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 17:24:24] Receive: obj=GenerateReqInput(rid='a5cb4c24d68f477bbc5774bb01948aab', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:24:24 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:24:26] Finish: obj=GenerateReqInput(rid='a5cb4c24d68f477bbc5774bb01948aab', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the largest city in', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 8407, 3758, 1065], 'meta_info': {'id': 'a5cb4c24d68f477bbc5774bb01948aab', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 2.488495349884033}}
[2025-10-13 17:24:26] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 17:24:26] The server is fired up and ready to roll!
[2025-10-13 17:24:51] INFO:     127.0.0.1:46082 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:24:53] INFO:     127.0.0.1:42200 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:24:54] INFO:     127.0.0.1:42210 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:25:14] Receive: obj=GenerateReqInput(rid='14f3aa8f3c9f4fe69e92047029c2aea0', text='Write a detailed essay about the history of artificial intelligence, covering key milestones from the 1950s to present day.', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:25:14 TP0] Prefill batch. #new-seq: 1, #new-token: 28, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:25:16 TP0] Decode batch. #running-req: 1, #token: 62, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.75, #queue-req: 0, 
[2025-10-13 17:25:19 TP0] Decode batch. #running-req: 1, #token: 102, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.99, #queue-req: 0, 
[2025-10-13 17:25:21 TP0] Decode batch. #running-req: 1, #token: 142, token usage: 0.00, cuda graph: True, gen throughput (token/s): 15.83, #queue-req: 0, 
[2025-10-13 17:25:24 TP0] Decode batch. #running-req: 1, #token: 182, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.89, #queue-req: 0, 
[2025-10-13 17:25:26 TP0] Decode batch. #running-req: 1, #token: 222, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.86, #queue-req: 0, 
[2025-10-13 17:25:29 TP0] Decode batch. #running-req: 1, #token: 262, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.82, #queue-req: 0, 
[2025-10-13 17:25:31 TP0] Decode batch. #running-req: 1, #token: 302, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.77, #queue-req: 0, 
[2025-10-13 17:25:34 TP0] Decode batch. #running-req: 1, #token: 342, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.71, #queue-req: 0, 
[2025-10-13 17:25:36 TP0] Decode batch. #running-req: 1, #token: 382, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.75, #queue-req: 0, 
[2025-10-13 17:25:39 TP0] Decode batch. #running-req: 1, #token: 422, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.73, #queue-req: 0, 
[2025-10-13 17:25:41 TP0] Decode batch. #running-req: 1, #token: 462, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.75, #queue-req: 0, 
[2025-10-13 17:25:44 TP0] Decode batch. #running-req: 1, #token: 502, token usage: 0.01, cuda graph: True, gen throughput (token/s): 15.77, #queue-req: 0, 
[2025-10-13 17:25:46 TP0] Decode batch. #running-req: 1, #token: 542, token usage: 0.02, cuda graph: True, gen throughput (token/s): 15.73, #queue-req: 0, 
[2025-10-13 17:25:49 TP0] Decode batch. #running-req: 1, #token: 582, token usage: 0.02, cuda graph: True, gen throughput (token/s): 15.67, #queue-req: 0, 
[2025-10-13 17:25:51 TP0] Decode batch. #running-req: 1, #token: 622, token usage: 0.02, cuda graph: True, gen throughput (token/s): 25.51, #queue-req: 0, 
[2025-10-13 17:25:52 TP0] Decode batch. #running-req: 1, #token: 662, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.77, #queue-req: 0, 
[2025-10-13 17:25:53 TP0] Decode batch. #running-req: 1, #token: 702, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.80, #queue-req: 0, 
[2025-10-13 17:25:54 TP0] Decode batch. #running-req: 1, #token: 742, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.81, #queue-req: 0, 
[2025-10-13 17:25:55 TP0] Decode batch. #running-req: 1, #token: 782, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.79, #queue-req: 0, 
[2025-10-13 17:25:56 TP0] Decode batch. #running-req: 1, #token: 822, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.80, #queue-req: 0, 
[2025-10-13 17:25:57] Finish: obj=GenerateReqInput(rid='14f3aa8f3c9f4fe69e92047029c2aea0', text='Write a detailed essay about the history of artificial intelligence, covering key milestones from the 1950s to present day.', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Discuss the major achievements, challenges faced, and future potential of AI.\n\nArtificial Intelligence: A Historical Overview\n\nArtificial Intelligence (AI) has undergone a remarkable evolution since its inception in the mid-20th century. This essay will explore the key milestones, achievements, challenges, and future potential of AI from the 1950s to the present day.\n\nThe Birth of AI (1950s-1960s)\nThe concept of AI was formally introduced at a conference at Dartmouth College in 1956. Pioneers such as John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon discussed the possibility of creating intelligent machines. In 1959, John McCarthy developed LISP, a programming language that became a key tool for AI research.\n\nEarly Successes and Challenges (1970s-1980s)\nThe 1970s saw the development of expert systems, which aimed to capture the knowledge and decision-making abilities of human experts in specific domains. One notable example was MYCIN, a system designed to assist physicians in diagnosing i' ... 'on-making, and addressing ethical concerns related to bias, privacy, and job displacement.\n\nLooking ahead, AI is poised to play a transformative role in shaping the future of work, society, and our understanding of intelligence itself. As AI systems become more sophisticated and ubiquitous, they will likely augment and collaborate with human intelligence in unprecedented ways. The continued development of AI will require a multidisciplinary approach, bringing together insights from computer science, cognitive science, philosophy, and other fields.\n\nIn conclusion, the history of AI is a testament to human ingenuity and the relentless pursuit of creating intelligent machines. From its humble beginnings to its current prominence, AI has revolutionized numerous domains and holds immense potential for the future. As we navigate the challenges and opportunities presented by this transformative technology, it is crucial to ensure that its development remains aligned with human values and benefits society as a whole.', 'output_ids': [29481, 1066, 2937, 2138, 29491, 4201, 13572, 1040, 3782, 28632, 29493, 11137, 13333, 29493, 1072, 4205, 5396, 1070, 16875, 29491, 781, 781, 11131, 15541, 23859, 29515, 1098, 24393, 6003, 2346, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 1684, 29489, 1306, 1032, 17727, 10963, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1619, 11128, 1390, 12186, 1040, 2713, 2888, 1142, 3170, 29493, 28632, 29493, 11137, 29493, 1072, 4205, 5396, 1070, 16875, 1245, 1040, 29473, 29508, 29542, 29550, 29502, 29481, 1066, 1040, 2937, 2138, 29491, 781, 781, 1782, 24094, 1070, 16875, 1093, 29508, 29542, 29550, 29502, 29481, 29501, 29508, 29542, 29552, 29502, 29481, 29499, 781, 1782, 6703, 1070, 16875, 1171, 26259, 8928, 1206, 1032, 10655, 1206, 1152, 1212, 20891, 6904, 1065, 29473, 29508, 29542, 29550, 29552, 29491, 1135, 3089, 1172, 2027, 1158, 2983, 10808, 4384, 29492, 29493, 2239, 9598, 1119, 27747, 29493, 21282, 1523, 27098, 3174, 29493, 1072, 27604, 2063, 18950, 10719, 1040, 10833, 1070, 7586, 19694, 12923, 29491, 1328, 29473, 29508, 29542, 29550, 29542, 29493, 2983, 10808, 4384, 29492, 6970, 1161, 2619, 29521, 29493, 1032, 17060, 4610, 1137, 4014, 1032, 2713, 4689, 1122, 16875, 4100, 29491, 781, 781, 29517, 2385, 17325, 1042, 1072, 16007, 1748, 1042, 1093, 29508, 29542, 29555, 29502, 29481, 29501, 29508, 29542, 29551, 29502, 29481, 29499, 781, 1782, 29473, 29508, 29542, 29555, 29502, 29481, 3440, 1040, 4867, 1070, 8351, 5686, 29493, 1458, 18707, 1066, 12087, 1040, 5556, 1072, 5929, 29501, 15495, 18634, 1070, 3698, 12493, 1065, 3716, 26766, 29491, 3155, 19993, 3525, 1171, 19525, 29511, 1543, 29493, 1032, 2355, 6450, 1066, 6799, 4024, 8787, 1065, 12776, 9229, 17759, 1693, 19025, 29491, 3761, 29493, 1040, 3703, 3056, 1042, 1070, 16875, 1422, 5794, 1054, 1254, 1040, 28333, 1137, 7586, 3720, 11663, 1171, 1448, 15129, 1589, 13503, 2422, 29491, 1183, 1113, 12509, 9307, 29507, 1070, 1040, 4677, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 14156, 1065, 12899, 1072, 2913, 3708, 1066, 1289, 3011, 14647, 29491, 781, 781, 1914, 3589, 1404, 1072, 14021, 18272, 1093, 29508, 29542, 29542, 29502, 29481, 29501, 29518, 29502, 29502, 29502, 29481, 29499, 781, 1425, 1040, 29473, 29508, 29542, 29542, 29502, 29481, 29493, 16875, 4100, 17386, 5851, 1032, 1448, 1946, 29501, 28137, 5199, 29491, 14021, 5936, 10572, 29493, 2027, 1158, 26494, 12935, 1072, 5929, 7867, 29493, 14786, 2925, 1030, 1404, 29491, 1328, 29473, 29508, 29542, 29542, 29555, 29493, 26477, 29510, 29481, 15740, 9604, 6842, 2050, 5787, 16999, 1765, 1587, 4255, 10316, 8489, 1411, 23733, 2157, 1394, 29493, 7463, 1845, 1040, 5396, 1070, 16875, 1065, 5398, 3468, 29501, 29481, 19158, 29491, 1183, 3703, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 9438, 2107, 1065, 4997, 4610, 10225, 29493, 19169, 1831, 29493, 1072, 6842, 8789, 29491, 781, 781, 23997, 18272, 1072, 1040, 16875, 4036, 1068, 1093, 29518, 29502, 29508, 29502, 29481, 29501, 20386, 29499, 781, 1782, 3377, 13018, 1427, 25153, 1164, 23026, 1065, 16875, 4100, 1072, 9197, 29493, 11392, 1054, 1254, 1040, 10755, 1070, 4302, 5936, 29491, 1619, 5199, 29493, 3586, 1124, 19046, 26494, 12935, 29493, 1427, 12779, 17727, 3056, 1065, 5788, 2027, 1158, 4237, 1072, 9434, 14596, 29493, 6367, 14614, 29493, 1072, 25777, 1375, 12767, 29491, 1328, 29473, 29518, 29502, 29508, 29508, 29493, 26477, 29510, 29481, 23371, 2355, 2515, 1040, 4059, 1178, 1256, 29492, 29576, 2807, 2115, 29493, 2115, 29485, 4474, 1040, 2750, 1070, 16875, 1065, 10225, 1072, 7167, 4997, 4610, 29491, 4532, 6683, 29493, 16875, 1427, 2037, 6632, 1885, 2694, 1065, 16008, 29493, 16746, 29493, 1072, 10579, 14248, 29493, 1163, 1040, 5396, 1066, 11797, 1421, 1935, 5848, 29491, 781, 781, 1977, 1223, 1748, 1042, 1072, 19010, 11418, 3024, 781, 3483, 5763, 1040, 17727, 5865, 29493, 16875, 2077, 10958, 6632, 11137, 29491, 3725, 3792, 1040, 1695, 1122, 1448, 9864, 1072, 20943, 5936, 19307, 29493, 2641, 15446, 1163, 3698, 18232, 1453, 1072, 5929, 29501, 15495, 29493, 1072, 25411, 27092, 11632, 5970, 1066, 16885, 29493, 13685, 29493, 1072, 3157, 14188, 7235, 29491, 781, 781, 10059, 1056, 7048, 29493, 16875, 1117, 2395, 3226, 1066, 1924, 1032, 6284, 1965, 4673, 1065, 1248, 14655, 1040, 4205, 1070, 1539, 29493, 6958, 29493, 1072, 1581, 7167, 1070, 11663, 4605, 29491, 1904, 16875, 5686, 3495, 1448, 23981, 1072, 16947, 3509, 1047, 1375, 29493, 1358, 1390, 4685, 21604, 1072, 9016, 1148, 1163, 3698, 11663, 1065, 27409, 2122, 13481, 5110, 29491, 1183, 6085, 4867, 1070, 16875, 1390, 3467, 1032, 3299, 1081, 1046, 9703, 4007, 5199, 29493, 11047, 3321, 21483, 1245, 6842, 7459, 29493, 26514, 7459, 29493, 14563, 29493, 1072, 1567, 5848, 29491, 781, 781, 1425, 13654, 29493, 1040, 4108, 1070, 16875, 1117, 1032, 2137, 2454, 1066, 3698, 4923, 5033, 1240, 1072, 1040, 1784, 1076, 2271, 25577, 1070, 7586, 19694, 12923, 29491, 4440, 1639, 2765, 1750, 2817, 22311, 1066, 1639, 2636, 2925, 1030, 1404, 29493, 16875, 1427, 11797, 2100, 10464, 26766, 1072, 9056, 27259, 5396, 1122, 1040, 4205, 29491, 1904, 1246, 23457, 1040, 11137, 1072, 9580, 8335, 1254, 1224, 6284, 1965, 6282, 29493, 1146, 1117, 13808, 1066, 6175, 1137, 1639, 4867, 8288, 24349, 1163, 3698, 3837, 1072, 7964, 6958, 1158, 1032, 3662, 29491, 2], 'meta_info': {'id': '14f3aa8f3c9f4fe69e92047029c2aea0', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 29, 'weight_version': 'default', 'completion_tokens': 817, 'cached_tokens': 1, 'e2e_latency': 43.09834599494934}}
[2025-10-13 17:25:57] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 17:32:48] INFO:     127.0.0.1:35114 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:32:48] INFO:     127.0.0.1:35124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 17:32:48] Receive: obj=GenerateReqInput(rid='5150dad497f94e1e94fdd2a294ff5864', text=None, input_ids=[1, 3, 2015, 15036, 29515, 781, 781, 781, 781, 1734, 20225, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:32:48 TP0] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:32:48 TP0] Decode batch. #running-req: 1, #token: 44, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.10, #queue-req: 0, 
[2025-10-13 17:32:49 TP0] Decode batch. #running-req: 1, #token: 84, token usage: 0.00, cuda graph: True, gen throughput (token/s): 35.95, #queue-req: 0, 
[2025-10-13 17:32:50 TP0] Decode batch. #running-req: 1, #token: 124, token usage: 0.00, cuda graph: True, gen throughput (token/s): 36.03, #queue-req: 0, 
[2025-10-13 17:32:52 TP0] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 35.92, #queue-req: 0, 
[2025-10-13 17:32:53 TP0] Decode batch. #running-req: 1, #token: 204, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.88, #queue-req: 0, 
[2025-10-13 17:32:54 TP0] Decode batch. #running-req: 1, #token: 244, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.88, #queue-req: 0, 
[2025-10-13 17:32:55 TP0] Decode batch. #running-req: 1, #token: 284, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.88, #queue-req: 0, 
[2025-10-13 17:32:56 TP0] Decode batch. #running-req: 1, #token: 324, token usage: 0.01, cuda graph: True, gen throughput (token/s): 35.88, #queue-req: 0, 
[2025-10-13 17:32:57] Finish: obj=GenerateReqInput(rid='5150dad497f94e1e94fdd2a294ff5864', text=None, input_ids=[1, 3, 2015, 15036, 29515, 781, 781, 781, 781, 1734, 20225, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' With options trading, instead of buying or selling shares of stock directly, you are buying or selling contracts that give you the right, but not the obligation, to buy (in the case of a "call" option) or sell (in the case of a "put" option) a specified number of shares of a stock at a predetermined price (the "strike price") on or before a certain date (the expiration date). \n\nThe key aspects are:\n\n1. Premium: You pay a price per share (the "premium") to purchase the option contract. This is the most you can lose on the trade.\n\n2. Strike Price: The price at which you can exercise your right to buy (for a call) or sell (for a put) the underlying stock if the option reaches expiration "in the money." \n\n3. Expiration Date: The date on which the option contract expires. If the option is not in the money at expiration, it becomes worthless.\n\n4. Going Long or Short: You can buy (go long) options, betting the stock price will go up (with calls) or down (with puts). Or you can sell/write (go short) options, betting the stock price will stay steady or go the opposite direction.\n\nOptions allow you to leverage your investing dollars for big potential profits, but also carry substantial risk of loss. It\'s important to carefully evaluate your risk tolerance before engaging in options trading.', 'output_ids': [2], 'meta_info': {'id': '5150dad497f94e1e94fdd2a294ff5864', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 28, 'weight_version': 'default', 'completion_tokens': 317, 'cached_tokens': 1, 'e2e_latency': 8.939878225326538}}
[2025-10-13 17:32:57] INFO:     127.0.0.1:36440 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:32:57] Receive: obj=GenerateReqInput(rid='a4ec92e9a1e44af3a401ba64adbfbec7', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 14470, 1194, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 781, 5584, 8816, 13506, 29515, 3094, 3645, 12442, 29493, 4287, 1070, 10505, 1210, 10926, 14076, 1070, 6524, 5858, 29493, 1136, 1228, 10505, 1210, 10926, 19946, 1137, 2879, 1136, 1040, 1871, 29493, 1330, 1227, 1040, 25543, 29493, 1066, 4616, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 3613, 29507, 4319, 29499, 1210, 6880, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 1527, 29507, 4319, 29499, 1032, 6908, 2242, 1070, 14076, 1070, 1032, 6524, 1206, 1032, 1478, 5469, 1626, 2079, 4912, 1093, 2005, 1113, 1071, 20533, 4912, 2011, 1124, 1210, 1927, 1032, 3320, 4376, 1093, 2005, 3133, 9447, 4376, 1377, 29473, 781, 781, 1782, 2713, 11704, 1228, 29515, 781, 781, 29508, 29491, 8577, 2730, 29515, 1763, 2904, 1032, 4912, 1428, 4866, 1093, 2005, 1113, 29488, 2337, 2730, 2011, 1066, 8493, 1040, 4319, 5769, 29491, 1619, 1117, 1040, 1848, 1136, 1309, 7556, 1124, 1040, 6994, 29491, 781, 781, 29518, 29491, 1430, 20533, 12357, 29515, 1183, 4912, 1206, 1458, 1136, 1309, 9863, 1342, 1871, 1066, 4616, 1093, 2160, 1032, 1802, 29499, 1210, 6880, 1093, 2160, 1032, 2426, 29499, 1040, 14932, 6524, 1281, 1040, 4319, 19478, 3133, 9447, 1113, 1030, 1040, 3213, 1379, 29473, 781, 781, 29538, 29491, 17558, 9447, 6259, 29515, 1183, 4376, 1124, 1458, 1040, 4319, 5769, 1207, 21238, 29491, 1815, 1040, 4319, 1117, 1227, 1065, 1040, 3213, 1206, 3133, 9447, 29493, 1146, 7523, 5175, 2271, 29491, 781, 781, 29549, 29491, 26427, 7196, 1210, 12298, 29515, 1763, 1309, 4616, 1093, 2412, 1811, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 1344, 1350, 1093, 4183, 7238, 29499, 1210, 1828, 1093, 4183, 13113, 1377, 2435, 1136, 1309, 6880, 29516, 3803, 1093, 2412, 3253, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 3847, 15657, 1210, 1344, 1040, 9857, 5775, 29491, 781, 781, 4786, 2682, 1136, 1066, 29152, 1342, 23213, 10175, 1122, 2800, 5396, 21053, 29493, 1330, 1603, 7864, 15813, 5391, 1070, 5088, 29491, 1429, 29510, 29481, 3046, 1066, 10315, 16395, 1342, 5391, 28085, 1927, 20407, 1065, 3645, 12442, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:32:57 TP0] Prefill batch. #new-seq: 1, #new-token: 573, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:32:58 TP0] Decode batch. #running-req: 1, #token: 594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 17.04, #queue-req: 0, 
[2025-10-13 17:32:59 TP0] Decode batch. #running-req: 1, #token: 634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.85, #queue-req: 0, 
[2025-10-13 17:33:00] Finish: obj=GenerateReqInput(rid='a4ec92e9a1e44af3a401ba64adbfbec7', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 14470, 1194, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 781, 5584, 8816, 13506, 29515, 3094, 3645, 12442, 29493, 4287, 1070, 10505, 1210, 10926, 14076, 1070, 6524, 5858, 29493, 1136, 1228, 10505, 1210, 10926, 19946, 1137, 2879, 1136, 1040, 1871, 29493, 1330, 1227, 1040, 25543, 29493, 1066, 4616, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 3613, 29507, 4319, 29499, 1210, 6880, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 1527, 29507, 4319, 29499, 1032, 6908, 2242, 1070, 14076, 1070, 1032, 6524, 1206, 1032, 1478, 5469, 1626, 2079, 4912, 1093, 2005, 1113, 1071, 20533, 4912, 2011, 1124, 1210, 1927, 1032, 3320, 4376, 1093, 2005, 3133, 9447, 4376, 1377, 29473, 781, 781, 1782, 2713, 11704, 1228, 29515, 781, 781, 29508, 29491, 8577, 2730, 29515, 1763, 2904, 1032, 4912, 1428, 4866, 1093, 2005, 1113, 29488, 2337, 2730, 2011, 1066, 8493, 1040, 4319, 5769, 29491, 1619, 1117, 1040, 1848, 1136, 1309, 7556, 1124, 1040, 6994, 29491, 781, 781, 29518, 29491, 1430, 20533, 12357, 29515, 1183, 4912, 1206, 1458, 1136, 1309, 9863, 1342, 1871, 1066, 4616, 1093, 2160, 1032, 1802, 29499, 1210, 6880, 1093, 2160, 1032, 2426, 29499, 1040, 14932, 6524, 1281, 1040, 4319, 19478, 3133, 9447, 1113, 1030, 1040, 3213, 1379, 29473, 781, 781, 29538, 29491, 17558, 9447, 6259, 29515, 1183, 4376, 1124, 1458, 1040, 4319, 5769, 1207, 21238, 29491, 1815, 1040, 4319, 1117, 1227, 1065, 1040, 3213, 1206, 3133, 9447, 29493, 1146, 7523, 5175, 2271, 29491, 781, 781, 29549, 29491, 26427, 7196, 1210, 12298, 29515, 1763, 1309, 4616, 1093, 2412, 1811, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 1344, 1350, 1093, 4183, 7238, 29499, 1210, 1828, 1093, 4183, 13113, 1377, 2435, 1136, 1309, 6880, 29516, 3803, 1093, 2412, 3253, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 3847, 15657, 1210, 1344, 1040, 9857, 5775, 29491, 781, 781, 4786, 2682, 1136, 1066, 29152, 1342, 23213, 10175, 1122, 2800, 5396, 21053, 29493, 1330, 1603, 7864, 15813, 5391, 1070, 5088, 29491, 1429, 29510, 29481, 3046, 1066, 10315, 16395, 1342, 5391, 28085, 1927, 20407, 1065, 3645, 12442, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' { \n  "follow_ups": [\n    "What are some specific strategies used in options trading?",\n    "How does buying a call option compare to simply buying shares of stock?", \n    "What are the main risks to consider with options trading versus investing in stocks?",\n    "Can you give me an example of when it would be beneficial to buy a put option?"\n  ]\n}', 'output_ids': [13700, 29498, 15973, 29535, 4, 1139, 29473, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 3963, 1228, 1509, 3716, 12876, 2075, 1065, 3645, 12442, 29572, 1316, 781, 3055, 1113, 6428, 2003, 10505, 1032, 1802, 4319, 10352, 1066, 4536, 10505, 14076, 1070, 6524, 29572, 1316, 29473, 781, 3055, 1113, 3963, 1228, 1040, 2959, 15447, 1066, 2685, 1163, 3645, 12442, 17831, 23213, 1065, 22549, 29572, 1316, 781, 3055, 1113, 7093, 1136, 2879, 1296, 1164, 3525, 1070, 1507, 1146, 1450, 1115, 21926, 1066, 4616, 1032, 2426, 4319, 1878, 781, 29473, 5477, 781, 29520, 2], 'meta_info': {'id': 'a4ec92e9a1e44af3a401ba64adbfbec7', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 575, 'weight_version': 'default', 'completion_tokens': 89, 'cached_tokens': 2, 'e2e_latency': 3.7014827728271484}}
[2025-10-13 17:33:00] INFO:     127.0.0.1:36456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 17:33:00] INFO:     127.0.0.1:36460 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:33:00] Receive: obj=GenerateReqInput(rid='c3c1109b4a5c4745b13a16f876c53f26', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 24110, 1032, 3846, 1632, 29493, 29473, 29538, 29501, 29550, 2475, 4709, 1163, 1164, 1645, 28581, 18830, 4632, 1040, 11474, 4108, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 1183, 4709, 1791, 7083, 3672, 1040, 2959, 8103, 1210, 4585, 1070, 1040, 7882, 29491, 781, 29501, 6706, 1645, 6822, 1046, 1137, 12744, 7167, 1070, 1040, 9835, 29493, 1330, 5229, 18296, 1120, 14959, 1210, 3609, 1989, 15526, 29491, 781, 29501, 12786, 1040, 4709, 1065, 1040, 11474, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 21612, 23943, 14020, 1522, 24981, 23085, 29513, 2608, 1146, 3849, 1072, 4356, 29491, 781, 29501, 4372, 4061, 3667, 2348, 5585, 20318, 1070, 1040, 10060, 2696, 29493, 2439, 1475, 25439, 3246, 1463, 1210, 6001, 9923, 3013, 29491, 781, 29501, 1183, 4593, 2348, 1115, 1032, 3460, 29493, 9203, 10060, 2696, 29493, 2439, 1475, 2484, 4022, 3464, 1053, 2887, 1210, 1567, 3292, 2650, 14390, 3013, 29491, 781, 29501, 25985, 1476, 6083, 2017, 3013, 29493, 26526, 1465, 29493, 1210, 11696, 1465, 12173, 29474, 1210, 2140, 1040, 9203, 10060, 4593, 29493, 1158, 1224, 1390, 5012, 2631, 1708, 12369, 8487, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 4669, 2032, 1113, 20030, 3846, 1632, 4709, 2004, 29507, 1211, 781, 28100, 27036, 29515, 781, 29501, 1139, 1113, 4669, 2032, 1113, 1011, 930, 918, 908, 12316, 12411, 1088, 5850, 29481, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 1011, 930, 912, 941, 25211, 1457, 12727, 1457, 1276, 4291, 4753, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 9227, 2868, 1070, 8530, 16683, 1056, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 16246, 5834, 9836, 3342, 27174, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 11131, 15541, 23859, 1065, 7145, 8769, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 1011, 930, 913, 945, 13041, 9047, 11108, 10281, 3920, 29507, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 14470, 1194, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 781, 5584, 8816, 13506, 29515, 3094, 3645, 12442, 29493, 4287, 1070, 10505, 1210, 10926, 14076, 1070, 6524, 5858, 29493, 1136, 1228, 10505, 1210, 10926, 19946, 1137, 2879, 1136, 1040, 1871, 29493, 1330, 1227, 1040, 25543, 29493, 1066, 4616, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 3613, 29507, 4319, 29499, 1210, 6880, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 1527, 29507, 4319, 29499, 1032, 6908, 2242, 1070, 14076, 1070, 1032, 6524, 1206, 1032, 1478, 5469, 1626, 2079, 4912, 1093, 2005, 1113, 1071, 20533, 4912, 2011, 1124, 1210, 1927, 1032, 3320, 4376, 1093, 2005, 3133, 9447, 4376, 1377, 29473, 781, 781, 1782, 2713, 11704, 1228, 29515, 781, 781, 29508, 29491, 8577, 2730, 29515, 1763, 2904, 1032, 4912, 1428, 4866, 1093, 2005, 1113, 29488, 2337, 2730, 2011, 1066, 8493, 1040, 4319, 5769, 29491, 1619, 1117, 1040, 1848, 1136, 1309, 7556, 1124, 1040, 6994, 29491, 781, 781, 29518, 29491, 1430, 20533, 12357, 29515, 1183, 4912, 1206, 1458, 1136, 1309, 9863, 1342, 1871, 1066, 4616, 1093, 2160, 1032, 1802, 29499, 1210, 6880, 1093, 2160, 1032, 2426, 29499, 1040, 14932, 6524, 1281, 1040, 4319, 19478, 3133, 9447, 1113, 1030, 1040, 3213, 1379, 29473, 781, 781, 29538, 29491, 17558, 9447, 6259, 29515, 1183, 4376, 1124, 1458, 1040, 4319, 5769, 1207, 21238, 29491, 1815, 1040, 4319, 1117, 1227, 1065, 1040, 3213, 1206, 3133, 9447, 29493, 1146, 7523, 5175, 2271, 29491, 781, 781, 29549, 29491, 26427, 7196, 1210, 12298, 29515, 1763, 1309, 4616, 1093, 2412, 1811, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 1344, 1350, 1093, 4183, 7238, 29499, 1210, 1828, 1093, 4183, 13113, 1377, 2435, 1136, 1309, 6880, 29516, 3803, 1093, 2412, 3253, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 3847, 15657, 1210, 1344, 1040, 9857, 5775, 29491, 781, 781, 4786, 2682, 1136, 1066, 29152, 1342, 23213, 10175, 1122, 2800, 5396, 21053, 29493, 1330, 1603, 7864, 15813, 5391, 1070, 5088, 29491, 1429, 29510, 29481, 3046, 1066, 10315, 16395, 1342, 5391, 28085, 1927, 20407, 1065, 3645, 12442, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': 1000, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:33:00 TP0] Prefill batch. #new-seq: 1, #new-token: 667, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:33:02 TP0] Decode batch. #running-req: 1, #token: 683, token usage: 0.02, cuda graph: True, gen throughput (token/s): 15.30, #queue-req: 0, 
[2025-10-13 17:33:02] Finish: obj=GenerateReqInput(rid='c3c1109b4a5c4745b13a16f876c53f26', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 24110, 1032, 3846, 1632, 29493, 29473, 29538, 29501, 29550, 2475, 4709, 1163, 1164, 1645, 28581, 18830, 4632, 1040, 11474, 4108, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 1183, 4709, 1791, 7083, 3672, 1040, 2959, 8103, 1210, 4585, 1070, 1040, 7882, 29491, 781, 29501, 6706, 1645, 6822, 1046, 1137, 12744, 7167, 1070, 1040, 9835, 29493, 1330, 5229, 18296, 1120, 14959, 1210, 3609, 1989, 15526, 29491, 781, 29501, 12786, 1040, 4709, 1065, 1040, 11474, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 21612, 23943, 14020, 1522, 24981, 23085, 29513, 2608, 1146, 3849, 1072, 4356, 29491, 781, 29501, 4372, 4061, 3667, 2348, 5585, 20318, 1070, 1040, 10060, 2696, 29493, 2439, 1475, 25439, 3246, 1463, 1210, 6001, 9923, 3013, 29491, 781, 29501, 1183, 4593, 2348, 1115, 1032, 3460, 29493, 9203, 10060, 2696, 29493, 2439, 1475, 2484, 4022, 3464, 1053, 2887, 1210, 1567, 3292, 2650, 14390, 3013, 29491, 781, 29501, 25985, 1476, 6083, 2017, 3013, 29493, 26526, 1465, 29493, 1210, 11696, 1465, 12173, 29474, 1210, 2140, 1040, 9203, 10060, 4593, 29493, 1158, 1224, 1390, 5012, 2631, 1708, 12369, 8487, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 4669, 2032, 1113, 20030, 3846, 1632, 4709, 2004, 29507, 1211, 781, 28100, 27036, 29515, 781, 29501, 1139, 1113, 4669, 2032, 1113, 1011, 930, 918, 908, 12316, 12411, 1088, 5850, 29481, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 1011, 930, 912, 941, 25211, 1457, 12727, 1457, 1276, 4291, 4753, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 9227, 2868, 1070, 8530, 16683, 1056, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 16246, 5834, 9836, 3342, 27174, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 11131, 15541, 23859, 1065, 7145, 8769, 29507, 2398, 781, 29501, 1139, 1113, 4669, 2032, 1113, 1011, 930, 913, 945, 13041, 9047, 11108, 10281, 3920, 29507, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 14470, 1194, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 781, 5584, 8816, 13506, 29515, 3094, 3645, 12442, 29493, 4287, 1070, 10505, 1210, 10926, 14076, 1070, 6524, 5858, 29493, 1136, 1228, 10505, 1210, 10926, 19946, 1137, 2879, 1136, 1040, 1871, 29493, 1330, 1227, 1040, 25543, 29493, 1066, 4616, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 3613, 29507, 4319, 29499, 1210, 6880, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 1527, 29507, 4319, 29499, 1032, 6908, 2242, 1070, 14076, 1070, 1032, 6524, 1206, 1032, 1478, 5469, 1626, 2079, 4912, 1093, 2005, 1113, 1071, 20533, 4912, 2011, 1124, 1210, 1927, 1032, 3320, 4376, 1093, 2005, 3133, 9447, 4376, 1377, 29473, 781, 781, 1782, 2713, 11704, 1228, 29515, 781, 781, 29508, 29491, 8577, 2730, 29515, 1763, 2904, 1032, 4912, 1428, 4866, 1093, 2005, 1113, 29488, 2337, 2730, 2011, 1066, 8493, 1040, 4319, 5769, 29491, 1619, 1117, 1040, 1848, 1136, 1309, 7556, 1124, 1040, 6994, 29491, 781, 781, 29518, 29491, 1430, 20533, 12357, 29515, 1183, 4912, 1206, 1458, 1136, 1309, 9863, 1342, 1871, 1066, 4616, 1093, 2160, 1032, 1802, 29499, 1210, 6880, 1093, 2160, 1032, 2426, 29499, 1040, 14932, 6524, 1281, 1040, 4319, 19478, 3133, 9447, 1113, 1030, 1040, 3213, 1379, 29473, 781, 781, 29538, 29491, 17558, 9447, 6259, 29515, 1183, 4376, 1124, 1458, 1040, 4319, 5769, 1207, 21238, 29491, 1815, 1040, 4319, 1117, 1227, 1065, 1040, 3213, 1206, 3133, 9447, 29493, 1146, 7523, 5175, 2271, 29491, 781, 781, 29549, 29491, 26427, 7196, 1210, 12298, 29515, 1763, 1309, 4616, 1093, 2412, 1811, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 1344, 1350, 1093, 4183, 7238, 29499, 1210, 1828, 1093, 4183, 13113, 1377, 2435, 1136, 1309, 6880, 29516, 3803, 1093, 2412, 3253, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 3847, 15657, 1210, 1344, 1040, 9857, 5775, 29491, 781, 781, 4786, 2682, 1136, 1066, 29152, 1342, 23213, 10175, 1122, 2800, 5396, 21053, 29493, 1330, 1603, 7864, 15813, 5391, 1070, 5088, 29491, 1429, 29510, 29481, 3046, 1066, 10315, 16395, 1342, 5391, 28085, 1927, 20407, 1065, 3645, 12442, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': 1000, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' { "title": "📊 Options Trading Explained" }', 'output_ids': [13700, 29498, 15973, 29535, 4, 1139, 1113, 4669, 2032, 1113, 1011, 930, 918, 909, 20409, 2087, 8604, 14470, 2506, 29507, 1211, 2], 'meta_info': {'id': 'c3c1109b4a5c4745b13a16f876c53f26', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 673, 'weight_version': 'default', 'completion_tokens': 17, 'cached_tokens': 6, 'e2e_latency': 1.9034619331359863}}
[2025-10-13 17:33:02] INFO:     127.0.0.1:36472 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 17:33:02] INFO:     127.0.0.1:41884 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:33:02] Receive: obj=GenerateReqInput(rid='c6489b92b41a48709026ec5bd3a95100', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 24110, 29473, 29508, 29501, 29538, 6609, 13712, 21345, 4632, 1040, 2959, 19746, 1070, 1040, 11474, 4108, 29493, 3035, 1163, 29473, 29508, 29501, 29538, 1448, 3716, 1851, 18601, 13712, 29491, 781, 781, 28100, 3248, 15787, 29515, 781, 29501, 7811, 1163, 2254, 29501, 5172, 26766, 1093, 29474, 29491, 29489, 29491, 10091, 29493, 13279, 29493, 22924, 29492, 29493, 12031, 29493, 26662, 29493, 8479, 29493, 7145, 29493, 13952, 29493, 20982, 29493, 10120, 29499, 781, 29501, 12540, 3258, 9366, 1851, 8202, 29516, 2434, 3688, 2374, 1281, 1358, 1228, 13055, 11251, 6241, 1040, 7882, 781, 29501, 1815, 3804, 1117, 2136, 3253, 1093, 2271, 1589, 29473, 29538, 9338, 29499, 1210, 2136, 13604, 29493, 1706, 1633, 8135, 16116, 3010, 781, 29501, 6706, 1040, 11474, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 781, 29501, 21612, 23943, 14020, 1522, 3716, 1240, 781, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 13354, 2032, 8135, 4779, 29508, 1316, 1113, 4779, 29518, 1316, 1113, 4779, 29538, 3010, 1211, 781, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 14470, 1194, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 781, 5584, 8816, 13506, 29515, 3094, 3645, 12442, 29493, 4287, 1070, 10505, 1210, 10926, 14076, 1070, 6524, 5858, 29493, 1136, 1228, 10505, 1210, 10926, 19946, 1137, 2879, 1136, 1040, 1871, 29493, 1330, 1227, 1040, 25543, 29493, 1066, 4616, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 3613, 29507, 4319, 29499, 1210, 6880, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 1527, 29507, 4319, 29499, 1032, 6908, 2242, 1070, 14076, 1070, 1032, 6524, 1206, 1032, 1478, 5469, 1626, 2079, 4912, 1093, 2005, 1113, 1071, 20533, 4912, 2011, 1124, 1210, 1927, 1032, 3320, 4376, 1093, 2005, 3133, 9447, 4376, 1377, 29473, 781, 781, 1782, 2713, 11704, 1228, 29515, 781, 781, 29508, 29491, 8577, 2730, 29515, 1763, 2904, 1032, 4912, 1428, 4866, 1093, 2005, 1113, 29488, 2337, 2730, 2011, 1066, 8493, 1040, 4319, 5769, 29491, 1619, 1117, 1040, 1848, 1136, 1309, 7556, 1124, 1040, 6994, 29491, 781, 781, 29518, 29491, 1430, 20533, 12357, 29515, 1183, 4912, 1206, 1458, 1136, 1309, 9863, 1342, 1871, 1066, 4616, 1093, 2160, 1032, 1802, 29499, 1210, 6880, 1093, 2160, 1032, 2426, 29499, 1040, 14932, 6524, 1281, 1040, 4319, 19478, 3133, 9447, 1113, 1030, 1040, 3213, 1379, 29473, 781, 781, 29538, 29491, 17558, 9447, 6259, 29515, 1183, 4376, 1124, 1458, 1040, 4319, 5769, 1207, 21238, 29491, 1815, 1040, 4319, 1117, 1227, 1065, 1040, 3213, 1206, 3133, 9447, 29493, 1146, 7523, 5175, 2271, 29491, 781, 781, 29549, 29491, 26427, 7196, 1210, 12298, 29515, 1763, 1309, 4616, 1093, 2412, 1811, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 1344, 1350, 1093, 4183, 7238, 29499, 1210, 1828, 1093, 4183, 13113, 1377, 2435, 1136, 1309, 6880, 29516, 3803, 1093, 2412, 3253, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 3847, 15657, 1210, 1344, 1040, 9857, 5775, 29491, 781, 781, 4786, 2682, 1136, 1066, 29152, 1342, 23213, 10175, 1122, 2800, 5396, 21053, 29493, 1330, 1603, 7864, 15813, 5391, 1070, 5088, 29491, 1429, 29510, 29481, 3046, 1066, 10315, 16395, 1342, 5391, 28085, 1927, 20407, 1065, 3645, 12442, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:33:02 TP0] Prefill batch. #new-seq: 1, #new-token: 522, #cached-token: 7, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:33:04 TP0] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 17.79, #queue-req: 0, 
[2025-10-13 17:33:04] Finish: obj=GenerateReqInput(rid='c6489b92b41a48709026ec5bd3a95100', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 24110, 29473, 29508, 29501, 29538, 6609, 13712, 21345, 4632, 1040, 2959, 19746, 1070, 1040, 11474, 4108, 29493, 3035, 1163, 29473, 29508, 29501, 29538, 1448, 3716, 1851, 18601, 13712, 29491, 781, 781, 28100, 3248, 15787, 29515, 781, 29501, 7811, 1163, 2254, 29501, 5172, 26766, 1093, 29474, 29491, 29489, 29491, 10091, 29493, 13279, 29493, 22924, 29492, 29493, 12031, 29493, 26662, 29493, 8479, 29493, 7145, 29493, 13952, 29493, 20982, 29493, 10120, 29499, 781, 29501, 12540, 3258, 9366, 1851, 8202, 29516, 2434, 3688, 2374, 1281, 1358, 1228, 13055, 11251, 6241, 1040, 7882, 781, 29501, 1815, 3804, 1117, 2136, 3253, 1093, 2271, 1589, 29473, 29538, 9338, 29499, 1210, 2136, 13604, 29493, 1706, 1633, 8135, 16116, 3010, 781, 29501, 6706, 1040, 11474, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 781, 29501, 21612, 23943, 14020, 1522, 3716, 1240, 781, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 13354, 2032, 8135, 4779, 29508, 1316, 1113, 4779, 29518, 1316, 1113, 4779, 29538, 3010, 1211, 781, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 14470, 1194, 3645, 12442, 1065, 4356, 4239, 1281, 1083, 29510, 29487, 8996, 1163, 10505, 1072, 10926, 22549, 29491, 781, 5584, 8816, 13506, 29515, 3094, 3645, 12442, 29493, 4287, 1070, 10505, 1210, 10926, 14076, 1070, 6524, 5858, 29493, 1136, 1228, 10505, 1210, 10926, 19946, 1137, 2879, 1136, 1040, 1871, 29493, 1330, 1227, 1040, 25543, 29493, 1066, 4616, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 3613, 29507, 4319, 29499, 1210, 6880, 1093, 1030, 1040, 1990, 1070, 1032, 1113, 1527, 29507, 4319, 29499, 1032, 6908, 2242, 1070, 14076, 1070, 1032, 6524, 1206, 1032, 1478, 5469, 1626, 2079, 4912, 1093, 2005, 1113, 1071, 20533, 4912, 2011, 1124, 1210, 1927, 1032, 3320, 4376, 1093, 2005, 3133, 9447, 4376, 1377, 29473, 781, 781, 1782, 2713, 11704, 1228, 29515, 781, 781, 29508, 29491, 8577, 2730, 29515, 1763, 2904, 1032, 4912, 1428, 4866, 1093, 2005, 1113, 29488, 2337, 2730, 2011, 1066, 8493, 1040, 4319, 5769, 29491, 1619, 1117, 1040, 1848, 1136, 1309, 7556, 1124, 1040, 6994, 29491, 781, 781, 29518, 29491, 1430, 20533, 12357, 29515, 1183, 4912, 1206, 1458, 1136, 1309, 9863, 1342, 1871, 1066, 4616, 1093, 2160, 1032, 1802, 29499, 1210, 6880, 1093, 2160, 1032, 2426, 29499, 1040, 14932, 6524, 1281, 1040, 4319, 19478, 3133, 9447, 1113, 1030, 1040, 3213, 1379, 29473, 781, 781, 29538, 29491, 17558, 9447, 6259, 29515, 1183, 4376, 1124, 1458, 1040, 4319, 5769, 1207, 21238, 29491, 1815, 1040, 4319, 1117, 1227, 1065, 1040, 3213, 1206, 3133, 9447, 29493, 1146, 7523, 5175, 2271, 29491, 781, 781, 29549, 29491, 26427, 7196, 1210, 12298, 29515, 1763, 1309, 4616, 1093, 2412, 1811, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 1344, 1350, 1093, 4183, 7238, 29499, 1210, 1828, 1093, 4183, 13113, 1377, 2435, 1136, 1309, 6880, 29516, 3803, 1093, 2412, 3253, 29499, 3645, 29493, 23335, 1040, 6524, 4912, 1390, 3847, 15657, 1210, 1344, 1040, 9857, 5775, 29491, 781, 781, 4786, 2682, 1136, 1066, 29152, 1342, 23213, 10175, 1122, 2800, 5396, 21053, 29493, 1330, 1603, 7864, 15813, 5391, 1070, 5088, 29491, 1429, 29510, 29481, 3046, 1066, 10315, 16395, 1342, 5391, 28085, 1927, 20407, 1065, 3645, 12442, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' {\n  "tags": [\n    "Finance",\n    "Investing",\n    "Options Trading"\n  ]\n}', 'output_ids': [13700, 29498, 15973, 29535, 4, 1139, 781, 29473, 1113, 13354, 2032, 1501, 781, 3055, 1113, 12259, 1385, 1316, 781, 3055, 1113, 1425, 7049, 1056, 1316, 781, 3055, 1113, 4786, 2087, 8604, 29507, 781, 29473, 5477, 781, 29520, 2], 'meta_info': {'id': 'c6489b92b41a48709026ec5bd3a95100', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 529, 'weight_version': 'default', 'completion_tokens': 33, 'cached_tokens': 7, 'e2e_latency': 2.0384457111358643}}
[2025-10-13 17:33:04] INFO:     127.0.0.1:41890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 17:34:09] INFO:     127.0.0.1:59730 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 17:49:35] Receive: obj=GenerateReqInput(rid='dacc77d2999049e7b82f8040c8ce21da', text='User: Say hello in exactly 5 words.\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 50}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:49:35 TP0] Prefill batch. #new-seq: 1, #new-token: 16, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:49:35] Finish: obj=GenerateReqInput(rid='dacc77d2999049e7b82f8040c8ce21da', text='User: Say hello in exactly 5 words.\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 50}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Hello, how can I assist?', 'output_ids': [781, 781, 7994, 11911, 29515, 23325, 29493, 1678, 1309, 1083, 6799, 29572, 2], 'meta_info': {'id': 'dacc77d2999049e7b82f8040c8ce21da', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 17, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 1, 'e2e_latency': 0.46004390716552734}}
[2025-10-13 17:49:35] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 17:56:20] Receive: obj=GenerateReqInput(rid='ca451d03a68648e0bc29cb4ddb0c5ff1', text='User Context:\n\n\n\nUser: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:56:20 TP0] Prefill batch. #new-seq: 1, #new-token: 42, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:56:22 TP0] Decode batch. #running-req: 1, #token: 76, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.03, #queue-req: 0, 
[2025-10-13 17:56:24 TP0] Decode batch. #running-req: 1, #token: 116, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.87, #queue-req: 0, 
[2025-10-13 17:56:26 TP0] Decode batch. #running-req: 1, #token: 156, token usage: 0.00, cuda graph: True, gen throughput (token/s): 19.80, #queue-req: 0, 
[2025-10-13 17:56:28 TP0] Decode batch. #running-req: 1, #token: 196, token usage: 0.01, cuda graph: True, gen throughput (token/s): 19.79, #queue-req: 0, 
[2025-10-13 17:56:30 TP0] Decode batch. #running-req: 1, #token: 236, token usage: 0.01, cuda graph: True, gen throughput (token/s): 19.80, #queue-req: 0, 
[2025-10-13 17:56:30] Finish: obj=GenerateReqInput(rid='ca451d03a68648e0bc29cb4ddb0c5ff1', text='User Context:\n\n\n\nUser: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' The original sentence is: \n\n"For this, you can use the \'pasted\' function to paste the code and then use the \'splitlines()\' function to split the code into lines, and finally use the \'pop()\' function to remove the last line."\n\nHere is the rewritten version for better readability while maintaining the original meaning:\n\n"To accomplish this, you can use the \'pasted\' function to paste the code. Then, utilize the \'splitlines()\' function to split the pasted code into individual lines. Finally, employ the \'pop()\' function to remove the last line from the resulting list of lines."\n\nThe rewritten sentence breaks down the steps more clearly and uses transitional phrases to enhance the flow and readability. It also replaces the word "and" with transitional words like "then" and "finally" to better delineate the sequence of actions.', 'output_ids': [781, 781, 7994, 11911, 29515, 1183, 4261, 13039, 1117, 29515, 29473, 781, 781, 29507, 3333, 1224, 29493, 1136, 1309, 1706, 1040, 1232, 29488, 13840, 29510, 1676, 1066, 26811, 1040, 3464, 1072, 1636, 1706, 1040, 1232, 6988, 8831, 1238, 29510, 1676, 1066, 8693, 1040, 3464, 1546, 5483, 29493, 1072, 5019, 1706, 1040, 1232, 6719, 1238, 29510, 1676, 1066, 6049, 1040, 2200, 2175, 1379, 781, 781, 16191, 1117, 1040, 1080, 16725, 3519, 1122, 2641, 1988, 3205, 2080, 18645, 1040, 4261, 6514, 29515, 781, 781, 29507, 2319, 18468, 1224, 29493, 1136, 1309, 1706, 1040, 1232, 29488, 13840, 29510, 1676, 1066, 26811, 1040, 3464, 29491, 3247, 29493, 23303, 1040, 1232, 6988, 8831, 1238, 29510, 1676, 1066, 8693, 1040, 3377, 1054, 3464, 1546, 4003, 5483, 29491, 8894, 29493, 4031, 1040, 1232, 6719, 1238, 29510, 1676, 1066, 6049, 1040, 2200, 2175, 1245, 1040, 11271, 2042, 1070, 5483, 1379, 781, 781, 1782, 1080, 16725, 13039, 16435, 1828, 1040, 6712, 1448, 7083, 1072, 6866, 1971, 3350, 28280, 1066, 12744, 1040, 5467, 1072, 1988, 3205, 29491, 1429, 1603, 5570, 2857, 1040, 2475, 1113, 1159, 29507, 1163, 1971, 3350, 3853, 1505, 1113, 6631, 29507, 1072, 1113, 3781, 1346, 29507, 1066, 2641, 1650, 1241, 1148, 1040, 8536, 1070, 7536, 29491, 2], 'meta_info': {'id': 'ca451d03a68648e0bc29cb4ddb0c5ff1', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 44, 'weight_version': 'default', 'completion_tokens': 195, 'cached_tokens': 2, 'e2e_latency': 9.937624454498291}}
[2025-10-13 17:56:30] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 17:56:30] Receive: obj=GenerateReqInput(rid='b331167984d94200b61bca915064d248', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\nASSISTANT: The original sentence is: \n\n"For this, you can use the \'pasted\' function to paste the code and then use the \'splitlines()\' function to split the code into lines, and finally use the \'pop()\' function to remove the last line."\n\nHere is the rewritten version for better readability while maintaining the original meaning:\n\n"To accomplish this, you can use the \'pasted\' function to paste the code. Then, utilize the \'splitlines()\' function to split the pasted code into individual lines. Finally, employ the \'pop()\' function to remove the last line from the resulting list of lines."\n\nThe rewritten sentence breaks down the steps more clearly and uses transitional phrases to enhance the flow and readability. It also replaces the word "and" with transitional words like "then" and "finally" to better delineate the sequence of actions.\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:56:30 TP0] Prefill batch. #new-seq: 1, #new-token: 466, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:56:33 TP0] Decode batch. #running-req: 1, #token: 506, token usage: 0.02, cuda graph: True, gen throughput (token/s): 12.60, #queue-req: 0, 
[2025-10-13 17:56:35 TP0] Decode batch. #running-req: 1, #token: 546, token usage: 0.02, cuda graph: True, gen throughput (token/s): 19.69, #queue-req: 0, 
[2025-10-13 17:56:37 TP0] Decode batch. #running-req: 1, #token: 586, token usage: 0.02, cuda graph: True, gen throughput (token/s): 19.70, #queue-req: 0, 
[2025-10-13 17:56:37] Finish: obj=GenerateReqInput(rid='b331167984d94200b61bca915064d248', text='User: ### Task:\nSuggest 3-5 relevant follow-up questions or prompts that the user might naturally ask next in this conversation as a **user**, based on the chat history, to help continue or deepen the discussion.\n### Guidelines:\n- Write all follow-up questions from the user’s point of view, directed to the assistant.\n- Make questions concise, clear, and directly related to the discussed topic(s).\n- Only suggest follow-ups that make sense given the chat content and do not repeat what was already covered.\n- If the conversation is very short or not specific, suggest more general (but relevant) follow-ups the user might ask.\n- Use the conversation\'s primary language; default to English if multilingual.\n- Response must be a JSON array of strings, no extra text or formatting.\n### Output:\nJSON format: { "follow_ups": ["Question 1?", "Question 2?", "Question 3?"] }\n### Chat History:\n<chat_history>\nUSER: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\nASSISTANT: The original sentence is: \n\n"For this, you can use the \'pasted\' function to paste the code and then use the \'splitlines()\' function to split the code into lines, and finally use the \'pop()\' function to remove the last line."\n\nHere is the rewritten version for better readability while maintaining the original meaning:\n\n"To accomplish this, you can use the \'pasted\' function to paste the code. Then, utilize the \'splitlines()\' function to split the pasted code into individual lines. Finally, employ the \'pop()\' function to remove the last line from the resulting list of lines."\n\nThe rewritten sentence breaks down the steps more clearly and uses transitional phrases to enhance the flow and readability. It also replaces the word "and" with transitional words like "then" and "finally" to better delineate the sequence of actions.\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' { "follow_ups": [\n  "Can you provide an example of how to use the \'pasted\', \'splitlines()\', and \'pop()\' functions in Python code?",\n  "What would be the output if I used these functions on a sample multi-line string?",\n  "Are there any alternative methods to achieve the same result without using these specific functions?",\n  "How do these functions handle empty lines or lines with only whitespace?",\n  "Can you explain the purpose and functionality of each of these functions in more detail?"\n] }', 'output_ids': [781, 781, 7994, 11911, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 1501, 781, 29473, 1113, 7093, 1136, 3852, 1164, 3525, 1070, 1678, 1066, 1706, 1040, 1232, 29488, 13840, 1415, 1232, 6988, 8831, 1238, 1415, 1072, 1232, 6719, 1238, 29510, 6340, 1065, 22134, 3464, 29572, 1316, 781, 29473, 1113, 3963, 1450, 1115, 1040, 4593, 1281, 1083, 2075, 1935, 6340, 1124, 1032, 8092, 6847, 29501, 1849, 2191, 29572, 1316, 781, 29473, 1113, 6753, 1504, 1475, 10053, 6330, 1066, 7387, 1040, 2116, 1972, 2439, 2181, 1935, 3716, 6340, 29572, 1316, 781, 29473, 1113, 6428, 1279, 1935, 6340, 5037, 5374, 5483, 1210, 5483, 1163, 1633, 20487, 3412, 29572, 1316, 781, 29473, 1113, 7093, 1136, 8050, 1040, 6800, 1072, 21057, 1070, 2198, 1070, 1935, 6340, 1065, 1448, 9059, 1878, 781, 29561, 1211, 2], 'meta_info': {'id': 'b331167984d94200b61bca915064d248', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 469, 'weight_version': 'default', 'completion_tokens': 123, 'cached_tokens': 3, 'e2e_latency': 7.350465297698975}}
[2025-10-13 17:56:37] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 17:56:37] Receive: obj=GenerateReqInput(rid='121a49f6e3864f11929a9fa7b8a901ca', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music S' ... 'ory>\nUSER: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\nASSISTANT: The original sentence is: \n\n"For this, you can use the \'pasted\' function to paste the code and then use the \'splitlines()\' function to split the code into lines, and finally use the \'pop()\' function to remove the last line."\n\nHere is the rewritten version for better readability while maintaining the original meaning:\n\n"To accomplish this, you can use the \'pasted\' function to paste the code. Then, utilize the \'splitlines()\' function to split the pasted code into individual lines. Finally, employ the \'pop()\' function to remove the last line from the resulting list of lines."\n\nThe rewritten sentence breaks down the steps more clearly and uses transitional phrases to enhance the flow and readability. It also replaces the word "and" with transitional words like "then" and "finally" to better delineate the sequence of actions.\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:56:37 TP0] Prefill batch. #new-seq: 1, #new-token: 560, #cached-token: 7, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:56:40] Finish: obj=GenerateReqInput(rid='121a49f6e3864f11929a9fa7b8a901ca', text='User: ### Task:\nGenerate a concise, 3-5 word title with an emoji summarizing the chat history.\n### Guidelines:\n- The title should clearly represent the main theme or subject of the conversation.\n- Use emojis that enhance understanding of the topic, but avoid quotation marks or special formatting.\n- Write the title in the chat\'s primary language; default to English if multilingual.\n- Prioritize accuracy over excessive creativity; keep it clear and simple.\n- Your entire response must consist solely of the JSON object, without any introductory or concluding text.\n- The output must be a single, raw JSON object, without any markdown code fences or other encapsulating text.\n- Ensure no conversational text, affirmations, or explanations precede or follow the raw JSON output, as this will cause direct parsing failure.\n### Output:\nJSON format: { "title": "your concise title here" }\n### Examples:\n- { "title": "📉 Stock Market Trends" },\n- { "title": "🍪 Perfect Chocolate Chip Recipe" },\n- { "title": "Evolution of Music S' ... 'ory>\nUSER: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\nASSISTANT: The original sentence is: \n\n"For this, you can use the \'pasted\' function to paste the code and then use the \'splitlines()\' function to split the code into lines, and finally use the \'pop()\' function to remove the last line."\n\nHere is the rewritten version for better readability while maintaining the original meaning:\n\n"To accomplish this, you can use the \'pasted\' function to paste the code. Then, utilize the \'splitlines()\' function to split the pasted code into individual lines. Finally, employ the \'pop()\' function to remove the last line from the resulting list of lines."\n\nThe rewritten sentence breaks down the steps more clearly and uses transitional phrases to enhance the flow and readability. It also replaces the word "and" with transitional words like "then" and "finally" to better delineate the sequence of actions.\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 1000}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' { "title": "🔤 Improving Sentence Readability" }', 'output_ids': [781, 781, 7994, 11911, 29515, 1139, 1113, 4669, 2032, 1113, 1011, 930, 919, 935, 22599, 4612, 1086, 1076, 1404, 5707, 3205, 29507, 1211, 2], 'meta_info': {'id': '121a49f6e3864f11929a9fa7b8a901ca', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 567, 'weight_version': 'default', 'completion_tokens': 19, 'cached_tokens': 7, 'e2e_latency': 2.2933833599090576}}
[2025-10-13 17:56:40] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 17:56:40] Receive: obj=GenerateReqInput(rid='9fcbff4168f940e78f4ae7312993b75e', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\nASSISTANT: The original sentence is: \n\n"For this, you can use the \'pasted\' function to paste the code and then use the \'splitlines()\' function to split the code into lines, and finally use the \'pop()\' function to remove the last line."\n\nHere is the rewritten version for better readability while maintaining the original meaning:\n\n"To accomplish this, you can use the \'pasted\' function to paste the code. Then, utilize the \'splitlines()\' function to split the pasted code into individual lines. Finally, employ the \'pop()\' function to remove the last line from the resulting list of lines."\n\nThe rewritten sentence breaks down the steps more clearly and uses transitional phrases to enhance the flow and readability. It also replaces the word "and" with transitional words like "then" and "finally" to better delineate the sequence of actions.\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 17:56:40 TP0] Prefill batch. #new-seq: 1, #new-token: 415, #cached-token: 8, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 17:56:42 TP0] Decode batch. #running-req: 1, #token: 438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 8.97, #queue-req: 0, 
[2025-10-13 17:56:43] Finish: obj=GenerateReqInput(rid='9fcbff4168f940e78f4ae7312993b75e', text='User: ### Task:\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\n\n### Guidelines:\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\n- If content is too short (less than 3 messages) or too diverse, use only ["General"]\n- Use the chat\'s primary language; default to English if multilingual\n- Prioritize accuracy over specificity\n\n### Output:\nJSON format: { "tags": ["tag1", "tag2", "tag3"] }\n\n### Chat History:\n<chat_history>\nUSER: Check the following sentence for grammar and clarity: "[sentence]". Rewrite it for better readability while maintaining its original meaning.\nASSISTANT: The original sentence is: \n\n"For this, you can use the \'pasted\' function to paste the code and then use the \'splitlines()\' function to split the code into lines, and finally use the \'pop()\' function to remove the last line."\n\nHere is the rewritten version for better readability while maintaining the original meaning:\n\n"To accomplish this, you can use the \'pasted\' function to paste the code. Then, utilize the \'splitlines()\' function to split the pasted code into individual lines. Finally, employ the \'pop()\' function to remove the last line from the resulting list of lines."\n\nThe rewritten sentence breaks down the steps more clearly and uses transitional phrases to enhance the flow and readability. It also replaces the word "and" with transitional words like "then" and "finally" to better delineate the sequence of actions.\n</chat_history>\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 2048}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' { "tags": ["Writing", "Grammar", "Clarity"], "subtopics": ["Sentence Structure", "Readability", "Transitional Phrases"] }', 'output_ids': [781, 781, 7994, 11911, 29515, 1139, 1113, 13354, 2032, 8135, 26055, 1056, 1316, 1113, 29545, 3980, 4247, 1316, 1113, 1967, 12678, 9651, 1113, 2434, 4514, 1831, 2032, 8135, 27736, 1404, 4606, 8955, 1316, 1113, 4158, 3205, 1316, 1113, 3899, 3350, 2740, 29480, 2786, 3010, 1211, 2], 'meta_info': {'id': '9fcbff4168f940e78f4ae7312993b75e', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 423, 'weight_version': 'default', 'completion_tokens': 41, 'cached_tokens': 8, 'e2e_latency': 3.0582282543182373}}
[2025-10-13 17:56:43] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 19:30:44] INFO:     127.0.0.1:50592 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 19:30:44] Receive: obj=GenerateReqInput(rid='f3b6b4f5a8244753b3873b1bc1c3d701', text='User: Say hello in 3 words\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 20}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 19:30:44 TP0] Prefill batch. #new-seq: 1, #new-token: 8, #cached-token: 7, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 19:30:45] Finish: obj=GenerateReqInput(rid='f3b6b4f5a8244753b3873b1bc1c3d701', text='User: Say hello in 3 words\n\nAssistant:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'top_p': 1.0, 'max_new_tokens': 20}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Hello there!', 'output_ids': [781, 781, 7994, 11911, 29515, 23325, 1504, 29576, 2], 'meta_info': {'id': 'f3b6b4f5a8244753b3873b1bc1c3d701', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 15, 'weight_version': 'default', 'completion_tokens': 4, 'cached_tokens': 7, 'e2e_latency': 0.2788827419281006}}
[2025-10-13 19:30:45] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
