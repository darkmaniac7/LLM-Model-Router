nohup: ignoring input
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:00:13] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=331249443, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:00:13] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:00:13] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:00:21 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:00:21 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:00:21 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:00:21 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:00:21 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:00:21 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:00:21 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:00:21 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:00:21 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:00:21 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:00:22 TP0] sglang is using nccl==2.27.3
[2025-10-12 23:00:22 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:00:22 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:00:22 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:00:22 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:00:22 TP0] Init torch distributed ends. mem usage=0.14 GB
[2025-10-12 23:00:22 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:00:22 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:00:22 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:00:22 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:00:23 TP0] Load weight begin. avail mem=23.16 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:10,  1.26it/s]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:01<00:09,  1.31it/s]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:02<00:08,  1.32it/s]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:02<00:07,  1.39it/s]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:03<00:06,  1.45it/s]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:04<00:05,  1.43it/s]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:04<00:04,  1.45it/s]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:05<00:03,  1.87it/s]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:05<00:02,  1.80it/s]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:06<00:02,  1.67it/s]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:07<00:01,  1.61it/s]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:07<00:01,  1.54it/s]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:08<00:00,  1.50it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.46it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.51it/s]

[2025-10-12 23:00:34 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.84 GB, mem usage=15.32 GB.
[2025-10-12 23:00:35 TP0] Using KV cache dtype: torch.float16
[2025-10-12 23:00:35 TP1] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:00:35 TP0] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:00:35 TP0] Memory pool end. avail mem=4.25 GB
[2025-10-12 23:00:35 TP2] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:00:35 TP3] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:00:35 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.74 GB
[2025-10-12 23:00:35 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   7%|▋         | 1/14 [00:01<00:17,  1.37s/it]Capturing batches (bs=72 avail_mem=3.65 GB):   7%|▋         | 1/14 [00:01<00:17,  1.37s/it]Capturing batches (bs=72 avail_mem=3.65 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.09it/s]Capturing batches (bs=64 avail_mem=3.64 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.09it/s]Capturing batches (bs=64 avail_mem=3.64 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.41it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.41it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.71it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.71it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.96it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.96it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.18it/s]Capturing batches (bs=32 avail_mem=3.56 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.18it/s]Capturing batches (bs=32 avail_mem=3.56 GB):  50%|█████     | 7/14 [00:03<00:02,  2.49it/s]Capturing batches (bs=24 avail_mem=3.55 GB):  50%|█████     | 7/14 [00:03<00:02,  2.49it/s]Capturing batches (bs=24 avail_mem=3.55 GB):  57%|█████▋    | 8/14 [00:04<00:02,  2.82it/s]Capturing batches (bs=16 avail_mem=3.52 GB):  57%|█████▋    | 8/14 [00:04<00:02,  2.82it/s]Capturing batches (bs=16 avail_mem=3.52 GB):  64%|██████▍   | 9/14 [00:04<00:01,  3.14it/s]Capturing batches (bs=12 avail_mem=3.51 GB):  64%|██████▍   | 9/14 [00:04<00:01,  3.14it/s]Capturing batches (bs=12 avail_mem=3.51 GB):  71%|███████▏  | 10/14 [00:04<00:01,  3.46it/s]Capturing batches (bs=8 avail_mem=3.48 GB):  71%|███████▏  | 10/14 [00:04<00:01,  3.46it/s] Capturing batches (bs=8 avail_mem=3.48 GB):  79%|███████▊  | 11/14 [00:04<00:00,  3.83it/s]Capturing batches (bs=4 avail_mem=3.47 GB):  79%|███████▊  | 11/14 [00:04<00:00,  3.83it/s]Capturing batches (bs=4 avail_mem=3.47 GB):  86%|████████▌ | 12/14 [00:04<00:00,  4.18it/s]Capturing batches (bs=2 avail_mem=3.43 GB):  86%|████████▌ | 12/14 [00:04<00:00,  4.18it/s]Capturing batches (bs=2 avail_mem=3.43 GB):  93%|█████████▎| 13/14 [00:05<00:00,  4.16it/s]Capturing batches (bs=1 avail_mem=3.42 GB):  93%|█████████▎| 13/14 [00:05<00:00,  4.16it/s]Capturing batches (bs=1 avail_mem=3.42 GB): 100%|██████████| 14/14 [00:05<00:00,  4.12it/s]Capturing batches (bs=1 avail_mem=3.42 GB): 100%|██████████| 14/14 [00:05<00:00,  2.57it/s]
[2025-10-12 23:00:41 TP0] Capture cuda graph end. Time elapsed: 5.99 s. mem usage=0.35 GB. avail mem=3.40 GB.
[2025-10-12 23:00:42 TP0] max_total_num_tokens=38187, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=3.40 GB
[2025-10-12 23:00:42] INFO:     Started server process [294280]
[2025-10-12 23:00:42] INFO:     Waiting for application startup.
[2025-10-12 23:00:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:00:42] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:00:42] INFO:     Application startup complete.
[2025-10-12 23:00:42] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-12 23:00:43] INFO:     127.0.0.1:56954 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-12 23:00:43] Receive: obj=GenerateReqInput(rid='054e359585624b2f8cf8b39cac7f1a92', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:00:43 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:00:44] Finish: obj=GenerateReqInput(rid='054e359585624b2f8cf8b39cac7f1a92', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': '054e359585624b2f8cf8b39cac7f1a92', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.2014639377593994}}
[2025-10-12 23:00:44] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-12 23:00:44] The server is fired up and ready to roll!
[2025-10-12 23:00:57] INFO:     127.0.0.1:60654 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-12 23:01:25] Receive: obj=GenerateReqInput(rid='f2163e1b8bce4b4dba0012185f5e8c2d', text='Hello, my name is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': 30, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:01:25 TP0] Prefill batch. #new-seq: 1, #new-token: 5, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:01:33] Finish: obj=GenerateReqInput(rid='f2163e1b8bce4b4dba0012185f5e8c2d', text='Hello, my name is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': 30, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': " Jason! I'm a 37 year old software developer in Houston, TX. I was born and raised in Germany, moved to the United", 'output_ids': [16998, 29493, 1354, 1909, 1117, 15645, 29576, 1083, 29510, 29487, 1032, 29473, 29538, 29555, 1647, 2339, 4698, 22550, 1065, 17672, 29493, 18447, 29491, 1083, 1171, 6149, 1072, 7101, 1065, 8061, 29493, 4910, 1066, 1040, 3737], 'meta_info': {'id': 'f2163e1b8bce4b4dba0012185f5e8c2d', 'finish_reason': {'type': 'length', 'length': 30}, 'prompt_tokens': 6, 'weight_version': 'default', 'completion_tokens': 30, 'cached_tokens': 1, 'e2e_latency': 7.8939971923828125}}
[2025-10-12 23:01:33] INFO:     127.0.0.1:43176 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:01:46] INFO:     127.0.0.1:49184 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:02:02] Receive: obj=GenerateReqInput(rid='2e996e8c737847f09c49816d4b0b278b', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:02:02 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:02:02 TP0] Decode batch. #running-req: 1, #token: 15, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.49, #queue-req: 0, 
[2025-10-12 23:02:06 TP0] Decode batch. #running-req: 1, #token: 55, token usage: 0.00, cuda graph: True, gen throughput (token/s): 12.47, #queue-req: 0, 
[2025-10-12 23:02:09 TP0] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, cuda graph: True, gen throughput (token/s): 12.46, #queue-req: 0, 
[2025-10-12 23:02:11 TP0] Decode batch. #running-req: 1, #token: 135, token usage: 0.00, cuda graph: True, gen throughput (token/s): 16.68, #queue-req: 0, 
[2025-10-12 23:02:13] Finish: obj=GenerateReqInput(rid='2e996e8c737847f09c49816d4b0b278b', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, a branch of physics that deals with the behavior of particles at very small scales, like atoms and subatomic particles. Unlike classical computers that use bits to process information, quantum computers use quantum bits, or qubits. Here's a simple breakdown:\n\n### Classical Computing vs. Quantum Computing\n\n**Classical Computing:**\n- **Bits:** The basic unit of information is a bit, which can be either 0 or 1.\n- **Operations:** Logic gates perform operations on bits. These gates include AND, OR, NOT, etc.\n- **Processing:** Computations are performed in a binary system", 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1032, 10567, 1070, 16941, 1137, 15453, 1163, 1040, 6942, 1070, 14706, 1206, 1983, 2509, 20000, 29493, 1505, 24989, 1072, 1851, 15203, 14706, 29491, 18151, 14146, 19286, 1137, 1706, 10124, 1066, 2527, 2639, 29493, 11683, 19286, 1706, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 4771, 29510, 29481, 1032, 4356, 27755, 29515, 781, 781, 28100, 5718, 1513, 14419, 1056, 8477, 29491, 23423, 1151, 14419, 1056, 781, 781, 1116, 3240, 1513, 14419, 1056, 4817, 781, 29501, 1387, 15489, 4817, 1183, 7239, 5796, 1070, 2639, 1117, 1032, 3054, 29493, 1458, 1309, 1115, 3245, 29473, 29502, 1210, 29473, 29508, 29491, 781, 29501, 1387, 24477, 4817, 6313, 1062, 22393, 2993, 7701, 1124, 10124, 29491, 3725, 22393, 3792, 6539, 29493, 4762, 29493, 6225, 29493, 5113, 29491, 781, 29501, 1387, 28387, 4817, 14419, 1465, 1228, 8653, 1065, 1032, 10904, 2355], 'meta_info': {'id': '2e996e8c737847f09c49816d4b0b278b', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 1, 'e2e_latency': 10.426499843597412}}
[2025-10-12 23:02:13] INFO:     127.0.0.1:47290 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:03:19] Receive: obj=GenerateReqInput(rid='9b0394c058c54046ae802459e3b795c6', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:03:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:03:20 TP0] Decode batch. #running-req: 1, #token: 25, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.58, #queue-req: 0, 
[2025-10-12 23:03:22 TP0] Decode batch. #running-req: 1, #token: 65, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.02, #queue-req: 0, 
[2025-10-12 23:03:24 TP0] Decode batch. #running-req: 1, #token: 105, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.00, #queue-req: 0, 
[2025-10-12 23:03:26 TP0] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.06, #queue-req: 0, 
[2025-10-12 23:03:27] Finish: obj=GenerateReqInput(rid='9b0394c058c54046ae802459e3b795c6', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, a branch of physics that deals with very small things like atoms and subatomic particles. Unlike classical computers that use bits to process information, quantum computers use something called qubits.\n\n### What are Qubits?\n\nA classical bit can be in one of two states: 0 or 1. A qubit, however, can be in a state of 0, 1, or both 0 and 1 at the same time, thanks to a property called superposition. Think of it like a coin that is spinning in the air: before it lands, it is both heads and tails at the same time.\n\n', 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1032, 10567, 1070, 16941, 1137, 15453, 1163, 1983, 2509, 2490, 1505, 24989, 1072, 1851, 15203, 14706, 29491, 18151, 14146, 19286, 1137, 1706, 10124, 1066, 2527, 2639, 29493, 11683, 19286, 1706, 2313, 2755, 1294, 8437, 29491, 781, 781, 28100, 2592, 1228, 1954, 1205, 1814, 29572, 781, 781, 29509, 14146, 3054, 1309, 1115, 1065, 1392, 1070, 1757, 5373, 29515, 29473, 29502, 1210, 29473, 29508, 29491, 1098, 1294, 3349, 29493, 4313, 29493, 1309, 1115, 1065, 1032, 2433, 1070, 29473, 29502, 29493, 29473, 29508, 29493, 1210, 2328, 29473, 29502, 1072, 29473, 29508, 1206, 1040, 2116, 1495, 29493, 8964, 1066, 1032, 4089, 2755, 2963, 4408, 29491, 16171, 1070, 1146, 1505, 1032, 17312, 1137, 1117, 26850, 1065, 1040, 3191, 29515, 1927, 1146, 16677, 29493, 1146, 1117, 2328, 11246, 1072, 1029, 2782, 1206, 1040, 2116, 1495, 29491, 781, 781], 'meta_info': {'id': '9b0394c058c54046ae802459e3b795c6', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 7.389344692230225}}
[2025-10-12 23:03:27] INFO:     127.0.0.1:60380 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:04:32] INFO:     127.0.0.1:52266 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:04:33] INFO:     127.0.0.1:52270 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:04:36] INFO:     127.0.0.1:47626 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:05:09] INFO:     127.0.0.1:54998 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:05:09] INFO:     127.0.0.1:55006 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-12 23:05:09] Receive: obj=GenerateReqInput(rid='925f0cb214ca4ed283c2afa5a0170b7d', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:05:09 TP0] Prefill batch. #new-seq: 1, #new-token: 80, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:05:11 TP0] Decode batch. #running-req: 1, #token: 104, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.38, #queue-req: 0, 
[2025-10-12 23:05:12 TP0] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.22, #queue-req: 0, 
[2025-10-12 23:05:14 TP0] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.13, #queue-req: 0, 
[2025-10-12 23:05:16 TP0] Decode batch. #running-req: 1, #token: 224, token usage: 0.01, cuda graph: True, gen throughput (token/s): 20.99, #queue-req: 0, 
[2025-10-12 23:05:18 TP0] Decode batch. #running-req: 1, #token: 264, token usage: 0.01, cuda graph: True, gen throughput (token/s): 21.09, #queue-req: 0, 
[2025-10-12 23:05:20 TP0] Decode batch. #running-req: 1, #token: 304, token usage: 0.01, cuda graph: True, gen throughput (token/s): 21.14, #queue-req: 0, 
[2025-10-12 23:05:22 TP0] Decode batch. #running-req: 1, #token: 344, token usage: 0.01, cuda graph: True, gen throughput (token/s): 21.00, #queue-req: 0, 
[2025-10-12 23:05:24 TP0] Decode batch. #running-req: 1, #token: 384, token usage: 0.01, cuda graph: True, gen throughput (token/s): 21.10, #queue-req: 0, 
[2025-10-12 23:05:26 TP0] Decode batch. #running-req: 1, #token: 424, token usage: 0.01, cuda graph: True, gen throughput (token/s): 20.82, #queue-req: 0, 
[2025-10-12 23:05:28 TP0] Decode batch. #running-req: 1, #token: 464, token usage: 0.01, cuda graph: True, gen throughput (token/s): 20.92, #queue-req: 0, 
[2025-10-12 23:05:30 TP0] Decode batch. #running-req: 1, #token: 504, token usage: 0.01, cuda graph: True, gen throughput (token/s): 20.97, #queue-req: 0, 
[2025-10-12 23:05:31 TP0] Decode batch. #running-req: 1, #token: 544, token usage: 0.01, cuda graph: True, gen throughput (token/s): 20.77, #queue-req: 0, 
[2025-10-12 23:05:33] Receive: obj=GenerateReqInput(rid='f220d94fda2d46f08131db7bceac6892', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:05:33 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.02, #running-req: 1, #queue-req: 0, 
[2025-10-12 23:05:34 TP0] Decode batch. #running-req: 2, #token: 608, token usage: 0.02, cuda graph: True, gen throughput (token/s): 21.38, #queue-req: 0, 
[2025-10-12 23:05:36 TP0] Decode batch. #running-req: 2, #token: 688, token usage: 0.02, cuda graph: True, gen throughput (token/s): 41.08, #queue-req: 0, 
[2025-10-12 23:05:38 TP0] Decode batch. #running-req: 2, #token: 768, token usage: 0.02, cuda graph: True, gen throughput (token/s): 40.92, #queue-req: 0, 
[2025-10-12 23:05:40 TP0] Decode batch. #running-req: 2, #token: 848, token usage: 0.02, cuda graph: True, gen throughput (token/s): 41.00, #queue-req: 0, 
[2025-10-12 23:05:41] Finish: obj=GenerateReqInput(rid='f220d94fda2d46f08131db7bceac6892', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, which is a branch of physics that deals with the behavior of matter and energy at the scale of atoms and subatomic particles. Here's a simple breakdown:\n\n1. **Classical Computing vs Quantum Computing**: In a regular (or classical) computer, the smallest unit of information is a bit. A bit can be either 0 or 1. Quantum computers, however, use something called quantum bits, or 'qubits'.\n\n2. **Qubits**: Unlike classical bits, a qubit can be in multiple states at once, thanks to a property called superposition. This means a qubit can be", 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1458, 1117, 1032, 10567, 1070, 16941, 1137, 15453, 1163, 1040, 6942, 1070, 3977, 1072, 4176, 1206, 1040, 6425, 1070, 24989, 1072, 1851, 15203, 14706, 29491, 4771, 29510, 29481, 1032, 4356, 27755, 29515, 781, 781, 29508, 29491, 1387, 3240, 1513, 14419, 1056, 8477, 23423, 1151, 14419, 1056, 9957, 1328, 1032, 5160, 1093, 1039, 14146, 29499, 6842, 29493, 1040, 23109, 5796, 1070, 2639, 1117, 1032, 3054, 29491, 1098, 3054, 1309, 1115, 3245, 29473, 29502, 1210, 29473, 29508, 29491, 23423, 1151, 19286, 29493, 4313, 29493, 1706, 2313, 2755, 11683, 10124, 29493, 1210, 1232, 1132, 8437, 4903, 781, 781, 29518, 29491, 1387, 29592, 1205, 1814, 9957, 18151, 14146, 10124, 29493, 1032, 1294, 3349, 1309, 1115, 1065, 5934, 5373, 1206, 3095, 29493, 8964, 1066, 1032, 4089, 2755, 2963, 4408, 29491, 1619, 3593, 1032, 1294, 3349, 1309, 1115], 'meta_info': {'id': 'f220d94fda2d46f08131db7bceac6892', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 7.871298313140869}}
[2025-10-12 23:05:41] INFO:     127.0.0.1:56822 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:05:42 TP0] Decode batch. #running-req: 1, #token: 744, token usage: 0.02, cuda graph: True, gen throughput (token/s): 29.16, #queue-req: 0, 
[2025-10-12 23:05:44 TP0] Decode batch. #running-req: 1, #token: 784, token usage: 0.02, cuda graph: True, gen throughput (token/s): 20.58, #queue-req: 0, 
[2025-10-12 23:05:46 TP0] Decode batch. #running-req: 1, #token: 824, token usage: 0.02, cuda graph: True, gen throughput (token/s): 20.87, #queue-req: 0, 
[2025-10-12 23:05:48 TP0] Decode batch. #running-req: 1, #token: 864, token usage: 0.02, cuda graph: True, gen throughput (token/s): 20.77, #queue-req: 0, 
[2025-10-12 23:05:49 TP0] Decode batch. #running-req: 1, #token: 904, token usage: 0.02, cuda graph: True, gen throughput (token/s): 20.88, #queue-req: 0, 
[2025-10-12 23:05:51 TP0] Decode batch. #running-req: 1, #token: 944, token usage: 0.02, cuda graph: True, gen throughput (token/s): 20.95, #queue-req: 0, 
[2025-10-12 23:05:53 TP0] Decode batch. #running-req: 1, #token: 984, token usage: 0.03, cuda graph: True, gen throughput (token/s): 20.72, #queue-req: 0, 
[2025-10-12 23:05:55 TP0] Decode batch. #running-req: 1, #token: 1024, token usage: 0.03, cuda graph: True, gen throughput (token/s): 20.77, #queue-req: 0, 
[2025-10-12 23:05:57 TP0] Decode batch. #running-req: 1, #token: 1064, token usage: 0.03, cuda graph: True, gen throughput (token/s): 20.80, #queue-req: 0, 
[2025-10-12 23:05:58] Finish: obj=GenerateReqInput(rid='925f0cb214ca4ed283c2afa5a0170b7d', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Sure, I\'d be happy to help with that. Let\'s generate a brief story to reach around 1000 tokens. Here we go:\n\n---\n\nIn the quiet town of Mossgrove, nestled between undulating hills and a whispers-thin river, there lived a watchmaker named Eli. His shop was a tiny thing, tucked away in a crooked alley, where the smell of aged wood and dust filled the air. Eli\'s hands were his magic, crafting timepieces so precise and beautiful that they were said to steal moments from the very gods who spun the wheels of time.\n\nOne day, a stranger named Orion stumbled into Eli\'s shop. He was a tall man with eyes like stormy seas and a voice that echoed like distant thunder. Orion carried with him an antique pocket watch, its glass face shattered and hands frozen at the eleventh hour.\n\n"Can you fix it?" Orion asked, his voice a low rumble.\n\nEli took the watch, his eyes widening as he examined the intricate mechanism. "This is exquisite," he murmured. "Where did you get it?"\n\nOrion hesitated before answering, "It\'s a family heirl' ... 'ng, as Eli was about to close his shop, he heard a soft knock at the door. Standing outside was a woman, her eyes filled with a familiar stormy intensity. She introduced herself as Lyra, Orion\'s sister. In her hand was the pocket watch.\n\n"Orion told me what happened," Lyra said, her voice soft yet commanding. "He said you saw something... unusual."\n\nEli nodded, his heart pounding. "Yes, I did. A forest, laughter, music. It was... enchanting."\n\nLyra\'s eyes met his, and she smiled. "The watch is more than just a timepiece. It\'s a key to our family\'s legacy, a gateway to memories long past. But it\'s also a burden, a responsibility. Are you willing to help us carry that burden, Eli?"\n\nEli didn\'t hesitate. "Yes," he said, his voice steady and sure. "I am."\n\nAnd so, Eli found himself drawn into a world far beyond the quiet streets of Mossgrove, a world where time was not just measured but remembered, where every tick of a clock held a story waiting to be told.\n\n---\n\nHope that helps! Is there anything else you need?', 'output_ids': [2], 'meta_info': {'id': '925f0cb214ca4ed283c2afa5a0170b7d', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 81, 'weight_version': 'default', 'completion_tokens': 1009, 'cached_tokens': 1, 'e2e_latency': 49.23596477508545}}
[2025-10-12 23:05:58] INFO:     127.0.0.1:44502 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:05:58] Receive: obj=GenerateReqInput(rid='47060243c68341afbde2dae4609a3806', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 3937, 29510, 29481, 9038, 1032, 7585, 3606, 1066, 5331, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29491, 4771, 1246, 1344, 29515, 781, 781, 8163, 781, 781, 1425, 1040, 6651, 4504, 1070, 1119, 2926, 26112, 1101, 29493, 18643, 1774, 2212, 1408, 14390, 19321, 1072, 1032, 8805, 5605, 29501, 1130, 1030, 8550, 29493, 1504, 7030, 1032, 3822, 20731, 5928, 17945, 29491, 3122, 5894, 1171, 1032, 9655, 2738, 29493, 27159, 2521, 1065, 1032, 8718, 9409, 29307, 29493, 1738, 1040, 11784, 1070, 18627, 5536, 1072, 9956, 7542, 1040, 3191, 29491, 17945, 29510, 29481, 3806, 1422, 1284, 10461, 29493, 10717, 1056, 1495, 9821, 2145, 1347, 17776, 1072, 5440, 1137, 1358, 1422, 1541, 1066, 21538, 10003, 1245, 1040, 1983, 17346, 1461, 26967, 1040, 20644, 1070, 1495, 29491, 781, 781, 6935, 2138, 29493, 1032, 20393, 5928, 2435, 1064, 28675, 1546, 17945, 29510, 29481, 5894, 29491, 1418, 1171, 1032, 10137, 1444, 1163, 3050, 1505, 10503, 29492, 29211, 1072, 1032, 4209, 1137, 4662, 1054, 1505, 16337, 26520, 29491, 2435, 1064, 7926, 1163, 1481, 1164, 3493, 2419, 11353, 3822, 29493, 1639, 6020, 2873, 1248, 17877, 1072, 3806, 15967, 1206, 1040, 6707, 1839, 29484, 5883, 29491, 781, 781, 29507, 7093, 1136, 7061, 1146, 1878, 2435, 1064, 3029, 29493, 1284, 4209, 1032, 3627, 10971, 1750, 29491, 781, 781, 29517, 1912, 2824, 1040, 3822, 29493, 1284, 3050, 8512, 4018, 1158, 1168, 21900, 1040, 8445, 6517, 14943, 29491, 1113, 4028, 1117, 1207, 11993, 1338, 1630, 1168, 25451, 29491, 1113, 10375, 1631, 1136, 1393, 1146, 1878, 781, 781, 2996, 1064, 25485, 1927, 25170, 29493, 1113, 2081, 29510, 29481, 1032, 2773, 1168, 1129, 1499, 1068, 29491, 2752, 20068, 1541, 1146, 1171, 10717, 1054, 1254, 1040, 12143, 5514, 1379, 781, 781, 29517, 1912, 3150, 1350, 29493, 1284, 23721, 1052, 3509, 1054, 29491, 1113, 1782, 12143, 29493, 1136, 2083, 1878, 781, 781, 2996, 1064, 8081, 29491, 1113, 3196, 1171, 1032, 9564, 1062, 1444, 29491, 1860, 1040, 3822, 1117, 3609, 29491, 1429, 3136, 29510, 29475, 1544, 2680, 1495, 29513, 1146, 9056, 13375, 29491, 7834, 15915, 1124, 1040, 1990, 29493, 2198, 2654, 1374, 1070, 9956, 1869, 1146, 29510, 29481, 1312, 1032, 1512, 1070, 1354, 2773, 29510, 29481, 4108, 1379, 781, 781, 29517, 1912, 6593, 1066, 12469, 1040, 3822, 29493, 11189, 1066, 1040, 18260, 1072, 1040, 8803, 29491, 1904, 1168, 4966, 29493, 1168, 9063, 2313, 15382, 29491, 1183, 3822, 1321, 1164, 4978, 13901, 29493, 1392, 1137, 2307, 29510, 29475, 2477, 1066, 7799, 1475, 6800, 29491, 1429, 1171, 27856, 10717, 1054, 29493, 1163, 1032, 9655, 12143, 10734, 1206, 1639, 5750, 29491, 17945, 4249, 29510, 29475, 15673, 29513, 1168, 1029, 1423, 2860, 1072, 22643, 29493, 2764, 1040, 22064, 13901, 27167, 1546, 2401, 29491, 781, 781, 2966, 1040, 13901, 3665, 29493, 1040, 5894, 7682, 7542, 1163, 1032, 3432, 29493, 8279, 1165, 1050, 2829, 29491, 17945, 3150, 1350, 29493, 1917, 1774, 29493, 1158, 1040, 2771, 3893, 1066, 3036, 29491, 1183, 8901, 24484, 2521, 29493, 9707, 1254, 1040, 3726, 1414, 14662, 1070, 1032, 12143, 19368, 9381, 29491, 1418, 1597, 4702, 18979, 29589, 29476, 3739, 29510, 29481, 18979, 29589, 1159, 1040, 16337, 9738, 1942, 1070, 1032, 4875, 1030, 29491, 3247, 29493, 1158, 7682, 1158, 1146, 1321, 14492, 29493, 1040, 8789, 7822, 29491, 1183, 5894, 1171, 1158, 1146, 1321, 2511, 1518, 29493, 7542, 1163, 1040, 6651, 7252, 1056, 1070, 1301, 8480, 29491, 781, 781, 29517, 1912, 1171, 1248, 3357, 1330, 8445, 1094, 3233, 29491, 1418, 5021, 1040, 3822, 1066, 2435, 1064, 29493, 21168, 1535, 1321, 5011, 1330, 7053, 1343, 1040, 1512, 1452, 1040, 8789, 29491, 2435, 1064, 15880, 1484, 3018, 29493, 1636, 2824, 1040, 3822, 1072, 7747, 1054, 17945, 1122, 1284, 1539, 29491, 781, 781, 2966, 2435, 1064, 2517, 1040, 5894, 29493, 17945, 4249, 29510, 29475, 2084, 1330, 2369, 1032, 1052, 1370, 1070, 19137, 29491, 1418, 1321, 2040, 1089, 1550, 1054, 2313, 21695, 29493, 2313, 1137, 25098, 3276, 1040, 4356, 25135, 1070, 3822, 15495, 29491, 1418, 2187, 3490, 11882, 1137, 2435, 1064, 1450, 1372, 29493, 1137, 1040, 18260, 1070, 1040, 3822, 3491, 29510, 29475, 1522, 29491, 781, 781, 29525, 1516, 3665, 1546, 5355, 29493, 1072, 17945, 2187, 3490, 13875, 1478, 19330, 1591, 1163, 1040, 4694, 12796, 2435, 1064, 29510, 29481, 3822, 29491, 1418, 5991, 4084, 2727, 1056, 1522, 2339, 20105, 1072, 3822, 15495, 1444, 13841, 29493, 13637, 1122, 1475, 5157, 1070, 1032, 3822, 1137, 4104, 13375, 1210, 2485, 1362, 29491, 3122, 1586, 6362, 8876, 29493, 7056, 1056, 1284, 8171, 1072, 13000, 29491, 781, 781, 6935, 7624, 29493, 1158, 17945, 1171, 1452, 1066, 3616, 1284, 5894, 29493, 1168, 4132, 1032, 3432, 14244, 1206, 1040, 3019, 29491, 8381, 1056, 4304, 1171, 1032, 3739, 29493, 1327, 3050, 7542, 1163, 1032, 8996, 10503, 29492, 17568, 29491, 1753, 8928, 5514, 1158, 11087, 1288, 29493, 2435, 1064, 29510, 29481, 6436, 29491, 1328, 1327, 1789, 1171, 1040, 11353, 3822, 29491, 781, 781, 29507, 2996, 1064, 3008, 1296, 1535, 5011, 1630, 11087, 1288, 1541, 29493, 1327, 4209, 3432, 3551, 4213, 1056, 29491, 1113, 3196, 1541, 1136, 3440, 2313, 1869, 15382, 1379, 781, 781, 29517, 1912, 8081, 29493, 1284, 3799, 16667, 1056, 29491, 1113, 6381, 29493, 1083, 1631, 29491, 1098, 9381, 29493, 18979, 29493, 4195, 29491, 1429, 1171, 1869, 1249, 12514, 1056, 1379, 781, 781, 29526, 29492, 1288, 29510, 29481, 3050, 2192, 1284, 29493, 1072, 1398, 8738, 29491, 1113, 1782, 3822, 1117, 1448, 1589, 1544, 1032, 1495, 17920, 29491, 1429, 29510, 29481, 1032, 2713, 1066, 1581, 2773, 29510, 29481, 18192, 29493, 1032, 10630, 1896, 1066, 13375, 1811, 3377, 29491, 1860, 1146, 29510, 29481, 1603, 1032, 19164, 29493, 1032, 10448, 29491, 5635, 1136, 8581, 1066, 2084, 1360, 7864, 1137, 19164, 29493, 17945, 1878, 781, 781, 29517, 1912, 2307, 29510, 29475, 11584, 10415, 29491, 1113, 6381, 1630, 1168, 1541, 29493, 1284, 4209, 15657, 1072, 2632, 29491, 1113, 29505, 1605, 1379, 781, 781, 3235, 1347, 29493, 17945, 2187, 3490, 11189, 1546, 1032, 2294, 2850, 6066, 1040, 6651, 11236, 1070, 1119, 2926, 26112, 1101, 29493, 1032, 2294, 1738, 1495, 1171, 1227, 1544, 13588, 1330, 10984, 29493, 1738, 1780, 7252, 1070, 1032, 10685, 4104, 1032, 3606, 6113, 1066, 1115, 3008, 29491, 781, 781, 8163, 781, 781, 29537, 2613, 1137, 8031, 29576, 2459, 1504, 3192, 1880, 1136, 1695, 29572, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:05:58 TP0] Prefill batch. #new-seq: 1, #new-token: 1324, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:06:03 TP0] Decode batch. #running-req: 1, #token: 1339, token usage: 0.04, cuda graph: True, gen throughput (token/s): 7.19, #queue-req: 0, 
[2025-10-12 23:06:05 TP0] Decode batch. #running-req: 1, #token: 1379, token usage: 0.04, cuda graph: True, gen throughput (token/s): 20.85, #queue-req: 0, 
[2025-10-12 23:06:06] Finish: obj=GenerateReqInput(rid='47060243c68341afbde2dae4609a3806', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 3937, 29510, 29481, 9038, 1032, 7585, 3606, 1066, 5331, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29491, 4771, 1246, 1344, 29515, 781, 781, 8163, 781, 781, 1425, 1040, 6651, 4504, 1070, 1119, 2926, 26112, 1101, 29493, 18643, 1774, 2212, 1408, 14390, 19321, 1072, 1032, 8805, 5605, 29501, 1130, 1030, 8550, 29493, 1504, 7030, 1032, 3822, 20731, 5928, 17945, 29491, 3122, 5894, 1171, 1032, 9655, 2738, 29493, 27159, 2521, 1065, 1032, 8718, 9409, 29307, 29493, 1738, 1040, 11784, 1070, 18627, 5536, 1072, 9956, 7542, 1040, 3191, 29491, 17945, 29510, 29481, 3806, 1422, 1284, 10461, 29493, 10717, 1056, 1495, 9821, 2145, 1347, 17776, 1072, 5440, 1137, 1358, 1422, 1541, 1066, 21538, 10003, 1245, 1040, 1983, 17346, 1461, 26967, 1040, 20644, 1070, 1495, 29491, 781, 781, 6935, 2138, 29493, 1032, 20393, 5928, 2435, 1064, 28675, 1546, 17945, 29510, 29481, 5894, 29491, 1418, 1171, 1032, 10137, 1444, 1163, 3050, 1505, 10503, 29492, 29211, 1072, 1032, 4209, 1137, 4662, 1054, 1505, 16337, 26520, 29491, 2435, 1064, 7926, 1163, 1481, 1164, 3493, 2419, 11353, 3822, 29493, 1639, 6020, 2873, 1248, 17877, 1072, 3806, 15967, 1206, 1040, 6707, 1839, 29484, 5883, 29491, 781, 781, 29507, 7093, 1136, 7061, 1146, 1878, 2435, 1064, 3029, 29493, 1284, 4209, 1032, 3627, 10971, 1750, 29491, 781, 781, 29517, 1912, 2824, 1040, 3822, 29493, 1284, 3050, 8512, 4018, 1158, 1168, 21900, 1040, 8445, 6517, 14943, 29491, 1113, 4028, 1117, 1207, 11993, 1338, 1630, 1168, 25451, 29491, 1113, 10375, 1631, 1136, 1393, 1146, 1878, 781, 781, 2996, 1064, 25485, 1927, 25170, 29493, 1113, 2081, 29510, 29481, 1032, 2773, 1168, 1129, 1499, 1068, 29491, 2752, 20068, 1541, 1146, 1171, 10717, 1054, 1254, 1040, 12143, 5514, 1379, 781, 781, 29517, 1912, 3150, 1350, 29493, 1284, 23721, 1052, 3509, 1054, 29491, 1113, 1782, 12143, 29493, 1136, 2083, 1878, 781, 781, 2996, 1064, 8081, 29491, 1113, 3196, 1171, 1032, 9564, 1062, 1444, 29491, 1860, 1040, 3822, 1117, 3609, 29491, 1429, 3136, 29510, 29475, 1544, 2680, 1495, 29513, 1146, 9056, 13375, 29491, 7834, 15915, 1124, 1040, 1990, 29493, 2198, 2654, 1374, 1070, 9956, 1869, 1146, 29510, 29481, 1312, 1032, 1512, 1070, 1354, 2773, 29510, 29481, 4108, 1379, 781, 781, 29517, 1912, 6593, 1066, 12469, 1040, 3822, 29493, 11189, 1066, 1040, 18260, 1072, 1040, 8803, 29491, 1904, 1168, 4966, 29493, 1168, 9063, 2313, 15382, 29491, 1183, 3822, 1321, 1164, 4978, 13901, 29493, 1392, 1137, 2307, 29510, 29475, 2477, 1066, 7799, 1475, 6800, 29491, 1429, 1171, 27856, 10717, 1054, 29493, 1163, 1032, 9655, 12143, 10734, 1206, 1639, 5750, 29491, 17945, 4249, 29510, 29475, 15673, 29513, 1168, 1029, 1423, 2860, 1072, 22643, 29493, 2764, 1040, 22064, 13901, 27167, 1546, 2401, 29491, 781, 781, 2966, 1040, 13901, 3665, 29493, 1040, 5894, 7682, 7542, 1163, 1032, 3432, 29493, 8279, 1165, 1050, 2829, 29491, 17945, 3150, 1350, 29493, 1917, 1774, 29493, 1158, 1040, 2771, 3893, 1066, 3036, 29491, 1183, 8901, 24484, 2521, 29493, 9707, 1254, 1040, 3726, 1414, 14662, 1070, 1032, 12143, 19368, 9381, 29491, 1418, 1597, 4702, 18979, 29589, 29476, 3739, 29510, 29481, 18979, 29589, 1159, 1040, 16337, 9738, 1942, 1070, 1032, 4875, 1030, 29491, 3247, 29493, 1158, 7682, 1158, 1146, 1321, 14492, 29493, 1040, 8789, 7822, 29491, 1183, 5894, 1171, 1158, 1146, 1321, 2511, 1518, 29493, 7542, 1163, 1040, 6651, 7252, 1056, 1070, 1301, 8480, 29491, 781, 781, 29517, 1912, 1171, 1248, 3357, 1330, 8445, 1094, 3233, 29491, 1418, 5021, 1040, 3822, 1066, 2435, 1064, 29493, 21168, 1535, 1321, 5011, 1330, 7053, 1343, 1040, 1512, 1452, 1040, 8789, 29491, 2435, 1064, 15880, 1484, 3018, 29493, 1636, 2824, 1040, 3822, 1072, 7747, 1054, 17945, 1122, 1284, 1539, 29491, 781, 781, 2966, 2435, 1064, 2517, 1040, 5894, 29493, 17945, 4249, 29510, 29475, 2084, 1330, 2369, 1032, 1052, 1370, 1070, 19137, 29491, 1418, 1321, 2040, 1089, 1550, 1054, 2313, 21695, 29493, 2313, 1137, 25098, 3276, 1040, 4356, 25135, 1070, 3822, 15495, 29491, 1418, 2187, 3490, 11882, 1137, 2435, 1064, 1450, 1372, 29493, 1137, 1040, 18260, 1070, 1040, 3822, 3491, 29510, 29475, 1522, 29491, 781, 781, 29525, 1516, 3665, 1546, 5355, 29493, 1072, 17945, 2187, 3490, 13875, 1478, 19330, 1591, 1163, 1040, 4694, 12796, 2435, 1064, 29510, 29481, 3822, 29491, 1418, 5991, 4084, 2727, 1056, 1522, 2339, 20105, 1072, 3822, 15495, 1444, 13841, 29493, 13637, 1122, 1475, 5157, 1070, 1032, 3822, 1137, 4104, 13375, 1210, 2485, 1362, 29491, 3122, 1586, 6362, 8876, 29493, 7056, 1056, 1284, 8171, 1072, 13000, 29491, 781, 781, 6935, 7624, 29493, 1158, 17945, 1171, 1452, 1066, 3616, 1284, 5894, 29493, 1168, 4132, 1032, 3432, 14244, 1206, 1040, 3019, 29491, 8381, 1056, 4304, 1171, 1032, 3739, 29493, 1327, 3050, 7542, 1163, 1032, 8996, 10503, 29492, 17568, 29491, 1753, 8928, 5514, 1158, 11087, 1288, 29493, 2435, 1064, 29510, 29481, 6436, 29491, 1328, 1327, 1789, 1171, 1040, 11353, 3822, 29491, 781, 781, 29507, 2996, 1064, 3008, 1296, 1535, 5011, 1630, 11087, 1288, 1541, 29493, 1327, 4209, 3432, 3551, 4213, 1056, 29491, 1113, 3196, 1541, 1136, 3440, 2313, 1869, 15382, 1379, 781, 781, 29517, 1912, 8081, 29493, 1284, 3799, 16667, 1056, 29491, 1113, 6381, 29493, 1083, 1631, 29491, 1098, 9381, 29493, 18979, 29493, 4195, 29491, 1429, 1171, 1869, 1249, 12514, 1056, 1379, 781, 781, 29526, 29492, 1288, 29510, 29481, 3050, 2192, 1284, 29493, 1072, 1398, 8738, 29491, 1113, 1782, 3822, 1117, 1448, 1589, 1544, 1032, 1495, 17920, 29491, 1429, 29510, 29481, 1032, 2713, 1066, 1581, 2773, 29510, 29481, 18192, 29493, 1032, 10630, 1896, 1066, 13375, 1811, 3377, 29491, 1860, 1146, 29510, 29481, 1603, 1032, 19164, 29493, 1032, 10448, 29491, 5635, 1136, 8581, 1066, 2084, 1360, 7864, 1137, 19164, 29493, 17945, 1878, 781, 781, 29517, 1912, 2307, 29510, 29475, 11584, 10415, 29491, 1113, 6381, 1630, 1168, 1541, 29493, 1284, 4209, 15657, 1072, 2632, 29491, 1113, 29505, 1605, 1379, 781, 781, 3235, 1347, 29493, 17945, 2187, 3490, 11189, 1546, 1032, 2294, 2850, 6066, 1040, 6651, 11236, 1070, 1119, 2926, 26112, 1101, 29493, 1032, 2294, 1738, 1495, 1171, 1227, 1544, 13588, 1330, 10984, 29493, 1738, 1780, 7252, 1070, 1032, 10685, 4104, 1032, 3606, 6113, 1066, 1115, 3008, 29491, 781, 781, 8163, 781, 781, 29537, 2613, 1137, 8031, 29576, 2459, 1504, 3192, 1880, 1136, 1695, 29572, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' ```json\n{\n  "follow_ups": [\n    "Can you continue the story of Eli and the magical watch?",\n    "What kind of responsibilities come with the watch’s legacy?",\n    "Could you write another story set in a similar magical world?",\n    "Does Eli ever find out more about the mysterious gear in the watch?"\n  ]\n}\n```', 'output_ids': [13700, 29498, 15973, 29535, 4, 9557, 4139, 781, 29519, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 7093, 1136, 4456, 1040, 3606, 1070, 17945, 1072, 1040, 21695, 3822, 29572, 1316, 781, 3055, 1113, 3963, 2880, 1070, 4327, 11564, 2335, 1163, 1040, 3822, 29577, 29481, 18192, 29572, 1316, 781, 3055, 1113, 17792, 1136, 4092, 2466, 3606, 1576, 1065, 1032, 4452, 21695, 2294, 29572, 1316, 781, 3055, 1113, 21278, 17945, 3038, 2068, 1343, 1448, 1452, 1040, 22064, 13901, 1065, 1040, 3822, 1878, 781, 29473, 5477, 781, 29520, 781, 14708, 29600, 2], 'meta_info': {'id': '47060243c68341afbde2dae4609a3806', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 1325, 'weight_version': 'default', 'completion_tokens': 87, 'cached_tokens': 1, 'e2e_latency': 7.792140007019043}}
[2025-10-12 23:06:06] INFO:     127.0.0.1:44506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-12 23:11:27] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-12 23:11:28] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/start_sglang_mistral_tp4.sh: line 17: 294280 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/Mistral-Large-Instruct-2411-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name mistral-large-2411-awq --mem-fraction-static 0.80 --log-level info --log-requests
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:14:18] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=32768, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=208283408, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:14:18] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:18] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:14:25 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:14:25 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:25 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:25 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:25 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:14:25 TP0] Init torch distributed begin.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:14:26 TP0] sglang is using nccl==2.27.3
[2025-10-12 23:14:26 TP1] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[2025-10-12 23:14:26 TP0] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:14:26 TP0] Init torch distributed ends. mem usage=0.15 GB
[2025-10-12 23:14:26 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:14:26 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:14:26 TP0] Load weight begin. avail mem=23.15 GB
[2025-10-12 23:14:27 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:14:27] Received sigquit from a child process. It usually means the child failed.
[2025-10-12 23:14:27 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 1 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:14:27] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_mistral_tp2.sh: line 20: 427580 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/Mistral-Large-Instruct-2411-AWQ --host 0.0.0.0 --port 8001 --tp 2 --context-length 32768 --quantization awq_marlin --served-model-name mistral-large-2411-awq --mem-fraction-static 0.85 --log-level info --log-requests
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:14:44] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=32768, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.85, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=573423195, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:14:44] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:44] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:14:51 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:51 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:51 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:14:51 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:14:51 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:14:51 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:14:51 TP0] sglang is using nccl==2.27.3
[2025-10-12 23:14:51 TP0] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[2025-10-12 23:14:51 TP1] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:14:51 TP0] Init torch distributed ends. mem usage=0.15 GB
[2025-10-12 23:14:51 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:14:51 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:14:52 TP0] Load weight begin. avail mem=23.15 GB
[2025-10-12 23:14:52 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:14:52] Received sigquit from a child process. It usually means the child failed.
[2025-10-12 23:14:52 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 1 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:14:52] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_mistral_tp2.sh: line 20: 432074 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/Mistral-Large-Instruct-2411-AWQ --host 0.0.0.0 --port 8001 --tp 2 --context-length 32768 --quantization awq_marlin --served-model-name mistral-large-2411-awq --mem-fraction-static 0.85 --log-level info --log-requests
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:15:15] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=904754304, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:15:15] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:15] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:15:22 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:22 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:22 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:15:22 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:15:22 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:22 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:15:22 TP0] sglang is using nccl==2.27.3
[2025-10-12 23:15:23 TP1] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[2025-10-12 23:15:23 TP0] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:15:23 TP0] Init torch distributed ends. mem usage=0.15 GB
[2025-10-12 23:15:23 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:15:23 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:15:23 TP0] Load weight begin. avail mem=23.15 GB
[2025-10-12 23:15:24 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:15:24] Received sigquit from a child process. It usually means the child failed.
[2025-10-12 23:15:24 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 1 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:15:24] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_mistral_tp2.sh: line 20: 433130 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/Mistral-Large-Instruct-2411-AWQ --host 0.0.0.0 --port 8001 --tp 2 --context-length 24576 --quantization awq_marlin --served-model-name mistral-large-2411-awq --mem-fraction-static 0.80 --log-level info --log-requests
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:15:41] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=431437953, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:15:41] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:41] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:15:48 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:48 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:48 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:15:48 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:15:48 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:15:48 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:15:49 TP0] sglang is using nccl==2.27.3
[2025-10-12 23:15:49 TP0] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[2025-10-12 23:15:49 TP1] Setup Custom allreduce failed with CUDART error: peer access is not supported between these two devices. To silence this warning, specify --disable-custom-all-reduce explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[2025-10-12 23:15:49 TP0] Init torch distributed ends. mem usage=0.15 GB
[2025-10-12 23:15:49 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:15:49 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:15:49 TP0] Load weight begin. avail mem=23.15 GB
[2025-10-12 23:15:50 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:15:50] Received sigquit from a child process. It usually means the child failed.
[2025-10-12 23:15:50 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 425, in __init__
    self.model = self._init_model(config, quant_config, add_prefix("model", prefix))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 457, in _init_model
    return LlamaModel(config, quant_config=quant_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 300, in __init__
    self.layers, self.start_layer, self.end_layer = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 504, in make_layers
    + get_offloader().wrap_modules(
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/offloader.py", line 36, in wrap_modules
    return list(all_modules_generator)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 506, in <genexpr>
    layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 302, in <lambda>
    lambda idx, prefix: LlamaDecoderLayer(
                        ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 241, in __init__
    self.mlp = LlamaMLP(
               ^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/llama.py", line 72, in __init__
    self.gate_up_proj = MergedColumnParallelLinear(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 484, in __init__
    super().__init__(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/linear.py", line 321, in __init__
    self.quant_method.create_weights(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 462, in create_weights
    data=torch.empty(
         ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/utils/_device.py", line 103, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 168.00 MiB. GPU 1 has a total capacity of 23.56 GiB of which 165.19 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 22.98 GiB is allocated by PyTorch, and 13.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

[2025-10-12 23:15:50] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_mistral_tp2.sh: line 20: 434054 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/Mistral-Large-Instruct-2411-AWQ --host 0.0.0.0 --port 8001 --tp 2 --context-length 24576 --quantization awq_marlin --served-model-name mistral-large-2411-awq --mem-fraction-static 0.80 --log-level info --log-requests
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:16:07] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=598852869, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:16:07] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:07] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/start_sglang_mistral_tp2.sh: line 20: 435035 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/Mistral-Large-Instruct-2411-AWQ --host 0.0.0.0 --port 8001 --tp 2 --context-length 24576 --quantization awq_marlin --served-model-name mistral-large-2411-awq --mem-fraction-static 0.80 --log-level info --log-requests
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:16:26] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=2, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=915519542, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=16, cuda_graph_bs=[1, 2, 4, 8, 12, 16], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:16:26] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:26] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:16:39] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=37093788, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:16:39] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:39] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:16:47 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:47 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:16:47 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:16:47 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:47 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:16:47 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:47 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:47 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:16:47 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:16:47 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:16:48 TP0] sglang is using nccl==2.27.3
[2025-10-12 23:16:48 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:16:48 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:16:48 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:16:48 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:16:48 TP0] Init torch distributed ends. mem usage=0.14 GB
[2025-10-12 23:16:48 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:16:48 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:16:48 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:16:48 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:16:49 TP0] Load weight begin. avail mem=23.16 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:09,  1.40it/s]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:01<00:08,  1.39it/s]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:02<00:08,  1.36it/s]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:02<00:07,  1.36it/s]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:03<00:06,  1.33it/s]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:04<00:05,  1.35it/s]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:05<00:05,  1.37it/s]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:05<00:03,  1.81it/s]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:05<00:02,  1.71it/s]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:06<00:02,  1.65it/s]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:07<00:01,  1.59it/s]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:08<00:01,  1.51it/s]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:08<00:00,  1.46it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.44it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.48it/s]

[2025-10-12 23:17:00 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.84 GB, mem usage=15.32 GB.
[2025-10-12 23:17:01 TP0] Using KV cache dtype: torch.float16
[2025-10-12 23:17:01 TP1] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:17:01 TP3] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:17:01 TP2] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:17:01 TP0] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:17:01 TP0] Memory pool end. avail mem=4.25 GB
[2025-10-12 23:17:01 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.74 GB
[2025-10-12 23:17:01 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   7%|▋         | 1/14 [00:01<00:16,  1.27s/it]Capturing batches (bs=72 avail_mem=3.65 GB):   7%|▋         | 1/14 [00:01<00:16,  1.27s/it]Capturing batches (bs=72 avail_mem=3.65 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.14it/s]Capturing batches (bs=64 avail_mem=3.64 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.14it/s]Capturing batches (bs=64 avail_mem=3.64 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.46it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.46it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.69it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.69it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.94it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.94it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.19it/s]Capturing batches (bs=32 avail_mem=3.56 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.19it/s]Capturing batches (bs=32 avail_mem=3.56 GB):  50%|█████     | 7/14 [00:03<00:02,  2.47it/s]Capturing batches (bs=24 avail_mem=3.55 GB):  50%|█████     | 7/14 [00:03<00:02,  2.47it/s]Capturing batches (bs=24 avail_mem=3.55 GB):  57%|█████▋    | 8/14 [00:04<00:02,  2.84it/s]Capturing batches (bs=16 avail_mem=3.52 GB):  57%|█████▋    | 8/14 [00:04<00:02,  2.84it/s]Capturing batches (bs=16 avail_mem=3.52 GB):  64%|██████▍   | 9/14 [00:04<00:01,  3.23it/s]Capturing batches (bs=12 avail_mem=3.51 GB):  64%|██████▍   | 9/14 [00:04<00:01,  3.23it/s]Capturing batches (bs=12 avail_mem=3.51 GB):  71%|███████▏  | 10/14 [00:04<00:01,  3.60it/s]Capturing batches (bs=8 avail_mem=3.48 GB):  71%|███████▏  | 10/14 [00:04<00:01,  3.60it/s] Capturing batches (bs=8 avail_mem=3.48 GB):  79%|███████▊  | 11/14 [00:04<00:00,  4.08it/s]Capturing batches (bs=4 avail_mem=3.47 GB):  79%|███████▊  | 11/14 [00:04<00:00,  4.08it/s]Capturing batches (bs=4 avail_mem=3.47 GB):  86%|████████▌ | 12/14 [00:04<00:00,  4.46it/s]Capturing batches (bs=2 avail_mem=3.43 GB):  86%|████████▌ | 12/14 [00:04<00:00,  4.46it/s]Capturing batches (bs=2 avail_mem=3.43 GB):  93%|█████████▎| 13/14 [00:04<00:00,  4.84it/s]Capturing batches (bs=1 avail_mem=3.42 GB):  93%|█████████▎| 13/14 [00:04<00:00,  4.84it/s]Capturing batches (bs=1 avail_mem=3.42 GB): 100%|██████████| 14/14 [00:05<00:00,  4.75it/s]Capturing batches (bs=1 avail_mem=3.42 GB): 100%|██████████| 14/14 [00:05<00:00,  2.70it/s]
[2025-10-12 23:17:07 TP0] Capture cuda graph end. Time elapsed: 5.80 s. mem usage=0.35 GB. avail mem=3.40 GB.
[2025-10-12 23:17:07 TP0] max_total_num_tokens=38187, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=3.40 GB
[2025-10-12 23:17:07] INFO:     Started server process [435971]
[2025-10-12 23:17:07] INFO:     Waiting for application startup.
[2025-10-12 23:17:07] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:17:07] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:17:07] INFO:     Application startup complete.
[2025-10-12 23:17:07] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-12 23:17:08] INFO:     127.0.0.1:46046 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-12 23:17:08] Receive: obj=GenerateReqInput(rid='61175ee687554aafa41e6af9002623dc', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:17:08 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:17:09] Finish: obj=GenerateReqInput(rid='61175ee687554aafa41e6af9002623dc', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': '61175ee687554aafa41e6af9002623dc', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 0.7348902225494385}}
[2025-10-12 23:17:09] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-12 23:17:09] The server is fired up and ready to roll!
[2025-10-12 23:17:21] INFO:     127.0.0.1:34308 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-12 23:17:50] Receive: obj=GenerateReqInput(rid='400fdee538e945baad805a6ece7cd925', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:17:50 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:17:51 TP0] Decode batch. #running-req: 1, #token: 45, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.91, #queue-req: 0, 
[2025-10-12 23:17:52 TP0] Decode batch. #running-req: 1, #token: 85, token usage: 0.00, cuda graph: True, gen throughput (token/s): 31.60, #queue-req: 0, 
[2025-10-12 23:17:53 TP0] Decode batch. #running-req: 1, #token: 125, token usage: 0.00, cuda graph: True, gen throughput (token/s): 30.76, #queue-req: 0, 
[2025-10-12 23:17:55] Finish: obj=GenerateReqInput(rid='400fdee538e945baad805a6ece7cd925', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, the physics of very small things like atoms and subatomic particles. Unlike classical computers that use bits to process information, quantum computers use something called quantum bits, or qubits.\n\n### Understanding Qubits\n\nIn classical computing, a bit can be either 0 or 1. In quantum computing, a qubit can be in multiple states at the same time, thanks to a property called superposition. This means a qubit can be 0, 1, or both 0 and 1 simultaneously. This allows quantum computers to process a vast amount of possibilities at once, making them potentially much faster for certain tasks.\n\n', 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1040, 16941, 1070, 1983, 2509, 2490, 1505, 24989, 1072, 1851, 15203, 14706, 29491, 18151, 14146, 19286, 1137, 1706, 10124, 1066, 2527, 2639, 29493, 11683, 19286, 1706, 2313, 2755, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 781, 781, 28100, 7373, 9277, 1954, 1205, 1814, 781, 781, 1425, 14146, 22031, 29493, 1032, 3054, 1309, 1115, 3245, 29473, 29502, 1210, 29473, 29508, 29491, 1328, 11683, 22031, 29493, 1032, 1294, 3349, 1309, 1115, 1065, 5934, 5373, 1206, 1040, 2116, 1495, 29493, 8964, 1066, 1032, 4089, 2755, 2963, 4408, 29491, 1619, 3593, 1032, 1294, 3349, 1309, 1115, 29473, 29502, 29493, 29473, 29508, 29493, 1210, 2328, 29473, 29502, 1072, 29473, 29508, 18902, 29491, 1619, 6744, 11683, 19286, 1066, 2527, 1032, 10323, 4326, 1070, 18821, 1206, 3095, 29493, 3260, 1474, 14725, 1956, 10324, 1122, 3320, 10564, 29491, 781, 781], 'meta_info': {'id': '400fdee538e945baad805a6ece7cd925', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 1, 'e2e_latency': 5.072768211364746}}
[2025-10-12 23:17:55] INFO:     127.0.0.1:55026 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:20:48] Receive: obj=GenerateReqInput(rid='5998432a96bb447884954572fb16a04b', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:20:48 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:20:49 TP0] Decode batch. #running-req: 1, #token: 15, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.23, #queue-req: 0, 
[2025-10-12 23:20:50 TP0] Decode batch. #running-req: 1, #token: 55, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.03, #queue-req: 0, 
[2025-10-12 23:20:52 TP0] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.05, #queue-req: 0, 
[2025-10-12 23:20:54 TP0] Decode batch. #running-req: 1, #token: 135, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.96, #queue-req: 0, 
[2025-10-12 23:20:56] Finish: obj=GenerateReqInput(rid='5998432a96bb447884954572fb16a04b', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, a branch of physics that deals with very small things like atoms and subatomic particles. Unlike classical computers that use bits to process information, quantum computers use something called quantum bits, or qubits.\n\n### Understanding Qubits\n\nA classical bit can be either 0 or 1. A qubit, however, can be in a state of 0, 1, or both 0 and 1 at the same time, thanks to a property called superposition. This means a quantum computer can process a vast amount of possibilities simultaneously, which can make it much faster for certain types of computations.\n\n### Quantum Super', 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1032, 10567, 1070, 16941, 1137, 15453, 1163, 1983, 2509, 2490, 1505, 24989, 1072, 1851, 15203, 14706, 29491, 18151, 14146, 19286, 1137, 1706, 10124, 1066, 2527, 2639, 29493, 11683, 19286, 1706, 2313, 2755, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 781, 781, 28100, 7373, 9277, 1954, 1205, 1814, 781, 781, 29509, 14146, 3054, 1309, 1115, 3245, 29473, 29502, 1210, 29473, 29508, 29491, 1098, 1294, 3349, 29493, 4313, 29493, 1309, 1115, 1065, 1032, 2433, 1070, 29473, 29502, 29493, 29473, 29508, 29493, 1210, 2328, 29473, 29502, 1072, 29473, 29508, 1206, 1040, 2116, 1495, 29493, 8964, 1066, 1032, 4089, 2755, 2963, 4408, 29491, 1619, 3593, 1032, 11683, 6842, 1309, 2527, 1032, 10323, 4326, 1070, 18821, 18902, 29493, 1458, 1309, 1806, 1146, 1956, 10324, 1122, 3320, 5282, 1070, 4401, 1465, 29491, 781, 781, 28100, 23423, 1151, 6845], 'meta_info': {'id': '5998432a96bb447884954572fb16a04b', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 7.297205924987793}}
[2025-10-12 23:20:56] INFO:     127.0.0.1:35008 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:21:22] Receive: obj=GenerateReqInput(rid='fcb3c8d773a244e1b95fc871aadb4f97', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:21:22 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:21:23 TP0] Decode batch. #running-req: 1, #token: 25, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.42, #queue-req: 0, 
[2025-10-12 23:21:24 TP0] Decode batch. #running-req: 1, #token: 65, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.04, #queue-req: 0, 
[2025-10-12 23:21:26 TP0] Decode batch. #running-req: 1, #token: 105, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.99, #queue-req: 0, 
[2025-10-12 23:21:28 TP0] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.67, #queue-req: 0, 
[2025-10-12 23:21:29] Finish: obj=GenerateReqInput(rid='fcb3c8d773a244e1b95fc871aadb4f97', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "\n\nSure, I'd be happy to explain quantum computing in simple terms!\n\n### Traditional Computing vs. Quantum Computing\n\n**Traditional Computing:**\n- **Bits:** Traditional computers use bits as the smallest unit of data. A bit can be either 0 or 1.\n- **Logic Gates:** These bits are manipulated using logic gates (like AND, OR, NOT) to perform calculations.\n- **Sequential Processing:** Traditional computers process one bit at a time, or in small groups, sequentially.\n\n**Quantum Computing:**\n- **Qubits:** Quantum computers use qubits instead of bits. A qubit can", 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 22867, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 8050, 11683, 22031, 1065, 4356, 4239, 29576, 781, 781, 28100, 18349, 3350, 14419, 1056, 8477, 29491, 23423, 1151, 14419, 1056, 781, 781, 1116, 2238, 1084, 3350, 14419, 1056, 4817, 781, 29501, 1387, 15489, 4817, 18349, 3350, 19286, 1706, 10124, 1158, 1040, 23109, 5796, 1070, 1946, 29491, 1098, 3054, 1309, 1115, 3245, 29473, 29502, 1210, 29473, 29508, 29491, 781, 29501, 1387, 3531, 1062, 1188, 1770, 4817, 3725, 10124, 1228, 14081, 7200, 2181, 12176, 22393, 1093, 4859, 6539, 29493, 4762, 29493, 6225, 29499, 1066, 2993, 19819, 29491, 781, 29501, 1387, 11776, 3024, 11473, 1056, 4817, 18349, 3350, 19286, 2527, 1392, 3054, 1206, 1032, 1495, 29493, 1210, 1065, 2509, 5706, 29493, 6099, 8114, 29491, 781, 781, 1116, 20469, 1151, 14419, 1056, 4817, 781, 29501, 1387, 29592, 1205, 1814, 4817, 23423, 1151, 19286, 1706, 1294, 8437, 4287, 1070, 10124, 29491, 1098, 1294, 3349, 1309], 'meta_info': {'id': 'fcb3c8d773a244e1b95fc871aadb4f97', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 7.223806381225586}}
[2025-10-12 23:21:29] INFO:     127.0.0.1:57364 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:21:49] Receive: obj=GenerateReqInput(rid='951311a97b3842e185d18837deda3b8c', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:21:49 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:21:50 TP0] Decode batch. #running-req: 1, #token: 35, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.87, #queue-req: 0, 
[2025-10-12 23:21:51 TP0] Decode batch. #running-req: 1, #token: 75, token usage: 0.00, cuda graph: True, gen throughput (token/s): 26.21, #queue-req: 0, 
[2025-10-12 23:21:53 TP0] Decode batch. #running-req: 1, #token: 115, token usage: 0.00, cuda graph: True, gen throughput (token/s): 26.44, #queue-req: 0, 
[2025-10-12 23:21:54 TP0] Decode batch. #running-req: 1, #token: 155, token usage: 0.00, cuda graph: True, gen throughput (token/s): 26.67, #queue-req: 0, 
[2025-10-12 23:21:55] Finish: obj=GenerateReqInput(rid='951311a97b3842e185d18837deda3b8c', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics to process information. Unlike classical computers, which use bits to represent and process data as either 0s or 1s, quantum computers use quantum bits, or qubits. Here's a simple breakdown:\n\n### 1. **Classical vs. Quantum Computing**\n- **Classical Computing**: Uses bits, which can be either 0 or 1.\n- **Quantum Computing**: Uses qubits, which can be in multiple states simultaneously thanks to a property called superposition.\n\n### 2. **Superposition**\nSuperposition allows a qubit to exist in a combination of states", 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 1066, 2527, 2639, 29491, 18151, 14146, 19286, 29493, 1458, 1706, 10124, 1066, 3672, 1072, 2527, 1946, 1158, 3245, 29473, 29502, 29481, 1210, 29473, 29508, 29481, 29493, 11683, 19286, 1706, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 4771, 29510, 29481, 1032, 4356, 27755, 29515, 781, 781, 28100, 29473, 29508, 29491, 1387, 3240, 1513, 8477, 29491, 23423, 1151, 14419, 1056, 1116, 781, 29501, 1387, 3240, 1513, 14419, 1056, 9957, 5315, 1042, 10124, 29493, 1458, 1309, 1115, 3245, 29473, 29502, 1210, 29473, 29508, 29491, 781, 29501, 1387, 20469, 1151, 14419, 1056, 9957, 5315, 1042, 1294, 8437, 29493, 1458, 1309, 1115, 1065, 5934, 5373, 18902, 8964, 1066, 1032, 4089, 2755, 2963, 4408, 29491, 781, 781, 28100, 29473, 29518, 29491, 1387, 16271, 4408, 1116, 781, 16271, 4408, 6744, 1032, 1294, 3349, 1066, 3356, 1065, 1032, 10238, 1070, 5373], 'meta_info': {'id': '951311a97b3842e185d18837deda3b8c', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 5.830449819564819}}
[2025-10-12 23:21:55] INFO:     127.0.0.1:60774 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:22:10] Receive: obj=GenerateReqInput(rid='868eedef935c4d6483be512fe562fa1f', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:22:10 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:22:11 TP0] Decode batch. #running-req: 1, #token: 45, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.39, #queue-req: 0, 
[2025-10-12 23:22:12 TP0] Decode batch. #running-req: 1, #token: 85, token usage: 0.00, cuda graph: True, gen throughput (token/s): 31.53, #queue-req: 0, 
[2025-10-12 23:22:14 TP0] Decode batch. #running-req: 1, #token: 125, token usage: 0.00, cuda graph: True, gen throughput (token/s): 29.33, #queue-req: 0, 
[2025-10-12 23:22:15] Finish: obj=GenerateReqInput(rid='868eedef935c4d6483be512fe562fa1f', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics to process information. Unlike classical computers that use bits (which can be either 0 or 1), quantum computers use quantum bits, or qubits. Here's a simple breakdown:\n\n1. **Qubits**: While classical bits can be only 0 or 1, qubits can be in multiple states at once, thanks to a property called superposition. Imagine a qubit as a spinning coin—while it's in the air, it's both heads and tails at the same time, but once it lands (or is measured), it's either heads or tails.\n\n2. **Superposition**:", 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 1066, 2527, 2639, 29491, 18151, 14146, 19286, 1137, 1706, 10124, 1093, 7742, 1309, 1115, 3245, 29473, 29502, 1210, 29473, 29508, 1325, 11683, 19286, 1706, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 4771, 29510, 29481, 1032, 4356, 27755, 29515, 781, 781, 29508, 29491, 1387, 29592, 1205, 1814, 9957, 4791, 14146, 10124, 1309, 1115, 1633, 29473, 29502, 1210, 29473, 29508, 29493, 1294, 8437, 1309, 1115, 1065, 5934, 5373, 1206, 3095, 29493, 8964, 1066, 1032, 4089, 2755, 2963, 4408, 29491, 22799, 1032, 1294, 3349, 1158, 1032, 26850, 17312, 29589, 6181, 1146, 29510, 29481, 1065, 1040, 3191, 29493, 1146, 29510, 29481, 2328, 11246, 1072, 1029, 2782, 1206, 1040, 2116, 1495, 29493, 1330, 3095, 1146, 16677, 1093, 1039, 1117, 13588, 1325, 1146, 29510, 29481, 3245, 11246, 1210, 1029, 2782, 29491, 781, 781, 29518, 29491, 1387, 16271, 4408, 9957], 'meta_info': {'id': '868eedef935c4d6483be512fe562fa1f', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 5.091456651687622}}
[2025-10-12 23:22:15] INFO:     127.0.0.1:50902 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:22:25] Receive: obj=GenerateReqInput(rid='0eed975b82c04de8b28efd553e38cc2c', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:22:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:22:25 TP0] Decode batch. #running-req: 1, #token: 15, token usage: 0.00, cuda graph: True, gen throughput (token/s): 3.45, #queue-req: 0, 
[2025-10-12 23:22:27 TP0] Decode batch. #running-req: 1, #token: 55, token usage: 0.00, cuda graph: True, gen throughput (token/s): 29.92, #queue-req: 0, 
[2025-10-12 23:22:28 TP0] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, cuda graph: True, gen throughput (token/s): 28.41, #queue-req: 0, 
[2025-10-12 23:22:29 TP0] Decode batch. #running-req: 1, #token: 135, token usage: 0.00, cuda graph: True, gen throughput (token/s): 31.43, #queue-req: 0, 
[2025-10-12 23:22:30] Finish: obj=GenerateReqInput(rid='0eed975b82c04de8b28efd553e38cc2c', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '\n\n### Quantum Computing: A Simple Explanation\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, the branch of physics that deals with the behavior of particles at the atomic and subatomic levels. Unlike classical computers, which use bits to process information, quantum computers use quantum bits, or qubits.\n\n#### What Are Qubits?\n\nA classical bit can be in one of two states: 0 or 1. A qubit, on the other hand, can be in a superposition of both 0 and 1 simultaneously. This means that a qubit can represent multiple states at once, leading to a massive increase in computing power and speed for certain types of', 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 28100, 23423, 1151, 14419, 1056, 29515, 1098, 14656, 2297, 11777, 1120, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1040, 10567, 1070, 16941, 1137, 15453, 1163, 1040, 6942, 1070, 14706, 1206, 1040, 19859, 1072, 1851, 15203, 6925, 29491, 18151, 14146, 19286, 29493, 1458, 1706, 10124, 1066, 2527, 2639, 29493, 11683, 19286, 1706, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 781, 781, 2768, 2592, 5635, 1954, 1205, 1814, 29572, 781, 781, 29509, 14146, 3054, 1309, 1115, 1065, 1392, 1070, 1757, 5373, 29515, 29473, 29502, 1210, 29473, 29508, 29491, 1098, 1294, 3349, 29493, 1124, 1040, 1567, 1789, 29493, 1309, 1115, 1065, 1032, 2963, 4408, 1070, 2328, 29473, 29502, 1072, 29473, 29508, 18902, 29491, 1619, 3593, 1137, 1032, 1294, 3349, 1309, 3672, 5934, 5373, 1206, 3095, 29493, 6142, 1066, 1032, 9898, 6015, 1065, 22031, 2750, 1072, 5743, 1122, 3320, 5282, 1070], 'meta_info': {'id': '0eed975b82c04de8b28efd553e38cc2c', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 5.060202121734619}}
[2025-10-12 23:22:30] INFO:     127.0.0.1:45192 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:23:03] INFO:     127.0.0.1:50568 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:23:07] INFO:     127.0.0.1:53356 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:23:07] INFO:     127.0.0.1:53358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-12 23:23:07] Receive: obj=GenerateReqInput(rid='d2bc27900b344c29952ad3651ca72f6e', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:23:07 TP0] Prefill batch. #new-seq: 1, #new-token: 80, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:23:07 TP0] Decode batch. #running-req: 1, #token: 94, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.05, #queue-req: 0, 
[2025-10-12 23:23:09 TP0] Decode batch. #running-req: 1, #token: 134, token usage: 0.00, cuda graph: True, gen throughput (token/s): 31.80, #queue-req: 0, 
[2025-10-12 23:23:10 TP0] Decode batch. #running-req: 1, #token: 174, token usage: 0.00, cuda graph: True, gen throughput (token/s): 31.97, #queue-req: 0, 
[2025-10-12 23:23:11 TP0] Decode batch. #running-req: 1, #token: 214, token usage: 0.01, cuda graph: True, gen throughput (token/s): 32.32, #queue-req: 0, 
[2025-10-12 23:23:12 TP0] Decode batch. #running-req: 1, #token: 254, token usage: 0.01, cuda graph: True, gen throughput (token/s): 32.49, #queue-req: 0, 
[2025-10-12 23:23:13 TP0] Decode batch. #running-req: 1, #token: 294, token usage: 0.01, cuda graph: True, gen throughput (token/s): 33.21, #queue-req: 0, 
[2025-10-12 23:23:15 TP0] Decode batch. #running-req: 1, #token: 334, token usage: 0.01, cuda graph: True, gen throughput (token/s): 32.56, #queue-req: 0, 
[2025-10-12 23:23:16 TP0] Decode batch. #running-req: 1, #token: 374, token usage: 0.01, cuda graph: True, gen throughput (token/s): 32.03, #queue-req: 0, 
[2025-10-12 23:23:17 TP0] Decode batch. #running-req: 1, #token: 414, token usage: 0.01, cuda graph: True, gen throughput (token/s): 31.73, #queue-req: 0, 
[2025-10-12 23:23:18 TP0] Decode batch. #running-req: 1, #token: 454, token usage: 0.01, cuda graph: True, gen throughput (token/s): 32.76, #queue-req: 0, 
[2025-10-12 23:23:20 TP0] Decode batch. #running-req: 1, #token: 494, token usage: 0.01, cuda graph: True, gen throughput (token/s): 32.11, #queue-req: 0, 
[2025-10-12 23:23:21 TP0] Decode batch. #running-req: 1, #token: 534, token usage: 0.01, cuda graph: True, gen throughput (token/s): 32.84, #queue-req: 0, 
[2025-10-12 23:23:22 TP0] Decode batch. #running-req: 1, #token: 574, token usage: 0.02, cuda graph: True, gen throughput (token/s): 32.75, #queue-req: 0, 
[2025-10-12 23:23:23 TP0] Decode batch. #running-req: 1, #token: 614, token usage: 0.02, cuda graph: True, gen throughput (token/s): 32.98, #queue-req: 0, 
[2025-10-12 23:23:25 TP0] Decode batch. #running-req: 1, #token: 654, token usage: 0.02, cuda graph: True, gen throughput (token/s): 32.37, #queue-req: 0, 
[2025-10-12 23:23:26 TP0] Decode batch. #running-req: 1, #token: 694, token usage: 0.02, cuda graph: True, gen throughput (token/s): 32.27, #queue-req: 0, 
[2025-10-12 23:23:27 TP0] Decode batch. #running-req: 1, #token: 734, token usage: 0.02, cuda graph: True, gen throughput (token/s): 32.81, #queue-req: 0, 
[2025-10-12 23:23:28 TP0] Decode batch. #running-req: 1, #token: 774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 32.77, #queue-req: 0, 
[2025-10-12 23:23:29 TP0] Decode batch. #running-req: 1, #token: 814, token usage: 0.02, cuda graph: True, gen throughput (token/s): 32.40, #queue-req: 0, 
[2025-10-12 23:23:30] Finish: obj=GenerateReqInput(rid='d2bc27900b344c29952ad3651ca72f6e', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Sure, I\'d be happy to help with that. Here\'s a brief overview of the history of artificial intelligence, which should be around 1000 tokens:\n\nArtificial Intelligence (AI) has evolved significantly since its inception, marked by several key milestones and developments. The term "Artificial Intelligence" was first coined in 1956 by John McCarthy, a computer scientist, at the now-famous Dartmouth Conference. This event brought together leading researchers to discuss the potential of machines that could think and learn like humans.\n\nThe early years of AI were filled with optimism and ambitious predictions. However, the field encountered several setbacks, leading to periods known as "AI winters," where funding and interest declined. These periods were often followed by breakthroughs that reinvigorated AI research.\n\nOne of the earliest achievements in AI was the development of expert systems in the 1970s. These systems were designed to emulate the decision-making abilities of human experts in specific domains, suc' ... 's numerous challenges, such as explainability, bias, and ethical considerations.\n\nThe future of AI holds promises of even more transformative changes. Quantum computing could revolutionize AI by providing unprecedented computational power. Explainable AI (XAI) aims to create AI systems that can explain their decisions in a way that humans can understand. And Artificial General Intelligence (AGI), which refers to AI that can understand, learn, and apply knowledge across a wide range of tasks at a level equal to or beyond human capabilities, remains the ultimate goal for many AI researchers.\n\nIn conclusion, AI has come a long way since its inception. It has the potential to solve complex problems and improve our lives in numerous ways. However, it also presents challenges that need to be addressed responsibly. As AI continues to evolve, so too will our relationship with technology and the world around us.\n\nThat should be around 1000 tokens. If you need more or have any specific topics in mind, feel free to ask!', 'output_ids': [2], 'meta_info': {'id': 'd2bc27900b344c29952ad3651ca72f6e', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 81, 'weight_version': 'default', 'completion_tokens': 741, 'cached_tokens': 1, 'e2e_latency': 23.13361120223999}}
[2025-10-12 23:23:30] INFO:     127.0.0.1:34296 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:23:30] Receive: obj=GenerateReqInput(rid='74521442321943f0b71a5484e50c6223', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 24383, 11885, 2622, 1639, 1065, 2045, 29493, 11495, 1254, 3624, 2713, 2888, 1142, 3170, 1072, 23078, 29491, 1183, 2618, 1113, 11131, 15541, 23859, 29507, 1171, 1675, 1769, 2079, 1065, 29473, 29508, 29542, 29550, 29552, 1254, 2983, 10808, 4384, 29492, 29493, 1032, 6842, 25248, 29493, 1206, 1040, 1823, 29501, 29490, 22994, 1152, 1212, 20891, 13883, 29491, 1619, 2719, 5016, 3321, 6142, 16102, 1066, 4110, 1040, 5396, 1070, 12923, 1137, 1597, 1841, 1072, 3590, 1505, 11357, 29491, 781, 781, 1782, 3703, 2035, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 1072, 28981, 21364, 29491, 3761, 29493, 1040, 2602, 19916, 3624, 1576, 19844, 29493, 6142, 1066, 16540, 3419, 1158, 1113, 12509, 3876, 2300, 1630, 1738, 12899, 1072, 2913, 23811, 29491, 3725, 16540, 1422, 3376, 5729, 1254, 2489, 15736, 29481, 1137, 1080, 9870, 1094, 1039, 1369, 16875, 4100, 29491, 781, 781, 6935, 1070, 1040, 22632, 28632, 1065, 16875, 1171, 1040, 4867, 1070, 8351, 5686, 1065, 1040, 29473, 29508, 29542, 29555, 29502, 29481, 29491, 3725, 5686, 1422, 6450, 1066, 1645, 6980, 1040, 5929, 29501, 15495, 18634, 1070, 3698, 12493, 1065, 3716, 26766, 29493, 2027, 1158, 6482, 22735, 29491, 2074, 2075, 1032, 1576, 1070, 6647, 1072, 12180, 1066, 1806, 1065, 9599, 1072, 3852, 19660, 29491, 781, 781, 1425, 1040, 29473, 29508, 29542, 29551, 29502, 29481, 1072, 29473, 29508, 29542, 29542, 29502, 29481, 29493, 16875, 4100, 17386, 5851, 1448, 1946, 29501, 28137, 14713, 29491, 14021, 18272, 1093, 4595, 1325, 1032, 20572, 1070, 16875, 29493, 4014, 16342, 29491, 19771, 19307, 2682, 19286, 1066, 3590, 1245, 1946, 2439, 2018, 16724, 2775, 2359, 29491, 1619, 4931, 1066, 6632, 9438, 2107, 1065, 6108, 14596, 29493, 4997, 4610, 10225, 29493, 1072, 19169, 1831, 29491, 781, 781, 1782, 29473, 29518, 29508, 1071, 6213, 1427, 3366, 1032, 1373, 3589, 1404, 1070, 2913, 1065, 16875, 29493, 12983, 1254, 1040, 19626, 1070, 3243, 27650, 29493, 8251, 4401, 2017, 2750, 29493, 1072, 9438, 2107, 1065, 19307, 29491, 15740, 18272, 29493, 1032, 10567, 1070, 19771, 1137, 6866, 26494, 12935, 1163, 2055, 13851, 29493, 1427, 12779, 2433, 29501, 1777, 29501, 2005, 29501, 1212, 3671, 1065, 4886, 16875, 10564, 29493, 2027, 1158, 4237, 1072, 9434, 14596, 29491, 781, 781, 6935, 1070, 1040, 1848, 19993, 28632, 1065, 16875, 1117, 27678, 8348, 29493, 6970, 1254, 15740, 29523, 1275, 29491, 1328, 29473, 29518, 29502, 29508, 29552, 29493, 27678, 8348, 16999, 8949, 24713, 1096, 29493, 1392, 1070, 1040, 2294, 29510, 29481, 2598, 3955, 5885, 29493, 1065, 1032, 2257, 29501, 1777, 29501, 13108, 4286, 29491, 1619, 1171, 1032, 6632, 2888, 19325, 1864, 3955, 1117, 1032, 5398, 2807, 1163, 1448, 3340, 12430, 1589, 1504, 1228, 24989, 1065, 1040, 12485, 29493, 3260, 1146, 1032, 15129, 4406, 1122, 16875, 29491, 781, 781, 29506, 20529, 29493, 16875, 1117, 16415, 1546, 2055, 11704, 1070, 1581, 7558, 5389, 29491, 1429, 12518, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 29493, 26845, 5686, 2075, 1254, 26374, 1072, 9469, 29493, 1072, 1787, 1776, 29501, 29483, 17750, 9068, 29491, 3761, 29493, 16875, 1603, 10958, 10464, 11137, 29493, 2027, 1158, 8050, 3205, 29493, 16885, 29493, 1072, 27092, 2685, 1465, 29491, 781, 781, 1782, 4205, 1070, 16875, 9056, 21394, 1070, 1787, 1448, 6284, 1965, 5203, 29491, 23423, 1151, 22031, 1597, 11797, 1421, 16875, 1254, 8269, 27409, 2122, 13481, 4401, 2017, 2750, 29491, 14470, 1194, 1290, 16875, 1093, 29582, 12509, 29499, 21334, 1066, 2999, 16875, 5686, 1137, 1309, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1783, 4719, 15541, 4360, 23859, 1093, 3145, 29505, 1325, 1458, 16422, 1066, 16875, 1137, 1309, 3148, 29493, 3590, 29493, 1072, 6348, 5556, 3441, 1032, 6103, 3587, 1070, 10564, 1206, 1032, 2952, 7298, 1066, 1210, 6066, 3698, 17353, 29493, 8288, 1040, 16614, 6309, 1122, 2055, 16875, 16102, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 29491, 1429, 1427, 1040, 5396, 1066, 12817, 5398, 5186, 1072, 5684, 1581, 5389, 1065, 10464, 5110, 29491, 3761, 29493, 1146, 1603, 16658, 11137, 1137, 1695, 1066, 1115, 17483, 4327, 5680, 29491, 1904, 16875, 11120, 1066, 1451, 6071, 29493, 1347, 2136, 1390, 1581, 4526, 1163, 6282, 1072, 1040, 2294, 2169, 1360, 29491, 781, 781, 4608, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29491, 1815, 1136, 1695, 1448, 1210, 1274, 1475, 3716, 14585, 1065, 3041, 29493, 2369, 2701, 1066, 2228, 29576, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:23:30 TP0] Prefill batch. #new-seq: 1, #new-token: 1056, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:23:33 TP0] Decode batch. #running-req: 1, #token: 1089, token usage: 0.03, cuda graph: True, gen throughput (token/s): 10.14, #queue-req: 0, 
[2025-10-12 23:23:35 TP0] Decode batch. #running-req: 1, #token: 1129, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.59, #queue-req: 0, 
[2025-10-12 23:23:36 TP0] Decode batch. #running-req: 1, #token: 1169, token usage: 0.03, cuda graph: True, gen throughput (token/s): 33.22, #queue-req: 0, 
[2025-10-12 23:23:36] Finish: obj=GenerateReqInput(rid='74521442321943f0b71a5484e50c6223', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 24383, 11885, 2622, 1639, 1065, 2045, 29493, 11495, 1254, 3624, 2713, 2888, 1142, 3170, 1072, 23078, 29491, 1183, 2618, 1113, 11131, 15541, 23859, 29507, 1171, 1675, 1769, 2079, 1065, 29473, 29508, 29542, 29550, 29552, 1254, 2983, 10808, 4384, 29492, 29493, 1032, 6842, 25248, 29493, 1206, 1040, 1823, 29501, 29490, 22994, 1152, 1212, 20891, 13883, 29491, 1619, 2719, 5016, 3321, 6142, 16102, 1066, 4110, 1040, 5396, 1070, 12923, 1137, 1597, 1841, 1072, 3590, 1505, 11357, 29491, 781, 781, 1782, 3703, 2035, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 1072, 28981, 21364, 29491, 3761, 29493, 1040, 2602, 19916, 3624, 1576, 19844, 29493, 6142, 1066, 16540, 3419, 1158, 1113, 12509, 3876, 2300, 1630, 1738, 12899, 1072, 2913, 23811, 29491, 3725, 16540, 1422, 3376, 5729, 1254, 2489, 15736, 29481, 1137, 1080, 9870, 1094, 1039, 1369, 16875, 4100, 29491, 781, 781, 6935, 1070, 1040, 22632, 28632, 1065, 16875, 1171, 1040, 4867, 1070, 8351, 5686, 1065, 1040, 29473, 29508, 29542, 29555, 29502, 29481, 29491, 3725, 5686, 1422, 6450, 1066, 1645, 6980, 1040, 5929, 29501, 15495, 18634, 1070, 3698, 12493, 1065, 3716, 26766, 29493, 2027, 1158, 6482, 22735, 29491, 2074, 2075, 1032, 1576, 1070, 6647, 1072, 12180, 1066, 1806, 1065, 9599, 1072, 3852, 19660, 29491, 781, 781, 1425, 1040, 29473, 29508, 29542, 29551, 29502, 29481, 1072, 29473, 29508, 29542, 29542, 29502, 29481, 29493, 16875, 4100, 17386, 5851, 1448, 1946, 29501, 28137, 14713, 29491, 14021, 18272, 1093, 4595, 1325, 1032, 20572, 1070, 16875, 29493, 4014, 16342, 29491, 19771, 19307, 2682, 19286, 1066, 3590, 1245, 1946, 2439, 2018, 16724, 2775, 2359, 29491, 1619, 4931, 1066, 6632, 9438, 2107, 1065, 6108, 14596, 29493, 4997, 4610, 10225, 29493, 1072, 19169, 1831, 29491, 781, 781, 1782, 29473, 29518, 29508, 1071, 6213, 1427, 3366, 1032, 1373, 3589, 1404, 1070, 2913, 1065, 16875, 29493, 12983, 1254, 1040, 19626, 1070, 3243, 27650, 29493, 8251, 4401, 2017, 2750, 29493, 1072, 9438, 2107, 1065, 19307, 29491, 15740, 18272, 29493, 1032, 10567, 1070, 19771, 1137, 6866, 26494, 12935, 1163, 2055, 13851, 29493, 1427, 12779, 2433, 29501, 1777, 29501, 2005, 29501, 1212, 3671, 1065, 4886, 16875, 10564, 29493, 2027, 1158, 4237, 1072, 9434, 14596, 29491, 781, 781, 6935, 1070, 1040, 1848, 19993, 28632, 1065, 16875, 1117, 27678, 8348, 29493, 6970, 1254, 15740, 29523, 1275, 29491, 1328, 29473, 29518, 29502, 29508, 29552, 29493, 27678, 8348, 16999, 8949, 24713, 1096, 29493, 1392, 1070, 1040, 2294, 29510, 29481, 2598, 3955, 5885, 29493, 1065, 1032, 2257, 29501, 1777, 29501, 13108, 4286, 29491, 1619, 1171, 1032, 6632, 2888, 19325, 1864, 3955, 1117, 1032, 5398, 2807, 1163, 1448, 3340, 12430, 1589, 1504, 1228, 24989, 1065, 1040, 12485, 29493, 3260, 1146, 1032, 15129, 4406, 1122, 16875, 29491, 781, 781, 29506, 20529, 29493, 16875, 1117, 16415, 1546, 2055, 11704, 1070, 1581, 7558, 5389, 29491, 1429, 12518, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 29493, 26845, 5686, 2075, 1254, 26374, 1072, 9469, 29493, 1072, 1787, 1776, 29501, 29483, 17750, 9068, 29491, 3761, 29493, 16875, 1603, 10958, 10464, 11137, 29493, 2027, 1158, 8050, 3205, 29493, 16885, 29493, 1072, 27092, 2685, 1465, 29491, 781, 781, 1782, 4205, 1070, 16875, 9056, 21394, 1070, 1787, 1448, 6284, 1965, 5203, 29491, 23423, 1151, 22031, 1597, 11797, 1421, 16875, 1254, 8269, 27409, 2122, 13481, 4401, 2017, 2750, 29491, 14470, 1194, 1290, 16875, 1093, 29582, 12509, 29499, 21334, 1066, 2999, 16875, 5686, 1137, 1309, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1783, 4719, 15541, 4360, 23859, 1093, 3145, 29505, 1325, 1458, 16422, 1066, 16875, 1137, 1309, 3148, 29493, 3590, 29493, 1072, 6348, 5556, 3441, 1032, 6103, 3587, 1070, 10564, 1206, 1032, 2952, 7298, 1066, 1210, 6066, 3698, 17353, 29493, 8288, 1040, 16614, 6309, 1122, 2055, 16875, 16102, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 29491, 1429, 1427, 1040, 5396, 1066, 12817, 5398, 5186, 1072, 5684, 1581, 5389, 1065, 10464, 5110, 29491, 3761, 29493, 1146, 1603, 16658, 11137, 1137, 1695, 1066, 1115, 17483, 4327, 5680, 29491, 1904, 16875, 11120, 1066, 1451, 6071, 29493, 1347, 2136, 1390, 1581, 4526, 1163, 6282, 1072, 1040, 2294, 2169, 1360, 29491, 781, 781, 4608, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29491, 1815, 1136, 1695, 1448, 1210, 1274, 1475, 3716, 14585, 1065, 3041, 29493, 2369, 2701, 1066, 2228, 29576, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' ```json\n{\n  "follow_ups": [\n    "Can you explain more about the concept of \'AI winters\'?",\n    "What are some of the key challenges currently facing AI?",\n    "How does Quantum Computing relate to AI?",\n    "How does Explainable AI aim to address the issue of explainability in AI systems?",\n    "What are some key differences between Artificial General Intelligence (AGI) and the current AI systems?"\n  ]\n}\n```', 'output_ids': [13700, 29498, 15973, 29535, 4, 9557, 4139, 781, 29519, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 7093, 1136, 8050, 1448, 1452, 1040, 6703, 1070, 1232, 12509, 3876, 2300, 29510, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1509, 1070, 1040, 2713, 11137, 6257, 12749, 16875, 29572, 1316, 781, 3055, 1113, 6428, 2003, 23423, 1151, 14419, 1056, 22274, 1066, 16875, 29572, 1316, 781, 3055, 1113, 6428, 2003, 14470, 1194, 1290, 16875, 7271, 1066, 3730, 1040, 5059, 1070, 8050, 3205, 1065, 16875, 5686, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1509, 2713, 11858, 2212, 4719, 15541, 4360, 23859, 1093, 3145, 29505, 29499, 1072, 1040, 2636, 16875, 5686, 1878, 781, 29473, 5477, 781, 29520, 781, 14708, 29600, 2], 'meta_info': {'id': '74521442321943f0b71a5484e50c6223', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 1057, 'weight_version': 'default', 'completion_tokens': 113, 'cached_tokens': 1, 'e2e_latency': 6.1555163860321045}}
[2025-10-12 23:23:36] INFO:     127.0.0.1:34306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-12 23:34:48] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-12 23:34:48] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:38:26] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=808946078, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:38:26] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:38:26] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:38:33 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:38:33 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:38:33 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:38:33 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:38:33 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:38:33 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:38:33 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:38:33 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:38:33 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:38:34 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:38:34 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:38:34 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:38:34 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:38:34 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:38:34 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:38:34 TP0] Init torch distributed ends. mem usage=0.14 GB
[2025-10-12 23:38:34 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:38:34 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:38:34 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:38:34 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:38:35 TP0] Load weight begin. avail mem=23.16 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:08,  1.46it/s]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:01<00:08,  1.35it/s]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:02<00:08,  1.35it/s]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:02<00:06,  1.43it/s]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:03<00:06,  1.45it/s]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:04<00:05,  1.42it/s]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:04<00:04,  1.44it/s]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:05<00:03,  1.81it/s]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:05<00:02,  1.74it/s]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:06<00:02,  1.61it/s]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:07<00:01,  1.53it/s]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:08<00:01,  1.45it/s]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:08<00:00,  1.44it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.39it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.47it/s]

[2025-10-12 23:38:46 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.84 GB, mem usage=15.32 GB.
[2025-10-12 23:38:47 TP0] Using KV cache dtype: torch.float16
[2025-10-12 23:38:47 TP0] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:38:47 TP1] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:38:47 TP0] Memory pool end. avail mem=4.25 GB
[2025-10-12 23:38:47 TP2] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:38:47 TP3] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:38:48 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.74 GB
[2025-10-12 23:38:48 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   7%|▋         | 1/14 [00:01<00:16,  1.26s/it]Capturing batches (bs=72 avail_mem=3.65 GB):   7%|▋         | 1/14 [00:01<00:16,  1.26s/it]Capturing batches (bs=72 avail_mem=3.65 GB):  14%|█▍        | 2/14 [00:02<00:12,  1.03s/it]Capturing batches (bs=64 avail_mem=3.64 GB):  14%|█▍        | 2/14 [00:02<00:12,  1.03s/it]Capturing batches (bs=64 avail_mem=3.64 GB):  21%|██▏       | 3/14 [00:02<00:08,  1.33it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  21%|██▏       | 3/14 [00:02<00:08,  1.33it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  29%|██▊       | 4/14 [00:02<00:06,  1.63it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  29%|██▊       | 4/14 [00:02<00:06,  1.63it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.95it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.95it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.17it/s]Capturing batches (bs=32 avail_mem=3.56 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.17it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.56 GB):  50%|█████     | 7/14 [00:26<00:53,  7.68s/it]Capturing batches (bs=24 avail_mem=3.54 GB):  50%|█████     | 7/14 [00:26<00:53,  7.68s/it]Capturing batches (bs=24 avail_mem=3.54 GB):  57%|█████▋    | 8/14 [00:50<01:17, 12.98s/it]Capturing batches (bs=16 avail_mem=3.51 GB):  57%|█████▋    | 8/14 [00:50<01:17, 12.98s/it]Capturing batches (bs=16 avail_mem=3.51 GB):  64%|██████▍   | 9/14 [01:22<01:34, 18.90s/it]Capturing batches (bs=12 avail_mem=3.50 GB):  64%|██████▍   | 9/14 [01:22<01:34, 18.90s/it]Capturing batches (bs=12 avail_mem=3.50 GB):  71%|███████▏  | 10/14 [02:17<02:00, 30.19s/it]Capturing batches (bs=8 avail_mem=3.47 GB):  71%|███████▏  | 10/14 [02:17<02:00, 30.19s/it] [rank2]:E1012 23:41:32.395000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank2]:E1012 23:41:32.395000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1012 23:41:32.395000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
[rank0]:E1012 23:41:32.433000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank0]:E1012 23:41:32.433000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1012 23:41:32.433000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_4 0.2510 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_8 0.2519 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_1 0.2621 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_16 0.2642 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_7 0.2652 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_11 0.2693 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2918 ms 86.0% 
  triton_mm_13 0.3348 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_3 0.3462 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_2 0.3523 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5810 seconds and 0.4897 seconds precompiling for 18 choices
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_8 0.2509 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_4 0.2540 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_1 0.2621 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_16 0.2642 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_7 0.2652 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_11 0.2693 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2908 ms 86.3% 
  triton_mm_13 0.3195 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_3 0.3430 ms 73.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_2 0.3543 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6948 seconds and 0.4700 seconds precompiling for 18 choices
[rank3]:E1012 23:41:32.873000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank3]:E1012 23:41:32.873000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1012 23:41:32.873000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
[rank1]:E1012 23:41:32.991000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank1]:E1012 23:41:32.991000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1012 23:41:32.991000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_8 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_4 0.2529 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_1 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_16 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_7 0.2662 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_11 0.2703 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2949 ms 85.4% 
  triton_mm_13 0.3274 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_3 0.3461 ms 72.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_2 0.3613 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5742 seconds and 0.4418 seconds precompiling for 18 choices
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_8 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_4 0.2540 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_1 0.2621 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_16 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_7 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_11 0.2703 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2918 ms 86.3% 
  triton_mm_13 0.3277 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_3 0.3429 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_2 0.3551 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5751 seconds and 0.4018 seconds precompiling for 18 choices
Capturing batches (bs=8 avail_mem=3.47 GB):  79%|███████▊  | 11/14 [02:46<01:28, 29.65s/it]Capturing batches (bs=4 avail_mem=3.45 GB):  79%|███████▊  | 11/14 [02:46<01:28, 29.65s/it][rank1]:E1012 23:42:11.110000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank1]:E1012 23:42:11.110000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1012 23:42:11.110000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_21 0.2529 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_25 0.2529 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_18 0.2632 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_24 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_33 0.2652 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_28 0.2693 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2918 ms 86.7% 
  triton_mm_30 0.3328 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_19 0.3482 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_20 0.3512 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5857 seconds and 0.2916 seconds precompiling for 18 choices
[rank2]:E1012 23:42:11.305000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank2]:E1012 23:42:11.305000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1012 23:42:11.305000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_25 0.2527 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_21 0.2529 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_18 0.2632 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_33 0.2632 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_24 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_28 0.2693 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2929 ms 86.3% 
  triton_mm_30 0.3308 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_19 0.3482 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_20 0.3512 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5862 seconds and 0.2854 seconds precompiling for 18 choices
[rank0]:E1012 23:42:11.613000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank0]:E1012 23:42:11.613000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1012 23:42:11.613000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
[rank3]:E1012 23:42:11.693000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank3]:E1012 23:42:11.693000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1012 23:42:11.693000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_21 0.2528 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_25 0.2529 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_24 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_33 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_18 0.2652 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_28 0.2703 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2949 ms 85.7% 
  triton_mm_30 0.3308 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_20 0.3492 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_19 0.3594 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6389 seconds and 0.3086 seconds precompiling for 18 choices
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_21 0.2509 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_25 0.2529 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_18 0.2621 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_24 0.2652 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_33 0.2652 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_28 0.2714 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2898 ms 86.6% 
  triton_mm_30 0.3348 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_20 0.3492 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_19 0.3602 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5672 seconds and 0.3229 seconds precompiling for 18 choices
Capturing batches (bs=4 avail_mem=3.45 GB):  86%|████████▌ | 12/14 [03:25<01:04, 32.39s/it]Capturing batches (bs=2 avail_mem=3.42 GB):  86%|████████▌ | 12/14 [03:25<01:04, 32.39s/it][rank3]:E1012 23:42:47.237000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank3]:E1012 23:42:47.237000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1012 23:42:47.237000 665470 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_42 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.2580 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_41 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.2693 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_35 0.2888 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2926 ms 86.1% 
  triton_mm_47 0.3308 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_36 0.3574 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_37 0.3635 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.6174 seconds and 0.3085 seconds precompiling for 18 choices
[rank2]:E1012 23:42:47.456000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank2]:E1012 23:42:47.456000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1012 23:42:47.456000 665469 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_42 0.2509 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.2560 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_50 0.2632 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_41 0.2650 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_45 0.2693 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_35 0.2796 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2898 ms 86.6% 
  triton_mm_47 0.3267 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_36 0.3502 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_37 0.3574 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5937 seconds and 0.3751 seconds precompiling for 18 choices
[rank1]:E1012 23:42:47.736000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank1]:E1012 23:42:47.736000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1012 23:42:47.736000 665468 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
[rank0]:E1012 23:42:47.834000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank0]:E1012 23:42:47.834000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1012 23:42:47.834000 665467 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_42 0.2518 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.2579 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_50 0.2632 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_41 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_45 0.2683 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_35 0.2775 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2888 ms 87.2% 
  triton_mm_47 0.3308 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_36 0.3533 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_37 0.3625 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5794 seconds and 0.3677 seconds precompiling for 18 choices
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_42 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.2550 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_41 0.2632 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.2693 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_35 0.2754 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2877 ms 87.5% 
  triton_mm_47 0.3256 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_36 0.3543 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_37 0.3564 ms 70.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.6512 seconds and 0.3332 seconds precompiling for 18 choices
Capturing batches (bs=2 avail_mem=3.42 GB):  93%|█████████▎| 13/14 [04:01<00:33, 33.71s/it]Capturing batches (bs=1 avail_mem=3.41 GB):  93%|█████████▎| 13/14 [04:01<00:33, 33.71s/it]Capturing batches (bs=1 avail_mem=3.41 GB): 100%|██████████| 14/14 [04:40<00:00, 35.28s/it]Capturing batches (bs=1 avail_mem=3.41 GB): 100%|██████████| 14/14 [04:40<00:00, 20.05s/it]
[2025-10-12 23:43:30 TP0] Capture cuda graph end. Time elapsed: 282.18 s. mem usage=0.37 GB. avail mem=3.37 GB.
[2025-10-12 23:43:30 TP0] max_total_num_tokens=38187, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=3.37 GB
[2025-10-12 23:43:30] INFO:     Started server process [665288]
[2025-10-12 23:43:30] INFO:     Waiting for application startup.
[2025-10-12 23:43:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:43:30] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:43:30] INFO:     Application startup complete.
[2025-10-12 23:43:30] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-12 23:43:31] INFO:     127.0.0.1:44640 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-12 23:43:31] Receive: obj=GenerateReqInput(rid='19da17a902e948dfb8c1f1a478699033', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:43:31 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:43:34] Finish: obj=GenerateReqInput(rid='19da17a902e948dfb8c1f1a478699033', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': '19da17a902e948dfb8c1f1a478699033', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 2.862769603729248}}
[2025-10-12 23:43:34] INFO:     127.0.0.1:44646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-12 23:43:34] The server is fired up and ready to roll!
[2025-10-12 23:43:43] INFO:     127.0.0.1:48442 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:43:43] INFO:     127.0.0.1:48454 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:43:43] INFO:     127.0.0.1:48470 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-12 23:43:53] Receive: obj=GenerateReqInput(rid='d3bd03c5330b49b6882388ac069d32cb', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:43:53 TP0] Prefill batch. #new-seq: 1, #new-token: 11, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:43:54 TP0] Decode batch. #running-req: 1, #token: 45, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.65, #queue-req: 0, 
[2025-10-12 23:43:55 TP0] Decode batch. #running-req: 1, #token: 85, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.71, #queue-req: 0, 
[2025-10-12 23:43:56 TP0] Decode batch. #running-req: 1, #token: 125, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.06, #queue-req: 0, 
[2025-10-12 23:43:58] Finish: obj=GenerateReqInput(rid='d3bd03c5330b49b6882388ac069d32cb', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, a branch of physics that deals with very small things like atoms and subatomic particles. Unlike classical computers that use bits to process information, quantum computers use something called quantum bits, or qubits.\n\n### What are Qubits?\n\nA classical bit can be either a 0 or a 1, but a qubit can be in a superposition of both 0 and 1 simultaneously. This means that a qubit can hold much more information than a classical bit. Additionally, qubits can be entangled, which means the state of one qubit can be instantly related to the state of another, no matter the distance between them', 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1032, 10567, 1070, 16941, 1137, 15453, 1163, 1983, 2509, 2490, 1505, 24989, 1072, 1851, 15203, 14706, 29491, 18151, 14146, 19286, 1137, 1706, 10124, 1066, 2527, 2639, 29493, 11683, 19286, 1706, 2313, 2755, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 781, 781, 28100, 2592, 1228, 1954, 1205, 1814, 29572, 781, 781, 29509, 14146, 3054, 1309, 1115, 3245, 1032, 29473, 29502, 1210, 1032, 29473, 29508, 29493, 1330, 1032, 1294, 3349, 1309, 1115, 1065, 1032, 2963, 4408, 1070, 2328, 29473, 29502, 1072, 29473, 29508, 18902, 29491, 1619, 3593, 1137, 1032, 1294, 3349, 1309, 3168, 1956, 1448, 2639, 1589, 1032, 14146, 3054, 29491, 17337, 29493, 1294, 8437, 1309, 1115, 1704, 21854, 29493, 1458, 3593, 1040, 2433, 1070, 1392, 1294, 3349, 1309, 1115, 16971, 5970, 1066, 1040, 2433, 1070, 2466, 29493, 1476, 3977, 1040, 6096, 2212, 1474], 'meta_info': {'id': 'd3bd03c5330b49b6882388ac069d32cb', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 1, 'e2e_latency': 4.77069354057312}}
[2025-10-12 23:43:58] INFO:     127.0.0.1:45418 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:44:11] Receive: obj=GenerateReqInput(rid='6e3c368845e9411a9b25ac9bfebaba80', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:44:11 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 11, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:44:11 TP0] Decode batch. #running-req: 1, #token: 15, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2.65, #queue-req: 0, 
[2025-10-12 23:44:13 TP0] Decode batch. #running-req: 1, #token: 55, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-10-12 23:44:14 TP0] Decode batch. #running-req: 1, #token: 95, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.52, #queue-req: 0, 
[2025-10-12 23:44:15 TP0] Decode batch. #running-req: 1, #token: 135, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.38, #queue-req: 0, 
[2025-10-12 23:44:16] Finish: obj=GenerateReqInput(rid='6e3c368845e9411a9b25ac9bfebaba80', text='Write a detailed explanation of quantum computing in simple terms:', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.7, 'max_new_tokens': 150, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': -1, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'json_schema': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '\n\nQuantum computing is a type of computing that uses the principles of quantum mechanics, a branch of physics that deals with very small things like atoms and subatomic particles. Unlike classical computers that use bits to process information, quantum computers use something called quantum bits, or qubits.\n\n### Understanding Qubits\n\nA classical bit can be either a 0 or a 1. A qubit, on the other hand, can be in a state of 0, 1, or both at the same time, thanks to a property called superposition. This means that a quantum computer can process a vast number of possibilities simultaneously, making it much more powerful for certain types of computations.\n\n### Superposition and', 'output_ids': [22031, 1065, 4356, 4239, 29515, 781, 781, 20469, 1151, 22031, 1117, 1032, 1980, 1070, 22031, 1137, 6866, 1040, 14586, 1070, 11683, 25135, 29493, 1032, 10567, 1070, 16941, 1137, 15453, 1163, 1983, 2509, 2490, 1505, 24989, 1072, 1851, 15203, 14706, 29491, 18151, 14146, 19286, 1137, 1706, 10124, 1066, 2527, 2639, 29493, 11683, 19286, 1706, 2313, 2755, 11683, 10124, 29493, 1210, 1294, 8437, 29491, 781, 781, 28100, 7373, 9277, 1954, 1205, 1814, 781, 781, 29509, 14146, 3054, 1309, 1115, 3245, 1032, 29473, 29502, 1210, 1032, 29473, 29508, 29491, 1098, 1294, 3349, 29493, 1124, 1040, 1567, 1789, 29493, 1309, 1115, 1065, 1032, 2433, 1070, 29473, 29502, 29493, 29473, 29508, 29493, 1210, 2328, 1206, 1040, 2116, 1495, 29493, 8964, 1066, 1032, 4089, 2755, 2963, 4408, 29491, 1619, 3593, 1137, 1032, 11683, 6842, 1309, 2527, 1032, 10323, 2242, 1070, 18821, 18902, 29493, 3260, 1146, 1956, 1448, 7555, 1122, 3320, 5282, 1070, 4401, 1465, 29491, 781, 781, 28100, 6845, 4408, 1072], 'meta_info': {'id': '6e3c368845e9411a9b25ac9bfebaba80', 'finish_reason': {'type': 'length', 'length': 150}, 'prompt_tokens': 12, 'weight_version': 'default', 'completion_tokens': 150, 'cached_tokens': 11, 'e2e_latency': 4.948509454727173}}
[2025-10-12 23:44:16] INFO:     127.0.0.1:40888 - "POST /v1/completions HTTP/1.1" 200 OK
[2025-10-12 23:48:56] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-12 23:48:56] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-12 23:48:56] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-12 23:49:28] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=961418792, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:49:28] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:49:28] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:49:36 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:49:36 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:49:36 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:49:36 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:49:36 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-12 23:49:36 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:49:36 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:49:36 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-12 23:49:36 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-12 23:49:36 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:49:37 TP0] sglang is using nccl==2.27.3
[2025-10-12 23:49:37 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:49:37 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:49:37 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-12 23:49:37 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-12 23:49:37 TP0] Init torch distributed ends. mem usage=0.14 GB
[2025-10-12 23:49:37 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:49:37 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:49:37 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:49:37 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-12 23:49:38 TP0] Load weight begin. avail mem=23.16 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:08,  1.58it/s]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:01<00:08,  1.49it/s]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:02<00:07,  1.48it/s]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:02<00:07,  1.36it/s]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:03<00:06,  1.41it/s]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:04<00:05,  1.44it/s]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:04<00:04,  1.46it/s]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:04<00:03,  1.93it/s]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:05<00:02,  1.79it/s]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:06<00:02,  1.72it/s]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:06<00:01,  1.63it/s]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:07<00:01,  1.44it/s]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:08<00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.31it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.47it/s]

[2025-10-12 23:49:49 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.84 GB, mem usage=15.32 GB.
[2025-10-12 23:49:50 TP0] Using KV cache dtype: torch.float16
[2025-10-12 23:49:50 TP2] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:49:50 TP0] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:49:50 TP0] Memory pool end. avail mem=4.25 GB
[2025-10-12 23:49:50 TP1] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:49:50 TP3] KV Cache is allocated. #tokens: 38187, K size: 1.60 GB, V size: 1.60 GB
[2025-10-12 23:49:50 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.74 GB
[2025-10-12 23:49:50 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.73 GB):   7%|▋         | 1/14 [00:01<00:18,  1.43s/it]Capturing batches (bs=72 avail_mem=3.65 GB):   7%|▋         | 1/14 [00:01<00:18,  1.43s/it]Capturing batches (bs=72 avail_mem=3.65 GB):  14%|█▍        | 2/14 [00:02<00:12,  1.07s/it]Capturing batches (bs=64 avail_mem=3.64 GB):  14%|█▍        | 2/14 [00:02<00:12,  1.07s/it]Capturing batches (bs=64 avail_mem=3.64 GB):  21%|██▏       | 3/14 [00:02<00:08,  1.30it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  21%|██▏       | 3/14 [00:02<00:08,  1.30it/s]Capturing batches (bs=56 avail_mem=3.63 GB):  29%|██▊       | 4/14 [00:03<00:06,  1.61it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  29%|██▊       | 4/14 [00:03<00:06,  1.61it/s]Capturing batches (bs=48 avail_mem=3.60 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.94it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  36%|███▌      | 5/14 [00:03<00:04,  1.94it/s]Capturing batches (bs=40 avail_mem=3.59 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.25it/s]Capturing batches (bs=32 avail_mem=3.56 GB):  43%|████▎     | 6/14 [00:03<00:03,  2.25it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.56 GB):  50%|█████     | 7/14 [00:22<00:44,  6.36s/it]Capturing batches (bs=24 avail_mem=3.54 GB):  50%|█████     | 7/14 [00:22<00:44,  6.36s/it]Capturing batches (bs=24 avail_mem=3.54 GB):  57%|█████▋    | 8/14 [00:42<01:04, 10.78s/it]Capturing batches (bs=16 avail_mem=3.51 GB):  57%|█████▋    | 8/14 [00:42<01:04, 10.78s/it]Capturing batches (bs=16 avail_mem=3.51 GB):  64%|██████▍   | 9/14 [01:03<01:10, 14.06s/it]Capturing batches (bs=12 avail_mem=3.50 GB):  64%|██████▍   | 9/14 [01:03<01:10, 14.06s/it]Capturing batches (bs=12 avail_mem=3.50 GB):  71%|███████▏  | 10/14 [01:56<01:44, 26.02s/it]Capturing batches (bs=8 avail_mem=3.47 GB):  71%|███████▏  | 10/14 [01:56<01:44, 26.02s/it] Capturing batches (bs=8 avail_mem=3.47 GB):  79%|███████▊  | 11/14 [02:19<01:15, 25.02s/it]Capturing batches (bs=4 avail_mem=3.45 GB):  79%|███████▊  | 11/14 [02:19<01:15, 25.02s/it]Capturing batches (bs=4 avail_mem=3.45 GB):  86%|████████▌ | 12/14 [02:44<00:50, 25.17s/it]Capturing batches (bs=2 avail_mem=3.42 GB):  86%|████████▌ | 12/14 [02:44<00:50, 25.17s/it]Capturing batches (bs=2 avail_mem=3.42 GB):  93%|█████████▎| 13/14 [03:11<00:25, 25.55s/it]Capturing batches (bs=1 avail_mem=3.41 GB):  93%|█████████▎| 13/14 [03:11<00:25, 25.55s/it]Capturing batches (bs=1 avail_mem=3.41 GB): 100%|██████████| 14/14 [03:39<00:00, 26.26s/it]Capturing batches (bs=1 avail_mem=3.41 GB): 100%|██████████| 14/14 [03:39<00:00, 15.65s/it]
[2025-10-12 23:53:30 TP0] Capture cuda graph end. Time elapsed: 220.40 s. mem usage=0.37 GB. avail mem=3.37 GB.
[2025-10-12 23:53:31 TP0] max_total_num_tokens=38187, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=3.37 GB
[2025-10-12 23:53:31] INFO:     Started server process [744370]
[2025-10-12 23:53:31] INFO:     Waiting for application startup.
[2025-10-12 23:53:31] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:53:31] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-12 23:53:31] INFO:     Application startup complete.
[2025-10-12 23:53:31] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-12 23:53:32] INFO:     127.0.0.1:33528 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-12 23:53:32] Receive: obj=GenerateReqInput(rid='aa8c5c5cb1b145c5b9ab510a1fe55570', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-12 23:53:32 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-12 23:53:33] Finish: obj=GenerateReqInput(rid='aa8c5c5cb1b145c5b9ab510a1fe55570', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': 'aa8c5c5cb1b145c5b9ab510a1fe55570', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.023322582244873}}
[2025-10-12 23:53:33] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-12 23:53:33] The server is fired up and ready to roll!
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 00:02:49] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=690831065, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:02:49] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:02:49] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:02:57 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:02:57 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:02:57 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:02:57 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 00:02:57 TP0] Init torch distributed begin.
[2025-10-13 00:02:57 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:02:57 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:02:58 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:02:58 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:02:58 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 00:02:58 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 00:02:59 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 00:02:59 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 00:02:59 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 00:02:59 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 00:02:59 TP0] Init torch distributed ends. mem usage=0.57 GB
[2025-10-13 00:02:59 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:02:59 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:02:59 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:02:59 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:03:01 TP0] Load weight begin. avail mem=18.21 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:24<05:15, 24.25s/it]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:44<04:25, 22.09s/it]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [01:04<03:52, 21.15s/it]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [01:23<03:23, 20.31s/it]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [01:42<02:58, 19.86s/it]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [02:03<02:41, 20.13s/it]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [02:22<02:17, 19.69s/it]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [02:25<01:26, 14.37s/it]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [02:46<01:22, 16.50s/it]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [03:06<01:10, 17.58s/it]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [03:26<00:54, 18.16s/it]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [03:45<00:37, 18.64s/it]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [04:04<00:18, 18.77s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [04:23<00:00, 18.79s/it]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [04:23<00:00, 18.83s/it]

[2025-10-13 00:07:26 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.84 GB, mem usage=10.38 GB.
[2025-10-13 00:07:26 TP0] Using KV cache dtype: torch.float16
[2025-10-13 00:07:27 TP3] KV Cache is allocated. #tokens: 49027, K size: 2.06 GB, V size: 2.06 GB
[2025-10-13 00:07:27 TP2] KV Cache is allocated. #tokens: 49027, K size: 2.06 GB, V size: 2.06 GB
[2025-10-13 00:07:27 TP1] KV Cache is allocated. #tokens: 49027, K size: 2.06 GB, V size: 2.06 GB
[2025-10-13 00:07:27 TP0] KV Cache is allocated. #tokens: 49027, K size: 2.06 GB, V size: 2.06 GB
[2025-10-13 00:07:27 TP0] Memory pool end. avail mem=3.55 GB
[2025-10-13 00:07:27 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.06 GB
[2025-10-13 00:07:27 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.05 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.05 GB):   7%|▋         | 1/14 [00:02<00:33,  2.56s/it]Capturing batches (bs=72 avail_mem=2.97 GB):   7%|▋         | 1/14 [00:02<00:33,  2.56s/it]Capturing batches (bs=72 avail_mem=2.97 GB):  14%|█▍        | 2/14 [00:03<00:22,  1.85s/it]Capturing batches (bs=64 avail_mem=2.96 GB):  14%|█▍        | 2/14 [00:03<00:22,  1.85s/it]Capturing batches (bs=64 avail_mem=2.96 GB):  21%|██▏       | 3/14 [00:04<00:13,  1.19s/it]Capturing batches (bs=56 avail_mem=2.95 GB):  21%|██▏       | 3/14 [00:04<00:13,  1.19s/it]Capturing batches (bs=56 avail_mem=2.95 GB):  29%|██▊       | 4/14 [00:04<00:08,  1.16it/s]Capturing batches (bs=48 avail_mem=2.92 GB):  29%|██▊       | 4/14 [00:04<00:08,  1.16it/s]Capturing batches (bs=48 avail_mem=2.92 GB):  36%|███▌      | 5/14 [00:05<00:06,  1.48it/s]Capturing batches (bs=40 avail_mem=2.91 GB):  36%|███▌      | 5/14 [00:05<00:06,  1.48it/s]Capturing batches (bs=40 avail_mem=2.91 GB):  43%|████▎     | 6/14 [00:05<00:04,  1.81it/s]Capturing batches (bs=32 avail_mem=2.87 GB):  43%|████▎     | 6/14 [00:05<00:04,  1.81it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
[rank3]:E1013 00:08:21.219000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank3]:E1013 00:08:21.219000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 00:08:21.219000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
[rank0]:E1013 00:08:21.276000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank0]:E1013 00:08:21.276000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 00:08:21.276000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
AUTOTUNE mm(32x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_8 0.2583 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_1 0.2632 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_7 0.2714 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_16 0.2775 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_11 0.2785 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2826 ms 91.4% 
  triton_mm_4 0.2885 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_3 0.3809 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_2 0.4128 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_13 0.4332 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5935 seconds and 0.6551 seconds precompiling for 18 choices
AUTOTUNE mm(32x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_8 0.2580 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_1 0.2642 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_7 0.2714 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_16 0.2775 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_11 0.2785 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2826 ms 91.3% 
  triton_mm_4 0.2876 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_3 0.3748 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_2 0.4168 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_13 0.4250 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.7427 seconds and 0.5922 seconds precompiling for 18 choices
[rank1]:E1013 00:08:21.540000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank1]:E1013 00:08:21.540000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 00:08:21.540000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
[rank2]:E1013 00:08:21.653000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank2]:E1013 00:08:21.653000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 00:08:21.653000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
AUTOTUNE mm(32x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_8 0.2572 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_1 0.2642 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_7 0.2714 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_16 0.2775 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_11 0.2806 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2856 ms 90.1% 
  triton_mm_4 0.2898 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_3 0.3820 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_2 0.4188 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_13 0.4209 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6047 seconds and 0.5239 seconds precompiling for 18 choices
AUTOTUNE mm(32x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_8 0.2570 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_1 0.2652 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_7 0.2724 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_16 0.2755 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_11 0.2814 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2857 ms 90.0% 
  triton_mm_4 0.2898 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_3 0.3727 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_2 0.4168 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_13 0.4270 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6086 seconds and 0.4498 seconds precompiling for 18 choices
Capturing batches (bs=32 avail_mem=2.87 GB):  50%|█████     | 7/14 [00:55<01:56, 16.68s/it]Capturing batches (bs=24 avail_mem=2.86 GB):  50%|█████     | 7/14 [00:55<01:56, 16.68s/it][rank0]:E1013 00:08:53.708000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank0]:E1013 00:08:53.708000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 00:08:53.708000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
AUTOTUNE mm(24x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_25 0.2580 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_24 0.2683 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_18 0.2693 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_33 0.2734 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.2765 ms 93.3% 
  triton_mm_28 0.2785 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_21 0.2806 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_20 0.3798 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_19 0.4180 ms 61.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_30 0.4198 ms 61.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.7036 seconds and 0.2983 seconds precompiling for 18 choices
[rank3]:E1013 00:08:54.449000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank3]:E1013 00:08:54.449000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 00:08:54.449000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
[rank2]:E1013 00:08:54.535000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank2]:E1013 00:08:54.535000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 00:08:54.535000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
AUTOTUNE mm(24x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_25 0.2580 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_24 0.2693 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_18 0.2703 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_33 0.2744 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.2785 ms 92.6% 
  triton_mm_28 0.2816 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_21 0.2826 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_20 0.3850 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_19 0.4188 ms 61.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_30 0.4260 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5548 seconds and 0.4811 seconds precompiling for 18 choices
[rank1]:E1013 00:08:54.634000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank1]:E1013 00:08:54.634000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 00:08:54.634000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
AUTOTUNE mm(24x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_25 0.2560 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_18 0.2662 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_24 0.2693 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_33 0.2734 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.2816 ms 90.9% 
  triton_mm_28 0.2816 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_21 0.2826 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_20 0.3738 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_19 0.4107 ms 62.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_30 0.4291 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5710 seconds and 0.4879 seconds precompiling for 18 choices
AUTOTUNE mm(24x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_25 0.2580 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_18 0.2703 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_24 0.2703 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_33 0.2724 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_28 0.2796 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2816 ms 91.6% 
  triton_mm_21 0.2816 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_20 0.3799 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_19 0.4137 ms 62.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_30 0.4280 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5652 seconds and 0.3565 seconds precompiling for 18 choices
Capturing batches (bs=24 avail_mem=2.86 GB):  57%|█████▋    | 8/14 [01:28<02:11, 21.88s/it]Capturing batches (bs=16 avail_mem=2.83 GB):  57%|█████▋    | 8/14 [01:28<02:11, 21.88s/it][rank3]:E1013 00:09:27.189000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank3]:E1013 00:09:27.189000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 00:09:27.189000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
[rank2]:E1013 00:09:27.195000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank2]:E1013 00:09:27.195000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 00:09:27.195000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
[rank1]:E1013 00:09:27.211000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank1]:E1013 00:09:27.211000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 00:09:27.211000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
[rank0]:E1013 00:09:27.214000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank0]:E1013 00:09:27.214000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 00:09:27.214000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
AUTOTUNE mm(16x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_42 0.2529 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.2540 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_35 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_41 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.2724 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2990 ms 84.6% 
  triton_mm_47 0.3256 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_37 0.3451 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_36 0.3604 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5703 seconds and 0.5166 seconds precompiling for 18 choices
AUTOTUNE mm(16x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_42 0.2509 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.2519 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_35 0.2621 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_41 0.2640 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.2642 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.2724 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2970 ms 84.5% 
  triton_mm_47 0.3256 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_37 0.3410 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_36 0.3553 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5630 seconds and 0.6158 seconds precompiling for 18 choices
AUTOTUNE mm(16x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_38 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_42 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_35 0.2621 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_41 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.2721 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2980 ms 84.5% 
  triton_mm_47 0.3226 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_37 0.3410 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_36 0.3512 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5586 seconds and 0.3612 seconds precompiling for 18 choices
AUTOTUNE mm(16x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_38 0.2540 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_42 0.2540 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_35 0.2621 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_41 0.2642 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.2652 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_45 0.2714 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2980 ms 85.2% 
  triton_mm_47 0.3215 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_37 0.3368 ms 75.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_36 0.3553 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6484 seconds and 0.6146 seconds precompiling for 18 choices
Capturing batches (bs=16 avail_mem=2.83 GB):  64%|██████▍   | 9/14 [02:01<02:06, 25.31s/it]Capturing batches (bs=12 avail_mem=2.81 GB):  64%|██████▍   | 9/14 [02:01<02:06, 25.31s/it][rank2]:E1013 00:10:02.940000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank2]:E1013 00:10:02.940000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 00:10:02.940000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
AUTOTUNE mm(12x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_55 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_59 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_52 0.2611 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_58 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_62 0.2724 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2949 ms 85.4% 
  triton_mm_64 0.3287 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_54 0.3430 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_53 0.3571 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6275 seconds and 0.2851 seconds precompiling for 18 choices
[rank0]:E1013 00:10:03.518000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank0]:E1013 00:10:03.518000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 00:10:03.518000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
[rank1]:E1013 00:10:03.534000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank1]:E1013 00:10:03.534000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 00:10:03.534000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
[rank3]:E1013 00:10:03.587000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank3]:E1013 00:10:03.587000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 00:10:03.587000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
AUTOTUNE mm(12x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_55 0.2528 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_59 0.2529 ms 99.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_52 0.2621 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_58 0.2652 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.2652 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_62 0.2734 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2959 ms 85.4% 
  triton_mm_64 0.3256 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_54 0.3420 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_53 0.3563 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5604 seconds and 0.4609 seconds precompiling for 18 choices
AUTOTUNE mm(12x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_59 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_55 0.2529 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_52 0.2621 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_58 0.2651 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_62 0.2715 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2970 ms 84.8% 
  triton_mm_64 0.3246 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_54 0.3451 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_53 0.3584 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6146 seconds and 0.4516 seconds precompiling for 18 choices
AUTOTUNE mm(12x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_59 0.2509 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_55 0.2529 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_52 0.2632 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_58 0.2652 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.2662 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_62 0.2734 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2990 ms 83.9% 
  triton_mm_64 0.3267 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_54 0.3523 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_53 0.3635 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5604 seconds and 0.3618 seconds precompiling for 18 choices
Capturing batches (bs=12 avail_mem=2.81 GB):  71%|███████▏  | 10/14 [02:37<01:55, 28.75s/it]Capturing batches (bs=8 avail_mem=2.78 GB):  71%|███████▏  | 10/14 [02:37<01:55, 28.75s/it] [rank3]:E1013 00:10:38.829000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank3]:E1013 00:10:38.829000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 00:10:38.829000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
[rank1]:E1013 00:10:38.951000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank1]:E1013 00:10:38.951000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 00:10:38.951000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_72 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_69 0.2611 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_75 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_79 0.2724 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2929 ms 86.0% 
  triton_mm_81 0.3277 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_71 0.3400 ms 74.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_70 0.3574 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5737 seconds and 0.3668 seconds precompiling for 18 choices
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_76 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_72 0.2550 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_69 0.2621 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_84 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_75 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_79 0.2724 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2939 ms 85.7% 
  triton_mm_81 0.3318 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_71 0.3441 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_70 0.3531 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5763 seconds and 0.3924 seconds precompiling for 18 choices
[rank2]:E1013 00:10:39.105000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank2]:E1013 00:10:39.105000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 00:10:39.105000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_72 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_69 0.2621 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_75 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_79 0.2703 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2929 ms 86.0% 
  triton_mm_81 0.3215 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_71 0.3379 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_70 0.3533 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5667 seconds and 0.3503 seconds precompiling for 18 choices
[rank0]:E1013 00:10:39.327000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank0]:E1013 00:10:39.327000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 00:10:39.327000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
AUTOTUNE mm(8x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_72 0.2529 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.2529 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_69 0.2621 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_75 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_79 0.2705 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2939 ms 86.1% 
  triton_mm_81 0.3308 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_71 0.3399 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_70 0.3533 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6311 seconds and 0.2867 seconds precompiling for 18 choices
Capturing batches (bs=8 avail_mem=2.78 GB):  79%|███████▊  | 11/14 [03:13<01:32, 30.95s/it]Capturing batches (bs=4 avail_mem=2.77 GB):  79%|███████▊  | 11/14 [03:13<01:32, 30.95s/it][rank2]:E1013 00:11:17.092000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank2]:E1013 00:11:17.092000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 00:11:17.092000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_93 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_89 0.2540 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_86 0.2632 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_101 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_92 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_96 0.2703 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2916 ms 86.4% 
  triton_mm_98 0.3227 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_88 0.3420 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_87 0.3471 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6389 seconds and 0.2692 seconds precompiling for 18 choices
[rank3]:E1013 00:11:17.443000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank3]:E1013 00:11:17.443000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 00:11:17.443000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_89 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.2529 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_92 0.2650 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.2652 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_86 0.2673 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_96 0.2732 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2959 ms 85.1% 
  triton_mm_98 0.3297 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_88 0.3453 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_87 0.3584 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5714 seconds and 0.3949 seconds precompiling for 18 choices
[rank0]:E1013 00:11:17.597000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank0]:E1013 00:11:17.597000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 00:11:17.597000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
[rank1]:E1013 00:11:17.601000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank1]:E1013 00:11:17.601000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 00:11:17.601000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_89 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_86 0.2632 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_101 0.2641 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_92 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_96 0.2705 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2908 ms 86.6% 
  triton_mm_98 0.3318 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_88 0.3410 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_87 0.3542 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5581 seconds and 0.3113 seconds precompiling for 18 choices
AUTOTUNE mm(4x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_89 0.2529 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.2540 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_86 0.2632 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_101 0.2640 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_92 0.2642 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_96 0.2703 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.2918 ms 86.7% 
  triton_mm_98 0.3328 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_88 0.3441 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_87 0.3553 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6629 seconds and 0.3708 seconds precompiling for 18 choices
Capturing batches (bs=4 avail_mem=2.77 GB):  86%|████████▌ | 12/14 [03:51<01:06, 33.12s/it]Capturing batches (bs=2 avail_mem=2.73 GB):  86%|████████▌ | 12/14 [03:51<01:06, 33.12s/it][rank2]:E1013 00:11:53.721000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank2]:E1013 00:11:53.721000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 00:11:53.721000 4872 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
[rank1]:E1013 00:11:53.798000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank1]:E1013 00:11:53.798000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 00:11:53.798000 4871 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
[rank0]:E1013 00:11:53.834000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank0]:E1013 00:11:53.834000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 00:11:53.834000 4870 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_106 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_110 0.2519 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_109 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_118 0.2642 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_113 0.2683 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_103 0.2744 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2898 ms 86.9% 
  triton_mm_115 0.3256 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_105 0.3430 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_104 0.3501 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5597 seconds and 0.4617 seconds precompiling for 18 choices
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_110 0.2540 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_106 0.2580 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_118 0.2642 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_109 0.2652 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_113 0.2693 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_103 0.2765 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2918 ms 87.0% 
  triton_mm_115 0.3195 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_104 0.3482 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_105 0.3492 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.5579 seconds and 0.4795 seconds precompiling for 18 choices
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_110 0.2508 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_106 0.2632 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_118 0.2638 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_109 0.2642 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_113 0.2703 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_103 0.2866 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2929 ms 85.7% 
  triton_mm_115 0.3256 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_104 0.3615 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_105 0.3697 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.6331 seconds and 0.3869 seconds precompiling for 18 choices
[rank3]:E1013 00:11:54.759000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank3]:E1013 00:11:54.759000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 00:11:54.759000 4873 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
AUTOTUNE mm(2x12288, 12288x8192)
strides: [12288, 1], [1, 12288]
dtypes: torch.float16, torch.float16
  triton_mm_110 0.2538 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_106 0.2580 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_118 0.2642 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_109 0.2652 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_113 0.2714 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_103 0.2898 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  mm 0.2918 ms 87.0% 
  triton_mm_115 0.3246 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_105 0.3553 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_104 0.3604 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6170 seconds and 0.3047 seconds precompiling for 18 choices
Capturing batches (bs=2 avail_mem=2.73 GB):  93%|█████████▎| 13/14 [04:28<00:34, 34.32s/it]Capturing batches (bs=1 avail_mem=2.72 GB):  93%|█████████▎| 13/14 [04:28<00:34, 34.32s/it]Capturing batches (bs=1 avail_mem=2.72 GB): 100%|██████████| 14/14 [05:07<00:00, 35.74s/it]Capturing batches (bs=1 avail_mem=2.72 GB): 100%|██████████| 14/14 [05:07<00:00, 21.98s/it]
[2025-10-13 00:12:36 TP0] Capture cuda graph end. Time elapsed: 309.13 s. mem usage=0.37 GB. avail mem=2.68 GB.
[2025-10-13 00:12:36 TP0] max_total_num_tokens=49027, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=2.68 GB
[2025-10-13 00:12:36] INFO:     Started server process [1444]
[2025-10-13 00:12:36] INFO:     Waiting for application startup.
[2025-10-13 00:12:36] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 00:12:36] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 00:12:36] INFO:     Application startup complete.
[2025-10-13 00:12:36] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 00:12:37] INFO:     127.0.0.1:60614 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 00:12:37] Receive: obj=GenerateReqInput(rid='4ba717d5f7e146d0a188fd5cc950be2e', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:12:37 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:12:40] Finish: obj=GenerateReqInput(rid='4ba717d5f7e146d0a188fd5cc950be2e', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': '4ba717d5f7e146d0a188fd5cc950be2e', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 2.858128309249878}}
[2025-10-13 00:12:40] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 00:12:40] The server is fired up and ready to roll!
[2025-10-13 00:16:41] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 00:16:41] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 00:16:42] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 00:19:52] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=871181976, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:19:52] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:19:53] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:19:59 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:20:00 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:20:00 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:20:00 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:20:00 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:20:00 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:20:00 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 00:20:00 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 00:20:01 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 00:20:01 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 00:20:01 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 00:20:02 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 00:20:02 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 00:20:02 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 00:20:02 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 00:20:02 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 00:20:02 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:20:02 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:20:02 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:20:02 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 00:20:02 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:10,  1.21it/s]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:01<00:09,  1.28it/s]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:02<00:08,  1.27it/s]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:03<00:07,  1.33it/s]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:03<00:06,  1.36it/s]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:04<00:05,  1.35it/s]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:05<00:05,  1.35it/s]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:05<00:03,  1.74it/s]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:06<00:03,  1.66it/s]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:06<00:02,  1.48it/s]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:07<00:02,  1.42it/s]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:08<00:01,  1.38it/s]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:09<00:00,  1.34it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:10<00:00,  1.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:10<00:00,  1.38it/s]

[2025-10-13 00:20:14 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.79 GB, mem usage=15.32 GB.
[2025-10-13 00:20:14 TP0] Using KV cache dtype: torch.float16
[2025-10-13 00:20:14 TP1] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 00:20:14 TP0] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 00:20:14 TP2] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 00:20:14 TP0] Memory pool end. avail mem=4.20 GB
[2025-10-13 00:20:14 TP3] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 00:20:14 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.69 GB
[2025-10-13 00:20:14 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.68 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.68 GB):   7%|▋         | 1/14 [00:01<00:14,  1.15s/it]Capturing batches (bs=72 avail_mem=3.61 GB):   7%|▋         | 1/14 [00:01<00:14,  1.15s/it]Capturing batches (bs=72 avail_mem=3.61 GB):  14%|█▍        | 2/14 [00:01<00:11,  1.07it/s]Capturing batches (bs=64 avail_mem=3.59 GB):  14%|█▍        | 2/14 [00:01<00:11,  1.07it/s]Capturing batches (bs=64 avail_mem=3.59 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.52it/s]Capturing batches (bs=56 avail_mem=3.58 GB):  21%|██▏       | 3/14 [00:02<00:07,  1.52it/s]Capturing batches (bs=56 avail_mem=3.58 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.92it/s]Capturing batches (bs=48 avail_mem=3.56 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.92it/s]Capturing batches (bs=48 avail_mem=3.56 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.33it/s]Capturing batches (bs=40 avail_mem=3.55 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.33it/s]Capturing batches (bs=40 avail_mem=3.55 GB):  43%|████▎     | 6/14 [00:03<00:02,  2.71it/s]Capturing batches (bs=32 avail_mem=3.51 GB):  43%|████▎     | 6/14 [00:03<00:02,  2.71it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.51 GB):  50%|█████     | 7/14 [00:43<01:35, 13.58s/it]Capturing batches (bs=24 avail_mem=3.50 GB):  50%|█████     | 7/14 [00:43<01:35, 13.58s/it]Capturing batches (bs=24 avail_mem=3.50 GB):  57%|█████▋    | 8/14 [01:04<01:35, 15.94s/it]Capturing batches (bs=16 avail_mem=3.46 GB):  57%|█████▋    | 8/14 [01:04<01:35, 15.94s/it]Capturing batches (bs=16 avail_mem=3.46 GB):  64%|██████▍   | 9/14 [01:31<01:36, 19.27s/it]Capturing batches (bs=12 avail_mem=3.45 GB):  64%|██████▍   | 9/14 [01:31<01:36, 19.27s/it]Capturing batches (bs=12 avail_mem=3.45 GB):  71%|███████▏  | 10/14 [01:56<01:23, 20.96s/it]Capturing batches (bs=8 avail_mem=3.42 GB):  71%|███████▏  | 10/14 [01:56<01:23, 20.96s/it] Capturing batches (bs=8 avail_mem=3.42 GB):  79%|███████▊  | 11/14 [02:23<01:08, 23.00s/it]Capturing batches (bs=4 avail_mem=3.40 GB):  79%|███████▊  | 11/14 [02:23<01:08, 23.00s/it]Capturing batches (bs=4 avail_mem=3.40 GB):  86%|████████▌ | 12/14 [02:54<00:50, 25.21s/it]Capturing batches (bs=2 avail_mem=3.37 GB):  86%|████████▌ | 12/14 [02:54<00:50, 25.21s/it]Capturing batches (bs=2 avail_mem=3.37 GB):  93%|█████████▎| 13/14 [03:24<00:26, 26.87s/it]Capturing batches (bs=1 avail_mem=3.36 GB):  93%|█████████▎| 13/14 [03:24<00:26, 26.87s/it]Capturing batches (bs=1 avail_mem=3.36 GB): 100%|██████████| 14/14 [03:59<00:00, 29.38s/it]Capturing batches (bs=1 avail_mem=3.36 GB): 100%|██████████| 14/14 [03:59<00:00, 17.14s/it]
[2025-10-13 00:24:15 TP0] Capture cuda graph end. Time elapsed: 241.26 s. mem usage=0.37 GB. avail mem=3.32 GB.
[2025-10-13 00:24:15 TP0] max_total_num_tokens=37697, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=3.32 GB
[2025-10-13 00:24:15] INFO:     Started server process [285268]
[2025-10-13 00:24:15] INFO:     Waiting for application startup.
[2025-10-13 00:24:15] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 00:24:15] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 00:24:15] INFO:     Application startup complete.
[2025-10-13 00:24:15] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 00:24:16] INFO:     127.0.0.1:42980 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 00:24:16] Receive: obj=GenerateReqInput(rid='d64d116e4c8448f6829e2e8fdf89f2a4', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:24:16 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:24:18] Finish: obj=GenerateReqInput(rid='d64d116e4c8448f6829e2e8fdf89f2a4', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': 'd64d116e4c8448f6829e2e8fdf89f2a4', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.0377004146575928}}
[2025-10-13 00:24:18] INFO:     127.0.0.1:42986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 00:24:18] The server is fired up and ready to roll!
[2025-10-13 00:30:46] INFO:     127.0.0.1:32948 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:31:12] INFO:     127.0.0.1:55072 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:31:12] INFO:     127.0.0.1:55084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 00:31:12] Receive: obj=GenerateReqInput(rid='43bec96afc334f77be76125ddd4c00fe', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:31:13 TP0] Prefill batch. #new-seq: 1, #new-token: 80, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:31:14 TP0] Decode batch. #running-req: 1, #token: 114, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.10, #queue-req: 0, 
[2025-10-13 00:31:15 TP0] Decode batch. #running-req: 1, #token: 154, token usage: 0.00, cuda graph: True, gen throughput (token/s): 36.71, #queue-req: 0, 
[2025-10-13 00:31:16 TP0] Decode batch. #running-req: 1, #token: 194, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.73, #queue-req: 0, 
[2025-10-13 00:31:17 TP0] Decode batch. #running-req: 1, #token: 234, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.74, #queue-req: 0, 
[2025-10-13 00:31:18 TP0] Decode batch. #running-req: 1, #token: 274, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.71, #queue-req: 0, 
[2025-10-13 00:31:19 TP0] Decode batch. #running-req: 1, #token: 314, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.72, #queue-req: 0, 
[2025-10-13 00:31:20 TP0] Decode batch. #running-req: 1, #token: 354, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.71, #queue-req: 0, 
[2025-10-13 00:31:21 TP0] Decode batch. #running-req: 1, #token: 394, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.70, #queue-req: 0, 
[2025-10-13 00:31:22 TP0] Decode batch. #running-req: 1, #token: 434, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.71, #queue-req: 0, 
[2025-10-13 00:31:24 TP0] Decode batch. #running-req: 1, #token: 474, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.70, #queue-req: 0, 
[2025-10-13 00:31:25 TP0] Decode batch. #running-req: 1, #token: 514, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.70, #queue-req: 0, 
[2025-10-13 00:31:26 TP0] Decode batch. #running-req: 1, #token: 554, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.68, #queue-req: 0, 
[2025-10-13 00:31:27 TP0] Decode batch. #running-req: 1, #token: 594, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.65, #queue-req: 0, 
[2025-10-13 00:31:28 TP0] Decode batch. #running-req: 1, #token: 634, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.33, #queue-req: 0, 
[2025-10-13 00:31:29 TP0] Decode batch. #running-req: 1, #token: 674, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.53, #queue-req: 0, 
[2025-10-13 00:31:30 TP0] Decode batch. #running-req: 1, #token: 714, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.60, #queue-req: 0, 
[2025-10-13 00:31:31 TP0] Decode batch. #running-req: 1, #token: 754, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.57, #queue-req: 0, 
[2025-10-13 00:31:32 TP0] Decode batch. #running-req: 1, #token: 794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.58, #queue-req: 0, 
[2025-10-13 00:31:34 TP0] Decode batch. #running-req: 1, #token: 834, token usage: 0.02, cuda graph: True, gen throughput (token/s): 35.56, #queue-req: 0, 
[2025-10-13 00:31:35 TP0] Decode batch. #running-req: 1, #token: 874, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.10, #queue-req: 0, 
[2025-10-13 00:31:36 TP0] Decode batch. #running-req: 1, #token: 914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.52, #queue-req: 0, 
[2025-10-13 00:31:37 TP0] Decode batch. #running-req: 1, #token: 954, token usage: 0.03, cuda graph: True, gen throughput (token/s): 36.55, #queue-req: 0, 
[2025-10-13 00:31:38 TP0] Decode batch. #running-req: 1, #token: 994, token usage: 0.03, cuda graph: True, gen throughput (token/s): 36.52, #queue-req: 0, 
[2025-10-13 00:31:39 TP0] Decode batch. #running-req: 1, #token: 1034, token usage: 0.03, cuda graph: True, gen throughput (token/s): 36.44, #queue-req: 0, 
[2025-10-13 00:31:40] Finish: obj=GenerateReqInput(rid='43bec96afc334f77be76125ddd4c00fe', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': " Sure, I'd be happy to help with that. Let's dive into a brief overview of a interesting topic.\n\n---\n\n**The Impact of Artificial Intelligence on Modern Industries**\n\nArtificial Intelligence (AI) has emerged as a transformative force across various industries, revolutionizing the way businesses operate and interact with consumers. The integration of AI technologies is not just a trend but a strategic necessity for companies aiming to stay competitive in the fast-evolving market landscape.\n\n**Healthcare**\n\nIn the healthcare sector, AI is playing a crucial role in diagnostics, treatment, and patient care. Machine learning algorithms can analyze vast amounts of medical data to identify patterns and predict disease outcomes with unprecedented accuracy. Robotic surgery systems, powered by AI, can perform complex procedures with finer precision than human hands. Telemedicine platforms, equipped with AI-driven chatbots, provide accessible healthcare services to remote and underserved populations. Additionally, AI is " ... 'on also presents challenges and ethical considerations. Data privacy and security remain top concerns as AI systems rely on vast amounts of sensitive information. Ethical AI development requires addressing biases in data and algorithms to ensure fairness and inclusivity. Job displacement due to automation is a significant issue, necessitating workforce training and reskilling initiatives. Additionally, regulatory frameworks are essential to govern the responsible use of AI and mitigate potential risks.\n\nIn conclusion, AI is undeniably reshaping various industries by enhancing efficiency, accuracy, and customer experiences. As AI technologies continue to evolve, businesses and policymakers must work together to harness their potential while addressing the accompanying challenges and ethical considerations. The future of AI holds immense promise, and its impact on modern industries is just beginning to unfold.\n\n---\n\nThat should be around 1000 tokens. How does that look? Need anything more specific or different?', 'output_ids': [2], 'meta_info': {'id': '43bec96afc334f77be76125ddd4c00fe', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 81, 'weight_version': 'default', 'completion_tokens': 980, 'cached_tokens': 1, 'e2e_latency': 27.322993755340576}}
[2025-10-13 00:31:40] INFO:     127.0.0.1:47042 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:31:40] Receive: obj=GenerateReqInput(rid='19d96e70595f437b8c452add78ff4a7f', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 3937, 29510, 29481, 25966, 1546, 1032, 7585, 23862, 1070, 1032, 6621, 9835, 29491, 781, 781, 8163, 781, 781, 1116, 1782, 28571, 1070, 4719, 15541, 23859, 1124, 14718, 11295, 2808, 1116, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 16956, 1158, 1032, 6284, 1965, 5042, 3441, 4886, 18677, 29493, 11797, 4632, 1040, 1837, 9457, 14626, 1072, 14881, 1163, 14544, 29491, 1183, 15446, 1070, 16875, 15648, 1117, 1227, 1544, 1032, 9924, 1330, 1032, 16823, 25349, 1122, 5567, 7271, 1056, 1066, 3847, 14241, 1065, 1040, 4870, 29501, 1596, 19158, 3436, 14662, 29491, 781, 781, 1116, 19238, 8769, 1116, 781, 781, 1425, 1040, 16008, 10410, 29493, 16875, 1117, 5311, 1032, 13808, 4673, 1065, 1658, 14895, 29493, 6595, 29493, 1072, 8517, 2424, 29491, 14021, 5936, 19307, 1309, 21533, 10323, 15606, 1070, 6482, 1946, 1066, 9819, 12301, 1072, 7550, 8798, 18782, 1163, 27409, 2122, 13481, 14020, 29491, 6544, 8607, 14069, 5686, 29493, 26774, 1254, 16875, 29493, 1309, 2993, 5398, 16019, 1163, 1622, 1031, 16789, 1589, 3698, 3806, 29491, 9975, 2359, 1062, 1241, 15694, 29493, 17810, 1163, 16875, 29501, 28137, 11474, 29494, 2417, 29493, 3852, 15262, 16008, 4113, 1066, 10076, 1072, 24527, 8833, 20207, 29491, 17337, 29493, 16875, 1117, 28934, 1065, 8644, 15811, 29493, 13825, 1845, 1040, 4867, 1070, 1401, 8629, 2071, 1254, 2058, 14390, 25687, 15751, 1072, 7550, 1056, 1040, 2165, 1062, 3194, 1070, 5396, 8644, 12947, 29491, 781, 781, 1116, 12259, 1385, 1116, 781, 781, 1782, 6361, 5547, 1427, 1603, 13202, 3469, 16875, 1066, 12744, 5929, 29501, 15495, 1072, 21965, 13600, 29491, 16875, 29501, 28137, 19307, 1309, 7473, 18532, 22602, 16620, 1065, 2121, 29501, 2304, 1254, 17650, 15382, 12301, 1065, 6361, 1946, 29491, 16707, 1369, 12442, 5686, 1706, 16875, 1066, 7550, 3436, 18637, 1072, 12851, 1235, 3318, 1206, 16167, 3189, 29493, 13001, 4632, 15447, 1072, 16118, 4632, 6491, 29491, 6544, 29477, 29501, 1084, 3993, 1502, 3173, 4095, 2100, 9310, 8246, 3586, 1124, 4003, 6361, 8429, 1072, 3436, 5099, 29493, 3260, 9888, 6179, 1448, 15262, 1066, 1032, 26709, 8706, 29491, 14463, 29493, 16875, 1117, 11797, 4632, 7114, 3140, 1065, 23033, 1163, 19694, 11474, 29494, 2417, 1137, 1309, 5037, 1032, 6103, 3587, 1070, 1065, 11619, 1265, 1072, 16620, 29493, 8269, 29473, 29518, 29549, 29516, 29555, 2528, 1066, 6456, 29491, 781, 781, 1116, 9405, 1382, 1116, 781, 781, 1425, 1040, 12371, 10410, 29493, 16875, 1117, 6284, 1056, 1040, 7114, 3427, 1072, 8770, 8418, 6179, 29491, 19890, 1616, 1263, 29437, 2084, 12371, 1172, 20680, 6405, 1072, 7765, 1421, 20698, 6925, 29493, 17800, 10164, 1072, 17520, 2861, 19626, 29491, 16875, 29501, 29203, 4095, 2605, 17816, 1959, 1148, 9443, 3223, 2861, 19660, 3586, 1124, 4003, 11925, 24729, 1072, 23499, 29493, 8578, 5105, 7114, 17224, 1072, 7060, 29491, 16707, 1369, 21878, 29484, 17575, 17810, 1163, 16875, 1072, 19169, 1831, 5222, 1849, 2513, 17195, 1234, 29493, 20851, 10324, 10330, 3189, 1072, 21965, 13600, 29491, 17337, 29493, 16875, 1117, 2018, 2075, 1066, 12744, 1065, 29501, 6755, 9789, 1827, 8223, 6844, 22948, 1072, 9020, 22761, 10466, 29493, 9823, 6456, 23737, 1072, 5261, 1172, 1263, 11925, 20799, 29491, 781, 781, 1116, 20804, 1120, 1116, 781, 781, 1782, 18176, 5547, 1117, 9917, 1056, 1032, 6632, 6907, 1163, 1040, 24824, 1070, 16875, 15648, 29491, 4513, 6964, 1375, 12767, 29493, 17810, 1163, 10791, 16875, 5686, 29493, 1228, 2395, 3226, 1066, 11797, 1421, 4095, 1072, 1566, 18176, 1254, 17800, 27660, 29493, 9280, 4474, 9243, 1147, 5879, 1064, 29493, 1072, 17520, 11392, 13600, 29491, 16875, 29501, 28137, 2853, 6281, 15694, 7765, 1421, 7871, 8162, 1072, 18443, 6179, 29493, 20851, 5872, 1491, 1072, 3202, 29501, 24330, 10330, 1070, 12050, 29491, 19890, 1616, 1263, 13132, 5686, 1706, 16875, 1066, 9777, 1040, 3296, 1070, 12767, 1072, 15229, 29493, 22421, 5396, 5150, 1927, 1358, 5012, 1472, 1319, 2488, 29491, 12070, 29493, 16875, 1117, 8818, 7929, 19758, 9789, 1827, 19694, 10887, 1056, 1072, 2121, 29501, 2304, 12524, 29493, 3260, 5298, 1448, 14535, 1072, 13995, 29491, 781, 781, 1116, 29517, 6782, 1120, 1116, 781, 781, 1425, 6130, 29493, 16875, 1117, 6284, 1056, 10200, 1072, 5936, 2806, 9544, 29491, 2732, 2227, 1263, 5936, 15694, 1706, 16875, 1066, 4095, 1421, 14933, 3804, 3586, 1124, 4003, 5936, 12846, 1072, 4208, 2858, 6925, 29493, 20851, 1137, 2198, 6484, 22183, 1040, 2528, 1358, 1695, 1066, 10249, 29491, 16875, 29501, 29203, 9079, 5358, 5686, 3852, 23737, 5936, 9789, 29493, 21604, 1056, 7830, 20497, 13894, 29491, 16707, 1369, 5085, 1056, 7808, 2084, 4368, 3885, 5370, 1495, 1072, 4000, 1448, 1124, 10200, 1072, 6484, 16381, 29491, 17337, 29493, 16875, 1117, 2018, 2075, 1066, 21533, 14933, 1946, 1072, 9819, 18637, 1072, 9580, 1122, 14972, 1065, 26531, 3389, 1072, 14933, 10854, 29491, 781, 781, 1116, 1977, 1223, 1748, 1042, 1072, 12250, 1513, 12540, 1465, 1116, 781, 781, 24243, 1040, 7964, 1070, 16875, 1228, 15653, 29493, 1639, 15446, 1603, 16658, 11137, 1072, 27092, 2685, 1465, 29491, 6052, 13685, 1072, 5676, 7112, 2598, 11632, 1158, 16875, 5686, 15955, 1124, 10323, 15606, 1070, 14277, 2639, 29491, 12250, 1513, 16875, 4867, 7716, 25411, 5008, 2786, 1065, 1946, 1072, 19307, 1066, 6175, 5736, 2235, 1072, 1939, 1149, 3342, 29491, 15401, 14188, 7235, 3708, 1066, 5375, 1120, 1117, 1032, 6632, 5059, 29493, 4295, 24019, 1539, 6480, 4922, 1072, 1373, 29497, 9085, 25828, 29491, 17337, 29493, 26019, 11550, 29481, 1228, 8742, 1066, 3179, 1040, 8100, 1706, 1070, 16875, 1072, 3135, 12358, 5396, 15447, 29491, 781, 781, 1425, 13654, 29493, 16875, 1117, 1408, 18682, 2675, 1373, 29484, 14655, 4886, 18677, 1254, 8818, 7929, 13600, 29493, 14020, 29493, 1072, 7114, 9789, 29491, 1904, 16875, 15648, 4456, 1066, 1451, 6071, 29493, 9457, 1072, 8266, 1850, 9542, 2348, 1539, 3321, 1066, 1063, 22932, 1420, 5396, 2080, 25411, 1040, 24529, 11137, 1072, 27092, 2685, 1465, 29491, 1183, 4205, 1070, 16875, 9056, 27259, 9849, 29493, 1072, 1639, 5856, 1124, 5406, 18677, 1117, 1544, 6166, 1066, 10845, 1506, 29491, 781, 781, 8163, 781, 781, 4608, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29491, 2370, 2003, 1137, 1681, 29572, 14446, 3192, 1448, 3716, 1210, 2349, 29572, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:31:40 TP0] Prefill batch. #new-seq: 1, #new-token: 1295, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:31:43 TP0] Decode batch. #running-req: 1, #token: 1309, token usage: 0.03, cuda graph: True, gen throughput (token/s): 10.33, #queue-req: 0, 
[2025-10-13 00:31:44 TP0] Decode batch. #running-req: 1, #token: 1349, token usage: 0.04, cuda graph: True, gen throughput (token/s): 36.32, #queue-req: 0, 
[2025-10-13 00:31:45 TP0] Decode batch. #running-req: 1, #token: 1389, token usage: 0.04, cuda graph: True, gen throughput (token/s): 36.37, #queue-req: 0, 
[2025-10-13 00:31:45] Finish: obj=GenerateReqInput(rid='19d96e70595f437b8c452add78ff4a7f', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 3937, 29510, 29481, 25966, 1546, 1032, 7585, 23862, 1070, 1032, 6621, 9835, 29491, 781, 781, 8163, 781, 781, 1116, 1782, 28571, 1070, 4719, 15541, 23859, 1124, 14718, 11295, 2808, 1116, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 16956, 1158, 1032, 6284, 1965, 5042, 3441, 4886, 18677, 29493, 11797, 4632, 1040, 1837, 9457, 14626, 1072, 14881, 1163, 14544, 29491, 1183, 15446, 1070, 16875, 15648, 1117, 1227, 1544, 1032, 9924, 1330, 1032, 16823, 25349, 1122, 5567, 7271, 1056, 1066, 3847, 14241, 1065, 1040, 4870, 29501, 1596, 19158, 3436, 14662, 29491, 781, 781, 1116, 19238, 8769, 1116, 781, 781, 1425, 1040, 16008, 10410, 29493, 16875, 1117, 5311, 1032, 13808, 4673, 1065, 1658, 14895, 29493, 6595, 29493, 1072, 8517, 2424, 29491, 14021, 5936, 19307, 1309, 21533, 10323, 15606, 1070, 6482, 1946, 1066, 9819, 12301, 1072, 7550, 8798, 18782, 1163, 27409, 2122, 13481, 14020, 29491, 6544, 8607, 14069, 5686, 29493, 26774, 1254, 16875, 29493, 1309, 2993, 5398, 16019, 1163, 1622, 1031, 16789, 1589, 3698, 3806, 29491, 9975, 2359, 1062, 1241, 15694, 29493, 17810, 1163, 16875, 29501, 28137, 11474, 29494, 2417, 29493, 3852, 15262, 16008, 4113, 1066, 10076, 1072, 24527, 8833, 20207, 29491, 17337, 29493, 16875, 1117, 28934, 1065, 8644, 15811, 29493, 13825, 1845, 1040, 4867, 1070, 1401, 8629, 2071, 1254, 2058, 14390, 25687, 15751, 1072, 7550, 1056, 1040, 2165, 1062, 3194, 1070, 5396, 8644, 12947, 29491, 781, 781, 1116, 12259, 1385, 1116, 781, 781, 1782, 6361, 5547, 1427, 1603, 13202, 3469, 16875, 1066, 12744, 5929, 29501, 15495, 1072, 21965, 13600, 29491, 16875, 29501, 28137, 19307, 1309, 7473, 18532, 22602, 16620, 1065, 2121, 29501, 2304, 1254, 17650, 15382, 12301, 1065, 6361, 1946, 29491, 16707, 1369, 12442, 5686, 1706, 16875, 1066, 7550, 3436, 18637, 1072, 12851, 1235, 3318, 1206, 16167, 3189, 29493, 13001, 4632, 15447, 1072, 16118, 4632, 6491, 29491, 6544, 29477, 29501, 1084, 3993, 1502, 3173, 4095, 2100, 9310, 8246, 3586, 1124, 4003, 6361, 8429, 1072, 3436, 5099, 29493, 3260, 9888, 6179, 1448, 15262, 1066, 1032, 26709, 8706, 29491, 14463, 29493, 16875, 1117, 11797, 4632, 7114, 3140, 1065, 23033, 1163, 19694, 11474, 29494, 2417, 1137, 1309, 5037, 1032, 6103, 3587, 1070, 1065, 11619, 1265, 1072, 16620, 29493, 8269, 29473, 29518, 29549, 29516, 29555, 2528, 1066, 6456, 29491, 781, 781, 1116, 9405, 1382, 1116, 781, 781, 1425, 1040, 12371, 10410, 29493, 16875, 1117, 6284, 1056, 1040, 7114, 3427, 1072, 8770, 8418, 6179, 29491, 19890, 1616, 1263, 29437, 2084, 12371, 1172, 20680, 6405, 1072, 7765, 1421, 20698, 6925, 29493, 17800, 10164, 1072, 17520, 2861, 19626, 29491, 16875, 29501, 29203, 4095, 2605, 17816, 1959, 1148, 9443, 3223, 2861, 19660, 3586, 1124, 4003, 11925, 24729, 1072, 23499, 29493, 8578, 5105, 7114, 17224, 1072, 7060, 29491, 16707, 1369, 21878, 29484, 17575, 17810, 1163, 16875, 1072, 19169, 1831, 5222, 1849, 2513, 17195, 1234, 29493, 20851, 10324, 10330, 3189, 1072, 21965, 13600, 29491, 17337, 29493, 16875, 1117, 2018, 2075, 1066, 12744, 1065, 29501, 6755, 9789, 1827, 8223, 6844, 22948, 1072, 9020, 22761, 10466, 29493, 9823, 6456, 23737, 1072, 5261, 1172, 1263, 11925, 20799, 29491, 781, 781, 1116, 20804, 1120, 1116, 781, 781, 1782, 18176, 5547, 1117, 9917, 1056, 1032, 6632, 6907, 1163, 1040, 24824, 1070, 16875, 15648, 29491, 4513, 6964, 1375, 12767, 29493, 17810, 1163, 10791, 16875, 5686, 29493, 1228, 2395, 3226, 1066, 11797, 1421, 4095, 1072, 1566, 18176, 1254, 17800, 27660, 29493, 9280, 4474, 9243, 1147, 5879, 1064, 29493, 1072, 17520, 11392, 13600, 29491, 16875, 29501, 28137, 2853, 6281, 15694, 7765, 1421, 7871, 8162, 1072, 18443, 6179, 29493, 20851, 5872, 1491, 1072, 3202, 29501, 24330, 10330, 1070, 12050, 29491, 19890, 1616, 1263, 13132, 5686, 1706, 16875, 1066, 9777, 1040, 3296, 1070, 12767, 1072, 15229, 29493, 22421, 5396, 5150, 1927, 1358, 5012, 1472, 1319, 2488, 29491, 12070, 29493, 16875, 1117, 8818, 7929, 19758, 9789, 1827, 19694, 10887, 1056, 1072, 2121, 29501, 2304, 12524, 29493, 3260, 5298, 1448, 14535, 1072, 13995, 29491, 781, 781, 1116, 29517, 6782, 1120, 1116, 781, 781, 1425, 6130, 29493, 16875, 1117, 6284, 1056, 10200, 1072, 5936, 2806, 9544, 29491, 2732, 2227, 1263, 5936, 15694, 1706, 16875, 1066, 4095, 1421, 14933, 3804, 3586, 1124, 4003, 5936, 12846, 1072, 4208, 2858, 6925, 29493, 20851, 1137, 2198, 6484, 22183, 1040, 2528, 1358, 1695, 1066, 10249, 29491, 16875, 29501, 29203, 9079, 5358, 5686, 3852, 23737, 5936, 9789, 29493, 21604, 1056, 7830, 20497, 13894, 29491, 16707, 1369, 5085, 1056, 7808, 2084, 4368, 3885, 5370, 1495, 1072, 4000, 1448, 1124, 10200, 1072, 6484, 16381, 29491, 17337, 29493, 16875, 1117, 2018, 2075, 1066, 21533, 14933, 1946, 1072, 9819, 18637, 1072, 9580, 1122, 14972, 1065, 26531, 3389, 1072, 14933, 10854, 29491, 781, 781, 1116, 1977, 1223, 1748, 1042, 1072, 12250, 1513, 12540, 1465, 1116, 781, 781, 24243, 1040, 7964, 1070, 16875, 1228, 15653, 29493, 1639, 15446, 1603, 16658, 11137, 1072, 27092, 2685, 1465, 29491, 6052, 13685, 1072, 5676, 7112, 2598, 11632, 1158, 16875, 5686, 15955, 1124, 10323, 15606, 1070, 14277, 2639, 29491, 12250, 1513, 16875, 4867, 7716, 25411, 5008, 2786, 1065, 1946, 1072, 19307, 1066, 6175, 5736, 2235, 1072, 1939, 1149, 3342, 29491, 15401, 14188, 7235, 3708, 1066, 5375, 1120, 1117, 1032, 6632, 5059, 29493, 4295, 24019, 1539, 6480, 4922, 1072, 1373, 29497, 9085, 25828, 29491, 17337, 29493, 26019, 11550, 29481, 1228, 8742, 1066, 3179, 1040, 8100, 1706, 1070, 16875, 1072, 3135, 12358, 5396, 15447, 29491, 781, 781, 1425, 13654, 29493, 16875, 1117, 1408, 18682, 2675, 1373, 29484, 14655, 4886, 18677, 1254, 8818, 7929, 13600, 29493, 14020, 29493, 1072, 7114, 9789, 29491, 1904, 16875, 15648, 4456, 1066, 1451, 6071, 29493, 9457, 1072, 8266, 1850, 9542, 2348, 1539, 3321, 1066, 1063, 22932, 1420, 5396, 2080, 25411, 1040, 24529, 11137, 1072, 27092, 2685, 1465, 29491, 1183, 4205, 1070, 16875, 9056, 27259, 9849, 29493, 1072, 1639, 5856, 1124, 5406, 18677, 1117, 1544, 6166, 1066, 10845, 1506, 29491, 781, 781, 8163, 781, 781, 4608, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29491, 2370, 2003, 1137, 1681, 29572, 14446, 3192, 1448, 3716, 1210, 2349, 29572, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' {\n  "follow_ups": [\n    "Can you explain more about how AI is used in drug discovery?",\n    "What are some examples of AI-driven algorithms used in finance?",\n    "How do predictive analytics work in the retail industry?",\n    "What are the main ethical considerations in developing AI?",\n    "Can you give some examples of AI applications in autonomous vehicles?"\n  ]\n}', 'output_ids': [13700, 29498, 15973, 29535, 4, 1139, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 7093, 1136, 8050, 1448, 1452, 1678, 16875, 1117, 2075, 1065, 8644, 15811, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1509, 10022, 1070, 16875, 29501, 28137, 19307, 2075, 1065, 16746, 29572, 1316, 781, 3055, 1113, 6428, 1279, 7550, 1263, 29437, 1539, 1065, 1040, 12371, 5547, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1040, 2959, 27092, 2685, 1465, 1065, 11191, 16875, 29572, 1316, 781, 3055, 1113, 7093, 1136, 2879, 1509, 10022, 1070, 16875, 9197, 1065, 25777, 1375, 12767, 1878, 781, 29473, 5477, 781, 29520, 2], 'meta_info': {'id': '19d96e70595f437b8c452add78ff4a7f', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 1296, 'weight_version': 'default', 'completion_tokens': 95, 'cached_tokens': 1, 'e2e_latency': 5.372305393218994}}
[2025-10-13 00:31:45] INFO:     127.0.0.1:47046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 00:53:36] INFO:     127.0.0.1:58510 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:53:36] INFO:     127.0.0.1:58522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 00:53:36] Receive: obj=GenerateReqInput(rid='24a8f9b1f7e04f7baeb6db5442da1f30', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:53:36 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 80, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:53:37 TP0] Decode batch. #running-req: 1, #token: 119, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.03, #queue-req: 0, 
[2025-10-13 00:53:38 TP0] Decode batch. #running-req: 1, #token: 159, token usage: 0.00, cuda graph: True, gen throughput (token/s): 36.33, #queue-req: 0, 
[2025-10-13 00:53:40 TP0] Decode batch. #running-req: 1, #token: 199, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.30, #queue-req: 0, 
[2025-10-13 00:53:41 TP0] Decode batch. #running-req: 1, #token: 239, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.29, #queue-req: 0, 
[2025-10-13 00:53:42 TP0] Decode batch. #running-req: 1, #token: 279, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.28, #queue-req: 0, 
[2025-10-13 00:53:43 TP0] Decode batch. #running-req: 1, #token: 319, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.28, #queue-req: 0, 
[2025-10-13 00:53:44 TP0] Decode batch. #running-req: 1, #token: 359, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.28, #queue-req: 0, 
[2025-10-13 00:53:45 TP0] Decode batch. #running-req: 1, #token: 399, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.27, #queue-req: 0, 
[2025-10-13 00:53:46 TP0] Decode batch. #running-req: 1, #token: 439, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.27, #queue-req: 0, 
[2025-10-13 00:53:47 TP0] Decode batch. #running-req: 1, #token: 479, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.26, #queue-req: 0, 
[2025-10-13 00:53:48 TP0] Decode batch. #running-req: 1, #token: 519, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.25, #queue-req: 0, 
[2025-10-13 00:53:50 TP0] Decode batch. #running-req: 1, #token: 559, token usage: 0.01, cuda graph: True, gen throughput (token/s): 36.26, #queue-req: 0, 
[2025-10-13 00:53:51 TP0] Decode batch. #running-req: 1, #token: 599, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.26, #queue-req: 0, 
[2025-10-13 00:53:52 TP0] Decode batch. #running-req: 1, #token: 639, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.24, #queue-req: 0, 
[2025-10-13 00:53:53 TP0] Decode batch. #running-req: 1, #token: 679, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.25, #queue-req: 0, 
[2025-10-13 00:53:54 TP0] Decode batch. #running-req: 1, #token: 719, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.24, #queue-req: 0, 
[2025-10-13 00:53:55 TP0] Decode batch. #running-req: 1, #token: 759, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.25, #queue-req: 0, 
[2025-10-13 00:53:56 TP0] Decode batch. #running-req: 1, #token: 799, token usage: 0.02, cuda graph: True, gen throughput (token/s): 36.23, #queue-req: 0, 
[2025-10-13 00:53:57] Finish: obj=GenerateReqInput(rid='24a8f9b1f7e04f7baeb6db5442da1f30', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Sure, I\'d be happy to help with that. Here\'s a brief overview of the history of artificial intelligence, which should be around 1000 tokens:\n\nArtificial Intelligence (AI) has come a long way since its inception in the mid-20th century. The term "artificial intelligence" was first coined by John McCarthy in 1956, when he held the first academic conference on the subject at Dartmouth College. The early days of AI were filled with optimism, with pioneers like Marvin Minsky predicting that machines would surpass human intelligence in just a few decades.\n\nHowever, the initial optimism was soon tempered by the reality of the task. The 1970s and 1980s saw a significant slowdown in AI research, a period known as the "AI winter." This was due to a combination of factors, including the limitations of early computers and the difficulty of programming machines to understand natural language.\n\nThe 1990s marked a turning point for AI, with the advent of the internet and the increasing availability of large datasets. This ' ... 'g issues like bias in AI algorithms, which can occur when the data used to train the algorithms is not representative of the population as a whole. It also includes ensuring that AI is used in a way that respects privacy and human rights.\n\nDespite these challenges, the future of AI looks bright. Advances in areas like quantum computing and neuromorphic engineering hold the promise of even more powerful AI systems, while the increasing availability of data and computing power is making AI more accessible than ever before.\n\nIn conclusion, AI has come a long way since its inception in the mid-20th century, and it continues to evolve at a rapid pace. While there are certainly challenges to be addressed, the potential benefits of AI are enormous, and it is likely to play an increasingly important role in our lives in the years to come. As we continue to develop and deploy AI systems, it is essential that we do so in a way that is ethical, responsible, and inclusive, so that the benefits of AI can be shared by all.', 'output_ids': [2], 'meta_info': {'id': '24a8f9b1f7e04f7baeb6db5442da1f30', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 81, 'weight_version': 'default', 'completion_tokens': 753, 'cached_tokens': 80, 'e2e_latency': 21.350709676742554}}
[2025-10-13 00:53:57] INFO:     127.0.0.1:46358 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:53:57] Receive: obj=GenerateReqInput(rid='c1df744143ed42309fadac78a6365936', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:53:57 TP0] Prefill batch. #new-seq: 1, #new-token: 747, #cached-token: 322, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:53:59 TP0] Decode batch. #running-req: 1, #token: 1074, token usage: 0.03, cuda graph: True, gen throughput (token/s): 14.76, #queue-req: 0, 
[2025-10-13 00:54:00 TP0] Decode batch. #running-req: 1, #token: 1114, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.94, #queue-req: 0, 
[2025-10-13 00:54:01 TP0] Decode batch. #running-req: 1, #token: 1154, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.84, #queue-req: 0, 
[2025-10-13 00:54:02] Finish: obj=GenerateReqInput(rid='c1df744143ed42309fadac78a6365936', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' ```json\n{\n  "follow_ups": [\n    "Which areas of AI are currently seeing the most rapid advancements?",\n    "Can you provide examples of recent ethical debates surrounding AI?",\n    "What are some practical applications of explainable AI?",\n    "How are researchers addressing bias in AI algorithms?",\n    "What potential benefits do you see from advances in quantum computing and neuromorphic engineering for AI?"\n  ]\n}\n```', 'output_ids': [13700, 29498, 15973, 29535, 4, 9557, 4139, 781, 29519, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 27471, 5788, 1070, 16875, 1228, 6257, 7020, 1040, 1848, 9189, 9438, 2107, 29572, 1316, 781, 3055, 1113, 7093, 1136, 3852, 10022, 1070, 6159, 27092, 6433, 1770, 12796, 16875, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1509, 11886, 9197, 1070, 8050, 1290, 16875, 29572, 1316, 781, 3055, 1113, 6428, 1228, 16102, 25411, 16885, 1065, 16875, 19307, 29572, 1316, 781, 3055, 1113, 3963, 5396, 7964, 1279, 1136, 1800, 1245, 28273, 1065, 11683, 22031, 1072, 24938, 23399, 14088, 1122, 16875, 1878, 781, 29473, 5477, 781, 29520, 781, 14708, 29600, 2], 'meta_info': {'id': 'c1df744143ed42309fadac78a6365936', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 1069, 'weight_version': 'default', 'completion_tokens': 102, 'cached_tokens': 322, 'e2e_latency': 4.435407876968384}}
[2025-10-13 00:54:02] INFO:     127.0.0.1:46360 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 00:54:33] INFO:     127.0.0.1:40672 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:54:45] INFO:     127.0.0.1:36646 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:54:45] INFO:     127.0.0.1:36660 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 00:54:45] Receive: obj=GenerateReqInput(rid='f61e3874e17d4d8caf05ccf9e88f956e', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:54:45 TP0] Prefill batch. #new-seq: 1, #new-token: 169, #cached-token: 833, token usage: 0.02, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:54:46 TP0] Decode batch. #running-req: 1, #token: 1025, token usage: 0.03, cuda graph: True, gen throughput (token/s): 0.88, #queue-req: 0, 
[2025-10-13 00:54:47 TP0] Decode batch. #running-req: 1, #token: 1065, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.49, #queue-req: 0, 
[2025-10-13 00:54:49 TP0] Decode batch. #running-req: 1, #token: 1105, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.77, #queue-req: 0, 
[2025-10-13 00:54:50 TP0] Decode batch. #running-req: 1, #token: 1145, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.82, #queue-req: 0, 
[2025-10-13 00:54:51 TP0] Decode batch. #running-req: 1, #token: 1185, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.83, #queue-req: 0, 
[2025-10-13 00:54:52 TP0] Decode batch. #running-req: 1, #token: 1225, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.83, #queue-req: 0, 
[2025-10-13 00:54:53 TP0] Decode batch. #running-req: 1, #token: 1265, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.82, #queue-req: 0, 
[2025-10-13 00:54:54 TP0] Decode batch. #running-req: 1, #token: 1305, token usage: 0.03, cuda graph: True, gen throughput (token/s): 35.82, #queue-req: 0, 
[2025-10-13 00:54:55 TP0] Decode batch. #running-req: 1, #token: 1345, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.82, #queue-req: 0, 
[2025-10-13 00:54:56 TP0] Decode batch. #running-req: 1, #token: 1385, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.82, #queue-req: 0, 
[2025-10-13 00:54:58 TP0] Decode batch. #running-req: 1, #token: 1425, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.80, #queue-req: 0, 
[2025-10-13 00:54:59 TP0] Decode batch. #running-req: 1, #token: 1465, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.77, #queue-req: 0, 
[2025-10-13 00:55:00 TP0] Decode batch. #running-req: 1, #token: 1505, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.80, #queue-req: 0, 
[2025-10-13 00:55:01 TP0] Decode batch. #running-req: 1, #token: 1545, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.79, #queue-req: 0, 
[2025-10-13 00:55:02 TP0] Decode batch. #running-req: 1, #token: 1585, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.78, #queue-req: 0, 
[2025-10-13 00:55:03 TP0] Decode batch. #running-req: 1, #token: 1625, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.78, #queue-req: 0, 
[2025-10-13 00:55:04 TP0] Decode batch. #running-req: 1, #token: 1665, token usage: 0.04, cuda graph: True, gen throughput (token/s): 35.77, #queue-req: 0, 
[2025-10-13 00:55:05 TP0] Decode batch. #running-req: 1, #token: 1705, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.75, #queue-req: 0, 
[2025-10-13 00:55:06 TP0] Decode batch. #running-req: 1, #token: 1745, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.77, #queue-req: 0, 
[2025-10-13 00:55:08 TP0] Decode batch. #running-req: 1, #token: 1785, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.78, #queue-req: 0, 
[2025-10-13 00:55:09 TP0] Decode batch. #running-req: 1, #token: 1825, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.76, #queue-req: 0, 
[2025-10-13 00:55:10 TP0] Decode batch. #running-req: 1, #token: 1865, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.76, #queue-req: 0, 
[2025-10-13 00:55:11 TP0] Decode batch. #running-req: 1, #token: 1905, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.78, #queue-req: 0, 
[2025-10-13 00:55:12 TP0] Decode batch. #running-req: 1, #token: 1945, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.76, #queue-req: 0, 
[2025-10-13 00:55:13 TP0] Decode batch. #running-req: 1, #token: 1985, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.77, #queue-req: 0, 
[2025-10-13 00:55:14 TP0] Decode batch. #running-req: 1, #token: 2025, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.71, #queue-req: 0, 
[2025-10-13 00:55:15 TP0] Decode batch. #running-req: 1, #token: 2065, token usage: 0.05, cuda graph: True, gen throughput (token/s): 35.71, #queue-req: 0, 
[2025-10-13 00:55:16] Finish: obj=GenerateReqInput(rid='f61e3874e17d4d8caf05ccf9e88f956e', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': " The inference speed of a deep learning model, such as one using the TabbyAPI/ExLLAv3 backend with the Open-webui frontend, is dependent on several factors. Given your hardware and software configuration, let's break down the potential impacts and performance expectations:\n\n### Hardware Configuration\n\n1. **6x NVIDIA RTX 3090 GPUs**: These are high-end consumer GPUs with 10,496 CUDA cores and 24GB of GDDR6X memory each. They are quite powerful for deep learning tasks.\n2. **AMD EPYC 73F3 CPU**: A 16-core processor with 32 threads, which should provide ample CPU resources for handling data preprocessing and other tasks.\n3. **512GB DDR4-3200 RAM**: This is a substantial amount of system memory, which ensures that the CPU and GPUs have sufficient resources for data loading and processing.\n\n### Software Environment\n\n1. **ESXi 8.0.3uG**: A hypervisor that allows you to run virtual machines. While ESXi provides strong isolation and management features, it can add some overhead compared to running directly on bare met" ... 'ou can expect a noticeable improvement in data transfer speeds between GPUs, reducing inference latency.\n\n- **With Overclocked Memory**: This can improve the speed of memory-intensive tasks, but the gain might be marginal unless the workload is very memory-bound.\n\n- **Adjusting Power Limits**: This will allow you to fine-tune the performance-power trade-off. Expect higher performance at the cost of higher power consumption and better thermal management.\n\n- **Bare Metal**: This is likely to provide the best performance by eliminating VM overhead. Expect lower latency and higher throughput compared to running within ESXi.\n\n### Conclusion\n\nTo maximize inference speed, consider running your workload on bare metal with modded P2P drivers and possibly overclocking the GPU memory. Adjusting power limits will help you find the right balance between performance and power consumption. While ESXi provides strong management features, the overhead it introduces may not be justified if raw performance is your primary goal.', 'output_ids': [2], 'meta_info': {'id': 'f61e3874e17d4d8caf05ccf9e88f956e', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 1002, 'weight_version': 'default', 'completion_tokens': 1071, 'cached_tokens': 833, 'e2e_latency': 30.352921962738037}}
[2025-10-13 00:55:16] INFO:     127.0.0.1:42172 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 00:55:16] Receive: obj=GenerateReqInput(rid='185f27d0525b48f7ba20978395fffe3a', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456] ... [27104, 1072, 5165, 14647, 29515, 781, 781, 28100, 10476, 2474, 20826, 781, 781, 29508, 29491, 1387, 29552, 29512, 1186, 7568, 8176, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 20763, 6263, 9957, 3725, 1228, 2254, 29501, 1184, 12845, 20763, 6263, 1163, 29473, 29508, 29502, 29493, 29549, 29542, 29552, 1102, 29547, 6337, 1769, 1179, 1072, 29473, 29518, 29549, 7671, 1070, 24985, 5000, 29552, 29582, 5501, 2198, 29491, 2074, 1228, 4216, 7555, 1122, 4302, 5936, 10564, 29491, 781, 29518, 29491, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 1098, 29473, 29508, 29552, 29501, 3059, 21614, 1163, 29473, 29538, 29518, 18575, 29493, 1458, 1791, 3852, 1605, 1560, 15633, 6591, 1122, 13620, 1946, 1478, 26956, 1072, 1567, 10564, 29491, 781, 29538, 29491, 1387, 29550, 29508, 29518, 7671, 1152, 5000, 29549, 29501, 29538, 29518, 29502, 29502, 28618, 9957, 1619, 1117, 1032, 15813, 4326, 1070, 2355, 5501, 29493, 1458, 21859, 1137, 1040, 15633, 1072, 20763, 6263, 1274, 10174, 6591, 1122, 1946, 15385, 1072, 10225, 29491, 781, 781, 28100, 7351, 15570, 781, 781, 29508, 29491, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 9957, 1098, 12271, 17297, 1137, 6744, 1136, 1066, 2250, 9020, 12923, 29491, 4791, 28919, 29582, 29478, 6080, 3735, 26018, 1072, 6179, 4958, 29493, 1146, 1309, 1735, 1509, 21447, 7499, 1066, 4590, 5858, 1124, 13802, 8271, 29491, 781, 29518, 29491, 1387, 29547, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 1098, 12079, 1072, 13343, 29501, 4204, 20254, 6791, 29493, 1330, 4590, 1146, 1065, 1032, 19571, 1309, 13865, 1509, 5165, 21447, 29491, 781, 29538, 29491, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 10037, 29538, 25095, 9957, 1619, 25095, 1390, 29152, 1040, 20763, 6263, 1122, 1065, 2871, 10564, 29491, 781, 29549, 29491, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 1183, 2956, 5739, 1122, 14881, 1056, 1163, 1040, 25095, 29491, 781, 781, 28100, 4581, 12748, 1072, 7491, 17550, 29481, 781, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 28238, 5887, 6744, 1122, 2254, 29501, 13317, 1946, 6766, 2212, 20763, 6263, 29493, 1458, 1309, 11885, 5684, 5165, 1507, 2181, 5934, 20763, 6263, 29491, 3761, 29493, 1040, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 2003, 1227, 1274, 5197, 29501, 1030, 28238, 5887, 2528, 29491, 9237, 29493, 1224, 1736, 4359, 2427, 1227, 1115, 9575, 2004, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 4010, 1031, 29501, 1300, 29501, 22931, 1093, 29521, 29518, 29521, 29499, 5501, 3503, 1309, 5684, 1040, 13600, 1070, 1946, 6766, 2212, 20763, 6263, 29491, 10384, 1736, 4570, 14464, 1137, 2528, 2631, 1135, 29518, 29521, 1597, 14725, 8411, 1065, 2871, 4807, 2858, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 1098, 14156, 1065, 4807, 2858, 1507, 1946, 3976, 1066, 1115, 17034, 2212, 20763, 6263, 29491, 1619, 1450, 1115, 7079, 21926, 1065, 5600, 4922, 1210, 1065, 2871, 1576, 9791, 1738, 5934, 20763, 6263, 1228, 2075, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 6779, 9957, 6003, 11255, 1056, 1040, 5501, 1070, 1040, 20763, 6263, 1309, 6015, 1040, 4796, 3385, 3400, 1122, 1946, 1971, 18324, 29493, 1458, 1309, 1115, 21926, 1122, 2254, 29501, 4553, 3385, 10564, 1505, 4302, 5936, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 22599, 2568, 5165, 1122, 5501, 29501, 1269, 4865, 10564, 29493, 1330, 1146, 1603, 12863, 1040, 5391, 1070, 2355, 1908, 3205, 1072, 5105, 2750, 17066, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 2732, 4790, 1056, 2750, 12424, 1309, 2084, 9362, 1040, 19734, 1072, 2750, 9064, 1070, 1040, 20763, 6263, 29491, 22225, 1056, 2750, 12424, 1309, 8411, 8312, 2750, 17066, 1072, 7369, 9110, 1330, 1761, 1603, 8411, 5165, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 10341, 7929, 5165, 1072, 2750, 17066, 29491, 24201, 5105, 2750, 12424, 1309, 6015, 5165, 1330, 1603, 6015, 1040, 1695, 1122, 23333, 1072, 2750, 4692, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8930, 1070, 28919, 29582, 29478, 19571, 9957, 29356, 1040, 1539, 2536, 5858, 1124, 13802, 8271, 9431, 10986, 1040, 9020, 2605, 21447, 29493, 1458, 1309, 3852, 1032, 11742, 1065, 5165, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 19630, 5165, 3708, 1066, 1040, 9431, 3003, 1070, 1040, 12271, 17297, 29510, 29481, 21447, 29491, 1619, 1597, 1972, 1065, 4657, 4807, 2858, 1072, 5105, 1827, 1527, 1122, 1065, 2871, 10564, 29491, 781, 781, 28100, 20627, 29236, 1465, 781, 781, 22200, 1952, 1056, 1040, 3515, 9380, 29493, 2004, 1228, 1509, 3720, 14647, 29515, 781, 781, 29501, 1387, 11604, 5429, 20627, 1093, 22237, 4581, 12748, 29499, 9957, 1183, 2355, 1791, 2993, 1930, 3708, 1066, 1040, 7555, 13986, 29493, 1330, 1146, 1761, 1227, 6662, 23303, 1040, 17353, 1070, 1040, 20763, 6263, 3708, 1066, 5396, 8486, 29389, 9271, 1065, 1946, 6766, 1072, 19571, 21447, 29491, 781, 781, 29501, 1387, 3714, 4581, 4570, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 1763, 1309, 2443, 1032, 6408, 1290, 14972, 1065, 1946, 6766, 28248, 2212, 20763, 6263, 29493, 17800, 1065, 2871, 4807, 2858, 29491, 781, 781, 29501, 1387, 3714, 6003, 1280, 7982, 19921, 9957, 1619, 1309, 5684, 1040, 5743, 1070, 5501, 29501, 1269, 4865, 10564, 29493, 1330, 1040, 9124, 2427, 1115, 26054, 7185, 1040, 1539, 2536, 1117, 1983, 5501, 29501, 8018, 29491, 781, 781, 29501, 1387, 24210, 1056, 9210, 11377, 1814, 9957, 1619, 1390, 2682, 1136, 1066, 5201, 29501, 29475, 2570, 1040, 5165, 29501, 7236, 6994, 29501, 2537, 29491, 29236, 5105, 5165, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 2641, 19734, 6179, 29491, 781, 781, 29501, 1387, 29528, 1260, 24912, 9957, 1619, 1117, 4685, 1066, 3852, 1040, 2257, 5165, 1254, 9431, 12019, 19571, 21447, 29491, 29236, 4657, 4807, 2858, 1072, 5105, 1827, 1527, 7499, 1066, 4590, 3141, 28919, 29582, 29478, 29491, 781, 781, 28100, 2093, 17481, 781, 781, 2319, 16118, 1421, 1065, 2871, 5743, 29493, 2685, 4590, 1342, 1539, 2536, 1124, 13802, 8271, 1163, 1736, 4570, 1135, 29518, 29521, 14464, 1072, 8957, 1522, 11255, 1056, 1040, 29243, 5501, 29491, 2732, 4790, 1056, 2750, 12424, 1390, 2084, 1136, 2068, 1040, 1871, 8641, 2212, 5165, 1072, 2750, 17066, 29491, 4791, 28919, 29582, 29478, 6080, 3735, 6179, 4958, 29493, 1040, 21447, 1146, 4948, 2145, 1761, 1227, 1115, 28134, 1281, 9203, 5165, 1117, 1342, 7026, 6309, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 00:55:16 TP0] Prefill batch. #new-seq: 1, #new-token: 1250, #cached-token: 1063, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[2025-10-13 00:55:19 TP0] Decode batch. #running-req: 1, #token: 2345, token usage: 0.06, cuda graph: True, gen throughput (token/s): 10.34, #queue-req: 0, 
[2025-10-13 00:55:20 TP0] Decode batch. #running-req: 1, #token: 2385, token usage: 0.06, cuda graph: True, gen throughput (token/s): 35.67, #queue-req: 0, 
[2025-10-13 00:55:21] Finish: obj=GenerateReqInput(rid='185f27d0525b48f7ba20978395fffe3a', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456] ... [27104, 1072, 5165, 14647, 29515, 781, 781, 28100, 10476, 2474, 20826, 781, 781, 29508, 29491, 1387, 29552, 29512, 1186, 7568, 8176, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 20763, 6263, 9957, 3725, 1228, 2254, 29501, 1184, 12845, 20763, 6263, 1163, 29473, 29508, 29502, 29493, 29549, 29542, 29552, 1102, 29547, 6337, 1769, 1179, 1072, 29473, 29518, 29549, 7671, 1070, 24985, 5000, 29552, 29582, 5501, 2198, 29491, 2074, 1228, 4216, 7555, 1122, 4302, 5936, 10564, 29491, 781, 29518, 29491, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 1098, 29473, 29508, 29552, 29501, 3059, 21614, 1163, 29473, 29538, 29518, 18575, 29493, 1458, 1791, 3852, 1605, 1560, 15633, 6591, 1122, 13620, 1946, 1478, 26956, 1072, 1567, 10564, 29491, 781, 29538, 29491, 1387, 29550, 29508, 29518, 7671, 1152, 5000, 29549, 29501, 29538, 29518, 29502, 29502, 28618, 9957, 1619, 1117, 1032, 15813, 4326, 1070, 2355, 5501, 29493, 1458, 21859, 1137, 1040, 15633, 1072, 20763, 6263, 1274, 10174, 6591, 1122, 1946, 15385, 1072, 10225, 29491, 781, 781, 28100, 7351, 15570, 781, 781, 29508, 29491, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 9957, 1098, 12271, 17297, 1137, 6744, 1136, 1066, 2250, 9020, 12923, 29491, 4791, 28919, 29582, 29478, 6080, 3735, 26018, 1072, 6179, 4958, 29493, 1146, 1309, 1735, 1509, 21447, 7499, 1066, 4590, 5858, 1124, 13802, 8271, 29491, 781, 29518, 29491, 1387, 29547, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 1098, 12079, 1072, 13343, 29501, 4204, 20254, 6791, 29493, 1330, 4590, 1146, 1065, 1032, 19571, 1309, 13865, 1509, 5165, 21447, 29491, 781, 29538, 29491, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 10037, 29538, 25095, 9957, 1619, 25095, 1390, 29152, 1040, 20763, 6263, 1122, 1065, 2871, 10564, 29491, 781, 29549, 29491, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 1183, 2956, 5739, 1122, 14881, 1056, 1163, 1040, 25095, 29491, 781, 781, 28100, 4581, 12748, 1072, 7491, 17550, 29481, 781, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 28238, 5887, 6744, 1122, 2254, 29501, 13317, 1946, 6766, 2212, 20763, 6263, 29493, 1458, 1309, 11885, 5684, 5165, 1507, 2181, 5934, 20763, 6263, 29491, 3761, 29493, 1040, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 2003, 1227, 1274, 5197, 29501, 1030, 28238, 5887, 2528, 29491, 9237, 29493, 1224, 1736, 4359, 2427, 1227, 1115, 9575, 2004, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 4010, 1031, 29501, 1300, 29501, 22931, 1093, 29521, 29518, 29521, 29499, 5501, 3503, 1309, 5684, 1040, 13600, 1070, 1946, 6766, 2212, 20763, 6263, 29491, 10384, 1736, 4570, 14464, 1137, 2528, 2631, 1135, 29518, 29521, 1597, 14725, 8411, 1065, 2871, 4807, 2858, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 1098, 14156, 1065, 4807, 2858, 1507, 1946, 3976, 1066, 1115, 17034, 2212, 20763, 6263, 29491, 1619, 1450, 1115, 7079, 21926, 1065, 5600, 4922, 1210, 1065, 2871, 1576, 9791, 1738, 5934, 20763, 6263, 1228, 2075, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 6779, 9957, 6003, 11255, 1056, 1040, 5501, 1070, 1040, 20763, 6263, 1309, 6015, 1040, 4796, 3385, 3400, 1122, 1946, 1971, 18324, 29493, 1458, 1309, 1115, 21926, 1122, 2254, 29501, 4553, 3385, 10564, 1505, 4302, 5936, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 22599, 2568, 5165, 1122, 5501, 29501, 1269, 4865, 10564, 29493, 1330, 1146, 1603, 12863, 1040, 5391, 1070, 2355, 1908, 3205, 1072, 5105, 2750, 17066, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 2732, 4790, 1056, 2750, 12424, 1309, 2084, 9362, 1040, 19734, 1072, 2750, 9064, 1070, 1040, 20763, 6263, 29491, 22225, 1056, 2750, 12424, 1309, 8411, 8312, 2750, 17066, 1072, 7369, 9110, 1330, 1761, 1603, 8411, 5165, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 10341, 7929, 5165, 1072, 2750, 17066, 29491, 24201, 5105, 2750, 12424, 1309, 6015, 5165, 1330, 1603, 6015, 1040, 1695, 1122, 23333, 1072, 2750, 4692, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8930, 1070, 28919, 29582, 29478, 19571, 9957, 29356, 1040, 1539, 2536, 5858, 1124, 13802, 8271, 9431, 10986, 1040, 9020, 2605, 21447, 29493, 1458, 1309, 3852, 1032, 11742, 1065, 5165, 29491, 781, 1027, 1155, 1387, 17783, 28571, 9957, 19630, 5165, 3708, 1066, 1040, 9431, 3003, 1070, 1040, 12271, 17297, 29510, 29481, 21447, 29491, 1619, 1597, 1972, 1065, 4657, 4807, 2858, 1072, 5105, 1827, 1527, 1122, 1065, 2871, 10564, 29491, 781, 781, 28100, 20627, 29236, 1465, 781, 781, 22200, 1952, 1056, 1040, 3515, 9380, 29493, 2004, 1228, 1509, 3720, 14647, 29515, 781, 781, 29501, 1387, 11604, 5429, 20627, 1093, 22237, 4581, 12748, 29499, 9957, 1183, 2355, 1791, 2993, 1930, 3708, 1066, 1040, 7555, 13986, 29493, 1330, 1146, 1761, 1227, 6662, 23303, 1040, 17353, 1070, 1040, 20763, 6263, 3708, 1066, 5396, 8486, 29389, 9271, 1065, 1946, 6766, 1072, 19571, 21447, 29491, 781, 781, 29501, 1387, 3714, 4581, 4570, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 1763, 1309, 2443, 1032, 6408, 1290, 14972, 1065, 1946, 6766, 28248, 2212, 20763, 6263, 29493, 17800, 1065, 2871, 4807, 2858, 29491, 781, 781, 29501, 1387, 3714, 6003, 1280, 7982, 19921, 9957, 1619, 1309, 5684, 1040, 5743, 1070, 5501, 29501, 1269, 4865, 10564, 29493, 1330, 1040, 9124, 2427, 1115, 26054, 7185, 1040, 1539, 2536, 1117, 1983, 5501, 29501, 8018, 29491, 781, 781, 29501, 1387, 24210, 1056, 9210, 11377, 1814, 9957, 1619, 1390, 2682, 1136, 1066, 5201, 29501, 29475, 2570, 1040, 5165, 29501, 7236, 6994, 29501, 2537, 29491, 29236, 5105, 5165, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 2641, 19734, 6179, 29491, 781, 781, 29501, 1387, 29528, 1260, 24912, 9957, 1619, 1117, 4685, 1066, 3852, 1040, 2257, 5165, 1254, 9431, 12019, 19571, 21447, 29491, 29236, 4657, 4807, 2858, 1072, 5105, 1827, 1527, 7499, 1066, 4590, 3141, 28919, 29582, 29478, 29491, 781, 781, 28100, 2093, 17481, 781, 781, 2319, 16118, 1421, 1065, 2871, 5743, 29493, 2685, 4590, 1342, 1539, 2536, 1124, 13802, 8271, 1163, 1736, 4570, 1135, 29518, 29521, 14464, 1072, 8957, 1522, 11255, 1056, 1040, 29243, 5501, 29491, 2732, 4790, 1056, 2750, 12424, 1390, 2084, 1136, 2068, 1040, 1871, 8641, 2212, 5165, 1072, 2750, 17066, 29491, 4791, 28919, 29582, 29478, 6080, 3735, 6179, 4958, 29493, 1040, 21447, 1146, 4948, 2145, 1761, 1227, 1115, 28134, 1281, 9203, 5165, 1117, 1342, 7026, 6309, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' ```json\n{\n  "follow_ups": [\n    "What specific steps would you recommend to transition from an ESXi VM to a bare metal setup?",\n    "How can I monitor and manage power consumption and thermal output effectively with this setup?",\n    "Are there any benchmarks or tools you would recommend for measuring the inference speed and latency improvements?"\n  ]\n}\n```', 'output_ids': [13700, 29498, 15973, 29535, 4, 9557, 4139, 781, 29519, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 3963, 3716, 6712, 1450, 1136, 7325, 1066, 9033, 1245, 1164, 28919, 29582, 29478, 19571, 1066, 1032, 13802, 8271, 10288, 29572, 1316, 781, 3055, 1113, 6428, 1309, 1083, 9777, 1072, 9362, 2750, 17066, 1072, 19734, 4593, 12234, 1163, 1224, 10288, 29572, 1316, 781, 3055, 1113, 6753, 1504, 1475, 12752, 17949, 1210, 7808, 1136, 1450, 7325, 1122, 25348, 1040, 1065, 2871, 5743, 1072, 4807, 2858, 20432, 1878, 781, 29473, 5477, 781, 29520, 781, 14708, 29600, 2], 'meta_info': {'id': '185f27d0525b48f7ba20978395fffe3a', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 2313, 'weight_version': 'default', 'completion_tokens': 89, 'cached_tokens': 1063, 'e2e_latency': 5.222644329071045}}
[2025-10-13 00:55:21] INFO:     127.0.0.1:42178 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 01:07:08] INFO:     127.0.0.1:51452 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:07:10] INFO:     127.0.0.1:51454 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:07:14] INFO:     127.0.0.1:51678 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:07:24] INFO:     127.0.0.1:55950 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:07:30] INFO:     127.0.0.1:55952 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:07:35] INFO:     127.0.0.1:55264 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:07:46] INFO:     127.0.0.1:54602 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:07:48] INFO:     127.0.0.1:54610 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:09:00] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:09:00] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:09:02] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 01:09:09] server_args=ServerArgs(model_path='/home/ivan/models/llama3.3-70B-instruct-abliterated-awq', tokenizer_path='/home/ivan/models/llama3.3-70B-instruct-abliterated-awq', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=292912188, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='llama-3.3-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:09:09] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:09:09] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:09:15 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:09:16 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:09:16 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 01:09:16 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:09:17 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:09:17 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:09:17 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:09:17 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:09:17 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:09:18 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:09:18 TP0] sglang is using nccl==2.27.3
[2025-10-13 01:09:18 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:09:18 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:09:18 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:09:18 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:09:18 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 01:09:18 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:09:18 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:09:18 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:09:18 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:09:19 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:03,  2.28it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:02,  2.54it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.67it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:01<00:02,  2.49it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:01<00:01,  3.15it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.98it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:02<00:00,  2.66it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:03<00:00,  2.46it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.55it/s]

[2025-10-13 01:09:23 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 01:09:23 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 01:09:23 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:09:23 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 01:09:23 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:09:23 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:09:23 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:09:24 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 01:09:24 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:19<04:09, 19.23s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:19<04:09, 19.23s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:19<01:38,  8.24s/it]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:19<01:38,  8.24s/it]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:20<00:50,  4.59s/it]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:20<00:50,  4.59s/it]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:20<00:28,  2.86s/it]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:20<00:28,  2.86s/it]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:20<00:17,  1.90s/it]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:20<00:17,  1.90s/it]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:20<00:10,  1.32s/it]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:20<00:10,  1.32s/it]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
[rank2]:E1013 01:10:44.324000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank2]:E1013 01:10:44.324000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 01:10:44.324000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
[rank3]:E1013 01:10:44.341000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank3]:E1013 01:10:44.341000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 01:10:44.341000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
[rank1]:E1013 01:10:44.365000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank1]:E1013 01:10:44.365000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 01:10:44.365000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
[rank0]:E1013 01:10:44.439000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Runtime error during autotuning: 
[rank0]:E1013 01:10:44.439000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 01:10:44.439000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/0] Ignoring this choice.
AUTOTUNE mm(32x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.6380 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_4 0.6420 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_8 0.6492 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_11 0.6492 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_1 0.6502 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_16 0.6932 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.7066 ms 90.3% 
  triton_mm_3 1.1233 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_14 1.1366 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_9 1.1428 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6081 seconds and 0.9038 seconds precompiling for 18 choices
AUTOTUNE mm(32x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.6349 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_11 0.6472 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_4 0.6482 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_1 0.6543 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_16 0.6922 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.7066 ms 89.9% 
  triton_mm_8 0.7711 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_3 1.1172 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_9 1.1366 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_14 1.1366 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.6085 seconds and 1.0075 seconds precompiling for 18 choices
AUTOTUNE mm(32x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.6328 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_1 0.6339 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_4 0.6390 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_8 0.6461 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_11 0.6523 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_16 0.6881 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.7004 ms 90.4% 
  triton_mm_3 1.1110 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_14 1.1295 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_9 1.1366 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6110 seconds and 1.0120 seconds precompiling for 18 choices
AUTOTUNE mm(32x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_7 0.6328 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_1 0.6349 ms 99.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_4 0.6390 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_11 0.6472 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_8 0.6482 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_16 0.6922 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.7044 ms 89.8% 
  triton_mm_3 1.1141 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_14 1.1357 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_9 1.1366 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6065 seconds and 0.8284 seconds precompiling for 18 choices
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [01:21<02:25, 20.72s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [01:21<02:25, 20.72s/it][rank0]:E1013 01:11:25.503000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank0]:E1013 01:11:25.503000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 01:11:25.503000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
[rank3]:E1013 01:11:25.560000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank3]:E1013 01:11:25.560000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 01:11:25.560000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
[rank1]:E1013 01:11:25.576000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank1]:E1013 01:11:25.576000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 01:11:25.576000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
[rank2]:E1013 01:11:25.633000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Runtime error during autotuning: 
[rank2]:E1013 01:11:25.633000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 122880 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 01:11:25.633000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/1] Ignoring this choice.
AUTOTUNE mm(24x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_21 0.6380 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_24 0.6380 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_25 0.6390 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_28 0.6461 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_18 0.6533 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  mm 0.6562 ms 97.2% 
  triton_mm_33 0.6994 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_20 1.1172 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_19 1.1387 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_31 1.1400 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.6050 seconds and 0.6048 seconds precompiling for 18 choices
AUTOTUNE mm(24x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_24 0.6349 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_28 0.6472 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_21 0.6564 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  mm 0.6574 ms 96.6% 
  triton_mm_18 0.6707 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_25 0.6963 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_33 0.7035 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_20 1.1121 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_31 1.1377 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_19 1.1489 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.6059 seconds and 0.6020 seconds precompiling for 18 choices
AUTOTUNE mm(24x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_21 0.6359 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_25 0.6400 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_24 0.6410 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_18 0.6451 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_28 0.6482 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.6554 ms 97.0% 
  triton_mm_33 0.6881 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_20 1.0936 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_31 1.1192 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_19 1.1356 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.6049 seconds and 0.6043 seconds precompiling for 18 choices
AUTOTUNE mm(24x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_24 0.6369 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=8
  triton_mm_21 0.6410 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_25 0.6472 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_28 0.6482 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.6584 ms 96.7% 
  triton_mm_18 0.6707 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=4
  triton_mm_33 0.7025 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_20 1.1162 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_31 1.1356 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=8
  triton_mm_19 1.1500 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.6083 seconds and 0.5439 seconds precompiling for 18 choices
Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [02:02<02:43, 27.24s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [02:02<02:43, 27.24s/it][rank2]:E1013 01:12:05.151000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank2]:E1013 01:12:05.151000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 01:12:05.151000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
[rank1]:E1013 01:12:05.160000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank1]:E1013 01:12:05.160000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 01:12:05.160000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
[rank3]:E1013 01:12:05.261000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank3]:E1013 01:12:05.261000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 01:12:05.261000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
AUTOTUNE mm(16x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_35 0.6103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_42 0.6144 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.6175 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_41 0.6246 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_45 0.6369 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.6513 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6523 ms 93.6% 
  triton_mm_36 1.0916 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_47 1.1284 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_43 1.1387 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6072 seconds and 0.8388 seconds precompiling for 18 choices
AUTOTUNE mm(16x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_35 0.6083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_42 0.6134 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.6205 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_41 0.6236 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_45 0.6339 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.6472 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6543 ms 93.0% 
  triton_mm_36 1.1039 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_47 1.1284 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_43 1.1315 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6093 seconds and 0.8488 seconds precompiling for 18 choices
[rank0]:E1013 01:12:05.306000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Runtime error during autotuning: 
[rank0]:E1013 01:12:05.306000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 01:12:05.306000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/2] Ignoring this choice.
AUTOTUNE mm(16x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_35 0.6103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_42 0.6216 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_41 0.6247 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_38 0.6257 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_45 0.6349 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.6523 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6543 ms 93.3% 
  triton_mm_36 1.0977 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_47 1.1192 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_43 1.1264 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6114 seconds and 0.7840 seconds precompiling for 18 choices
AUTOTUNE mm(16x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_35 0.6103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_42 0.6154 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_38 0.6226 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_41 0.6236 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_45 0.6328 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_50 0.6472 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6561 ms 93.0% 
  triton_mm_36 1.1100 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_47 1.1203 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_43 1.1295 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6080 seconds and 0.7601 seconds precompiling for 18 choices
Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [02:42<02:36, 31.30s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [02:42<02:36, 31.30s/it][rank2]:E1013 01:12:46.125000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank2]:E1013 01:12:46.125000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 01:12:46.125000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
[rank3]:E1013 01:12:46.128000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank3]:E1013 01:12:46.128000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 01:12:46.128000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
AUTOTUNE mm(12x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_52 0.6093 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_59 0.6216 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_55 0.6226 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_58 0.6236 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_62 0.6369 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.6410 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6492 ms 93.8% 
  triton_mm_53 1.1049 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_64 1.1295 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_60 1.1336 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6227 seconds and 0.3389 seconds precompiling for 18 choices
AUTOTUNE mm(12x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_52 0.6124 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_55 0.6216 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_58 0.6244 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_59 0.6257 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_62 0.6380 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.6410 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6502 ms 94.2% 
  triton_mm_53 1.0926 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_54 1.1305 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_64 1.1336 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6141 seconds and 0.3415 seconds precompiling for 18 choices
[rank0]:E1013 01:12:46.649000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank0]:E1013 01:12:46.649000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 01:12:46.649000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
AUTOTUNE mm(12x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_52 0.6093 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_59 0.6175 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_55 0.6216 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_58 0.6236 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_62 0.6369 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.6420 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6502 ms 93.7% 
  triton_mm_53 1.0916 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_54 1.1284 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_64 1.1336 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6225 seconds and 0.2808 seconds precompiling for 18 choices
[rank1]:E1013 01:12:47.335000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Runtime error during autotuning: 
[rank1]:E1013 01:12:47.335000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 01:12:47.335000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/3] Ignoring this choice.
AUTOTUNE mm(12x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_52 0.6072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_59 0.6154 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_55 0.6205 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_58 0.6226 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_62 0.6339 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_67 0.6359 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6543 ms 92.8% 
  triton_mm_53 1.0967 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_64 1.1397 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_60 1.1407 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6170 seconds and 0.2948 seconds precompiling for 18 choices
Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [03:24<02:18, 34.58s/it]Capturing batches (bs=8 avail_mem=3.30 GB):  71%|███████▏  | 10/14 [03:24<02:18, 34.58s/it] [rank1]:E1013 01:13:17.413000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank1]:E1013 01:13:17.413000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 01:13:17.413000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
[rank3]:E1013 01:13:17.475000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank3]:E1013 01:13:17.475000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 01:13:17.475000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
[rank0]:E1013 01:13:17.525000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank0]:E1013 01:13:17.525000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 01:13:17.525000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
AUTOTUNE mm(8x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_69 0.6062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_76 0.6154 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_72 0.6164 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_75 0.6205 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_79 0.6339 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.6420 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6482 ms 93.5% 
  triton_mm_70 1.0732 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_71 1.1366 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_77 1.1407 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6103 seconds and 0.6991 seconds precompiling for 18 choices
AUTOTUNE mm(8x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_69 0.6093 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_75 0.6195 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_72 0.6205 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.6328 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_79 0.6349 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.6461 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6472 ms 94.1% 
  triton_mm_70 1.1049 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_71 1.1233 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_82 1.1489 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6168 seconds and 0.7189 seconds precompiling for 18 choices
AUTOTUNE mm(8x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_69 0.6062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_72 0.6134 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_76 0.6134 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_75 0.6205 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_79 0.6339 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.6431 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6492 ms 93.4% 
  triton_mm_70 1.0824 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_71 1.1295 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_81 1.1479 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6156 seconds and 0.6925 seconds precompiling for 18 choices
[rank2]:E1013 01:13:18.122000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Runtime error during autotuning: 
[rank2]:E1013 01:13:18.122000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 01:13:18.122000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/4] Ignoring this choice.
AUTOTUNE mm(8x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_69 0.6072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_72 0.6205 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_75 0.6226 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_76 0.6246 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_79 0.6400 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_84 0.6431 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6492 ms 93.5% 
  triton_mm_70 1.0660 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_71 1.1295 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_81 1.1295 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6247 seconds and 0.5456 seconds precompiling for 18 choices
Capturing batches (bs=8 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [03:55<01:40, 33.53s/it]Capturing batches (bs=4 avail_mem=3.29 GB):  79%|███████▊  | 11/14 [03:55<01:40, 33.53s/it][rank0]:E1013 01:14:03.239000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank0]:E1013 01:14:03.239000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 01:14:03.239000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_89 0.6111 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.6144 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_86 0.6164 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_92 0.6207 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_96 0.6328 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.6420 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6444 ms 94.8% 
  triton_mm_87 1.0732 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_88 1.1182 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_98 1.1438 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6150 seconds and 0.6289 seconds precompiling for 18 choices
[rank2]:E1013 01:14:03.700000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank2]:E1013 01:14:03.700000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 01:14:03.700000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_89 0.6103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.6175 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_86 0.6205 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_92 0.6216 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_96 0.6349 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.6380 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6433 ms 94.9% 
  triton_mm_87 1.0639 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_88 1.1162 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_98 1.1336 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6513 seconds and 0.5719 seconds precompiling for 18 choices
[rank1]:E1013 01:14:04.458000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank1]:E1013 01:14:04.458000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 01:14:04.458000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
[rank3]:E1013 01:14:04.542000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Runtime error during autotuning: 
[rank3]:E1013 01:14:04.542000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 01:14:04.542000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/5] Ignoring this choice.
AUTOTUNE mm(4x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_89 0.6072 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_93 0.6134 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_86 0.6154 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_92 0.6216 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_96 0.6329 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.6369 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6482 ms 93.7% 
  triton_mm_87 1.0639 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_88 1.1223 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_98 1.1387 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6153 seconds and 0.6780 seconds precompiling for 18 choices
AUTOTUNE mm(4x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_89 0.6144 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_92 0.6195 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_86 0.6236 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_93 0.6248 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_96 0.6349 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_101 0.6390 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6533 ms 94.0% 
  triton_mm_87 1.0875 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_88 1.1223 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_99 1.1510 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=4, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6207 seconds and 0.5589 seconds precompiling for 18 choices
Capturing batches (bs=4 avail_mem=3.29 GB):  86%|████████▌ | 12/14 [04:41<01:14, 37.35s/it]Capturing batches (bs=2 avail_mem=3.26 GB):  86%|████████▌ | 12/14 [04:41<01:14, 37.35s/it][rank2]:E1013 01:14:48.539000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank2]:E1013 01:14:48.539000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank2]:E1013 01:14:48.539000 700588 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
[rank0]:E1013 01:14:48.571000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank0]:E1013 01:14:48.571000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank0]:E1013 01:14:48.571000 700586 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
[rank1]:E1013 01:14:48.607000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank1]:E1013 01:14:48.607000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank1]:E1013 01:14:48.607000 700587 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
[rank3]:E1013 01:14:48.613000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Runtime error during autotuning: 
[rank3]:E1013 01:14:48.613000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 110592 Hardware limit:101376 Reducing block sizes or `num_stages` may help.. 
[rank3]:E1013 01:14:48.613000 700589 sglang-env/lib/python3.12/site-packages/torch/_inductor/select_algorithm.py:2691] [37/6] Ignoring this choice.
AUTOTUNE mm(2x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_109 0.6359 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_113 0.6359 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_110 0.6380 ms 99.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_118 0.6380 ms 99.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_106 0.6400 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  mm 0.6410 ms 99.2% 
  triton_mm_103 0.7619 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 1.0568 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_105 1.1182 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_115 1.1284 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6053 seconds and 0.8514 seconds precompiling for 18 choices
AUTOTUNE mm(2x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_110 0.6083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_106 0.6318 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_113 0.6328 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_109 0.6339 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_118 0.6398 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6441 ms 94.4% 
  triton_mm_103 0.7700 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 1.0680 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_105 1.1223 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_115 1.1459 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6163 seconds and 0.7839 seconds precompiling for 18 choices
AUTOTUNE mm(2x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_110 0.6133 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_113 0.6318 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_109 0.6349 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_106 0.6359 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_118 0.6369 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  mm 0.6420 ms 95.5% 
  triton_mm_103 0.7629 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 1.0557 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_105 1.1182 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_111 1.1387 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6091 seconds and 0.8211 seconds precompiling for 18 choices
AUTOTUNE mm(2x8192, 8192x32064)
strides: [8192, 1], [1, 8192]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_113 0.6328 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  triton_mm_110 0.6339 ms 99.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_118 0.6390 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=8
  triton_mm_106 0.6400 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_109 0.6419 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
  mm 0.6420 ms 98.6% 
  triton_mm_103 0.7946 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=2, num_warps=2
  triton_mm_104 1.0844 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=4
  triton_mm_105 1.1317 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=5, num_warps=2
  triton_mm_115 1.1500 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6137 seconds and 0.8849 seconds precompiling for 18 choices
Capturing batches (bs=2 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [05:26<00:39, 39.49s/it]Capturing batches (bs=1 avail_mem=3.25 GB):  93%|█████████▎| 13/14 [05:26<00:39, 39.49s/it]Capturing batches (bs=1 avail_mem=3.25 GB): 100%|██████████| 14/14 [06:06<00:00, 39.68s/it]Capturing batches (bs=1 avail_mem=3.25 GB): 100%|██████████| 14/14 [06:06<00:00, 26.17s/it]
[2025-10-13 01:15:31 TP0] Capture cuda graph end. Time elapsed: 367.78 s. mem usage=0.42 GB. avail mem=3.22 GB.
[2025-10-13 01:15:32 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.22 GB
[2025-10-13 01:15:32] INFO:     Started server process [700413]
[2025-10-13 01:15:32] INFO:     Waiting for application startup.
[2025-10-13 01:15:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-10-13 01:15:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-10-13 01:15:32] INFO:     Application startup complete.
[2025-10-13 01:15:32] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 01:15:33] INFO:     127.0.0.1:33166 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 01:15:33] Receive: obj=GenerateReqInput(rid='996fdd9b41534418b64f01fbd453ff28', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:15:33 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:15:34] Finish: obj=GenerateReqInput(rid='996fdd9b41534418b64f01fbd453ff28', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' famous for its stunning architecture, art museums', 'output_ids': [6864, 3363, 315, 9822, 374, 11495, 369, 1202, 20441, 18112, 11, 1989, 51677], 'meta_info': {'id': '996fdd9b41534418b64f01fbd453ff28', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.0564045906066895}}
[2025-10-13 01:15:34] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 01:15:34] The server is fired up and ready to roll!
[2025-10-13 01:16:11] INFO:     127.0.0.1:57378 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:21:32] INFO:     127.0.0.1:52112 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:23:58] INFO:     127.0.0.1:54218 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:24:25] INFO:     127.0.0.1:60774 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:25:03] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:25:03] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:25:08] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 01:25:16] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=1006135308, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:25:16] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:25:16] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:25:24 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:25:24 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:25:24 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:25:24 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:25:24 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:25:25 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:25:25 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:25:25 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:25:25 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 01:25:25 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:25:26 TP0] sglang is using nccl==2.27.3
[2025-10-13 01:25:26 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:25:26 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:25:26 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:25:26 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:25:26 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 01:25:26 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:25:26 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:25:26 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:25:26 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:25:27 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:11,  1.16it/s]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:01<00:09,  1.27it/s]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:02<00:08,  1.29it/s]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:03<00:07,  1.33it/s]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:03<00:06,  1.37it/s]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:04<00:05,  1.36it/s]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:05<00:05,  1.35it/s]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:05<00:03,  1.72it/s]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:06<00:03,  1.62it/s]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:06<00:02,  1.52it/s]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:07<00:02,  1.47it/s]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:08<00:01,  1.40it/s]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:09<00:00,  1.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.36it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.40it/s]

[2025-10-13 01:25:38 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.79 GB, mem usage=15.32 GB.
[2025-10-13 01:25:38 TP0] Using KV cache dtype: torch.float16
[2025-10-13 01:25:38 TP3] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 01:25:38 TP2] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 01:25:38 TP1] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 01:25:38 TP0] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 01:25:38 TP0] Memory pool end. avail mem=4.20 GB
[2025-10-13 01:25:39 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.69 GB
[2025-10-13 01:25:39 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.68 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.68 GB):   7%|▋         | 1/14 [00:01<00:14,  1.13s/it]Capturing batches (bs=72 avail_mem=3.61 GB):   7%|▋         | 1/14 [00:01<00:14,  1.13s/it]Capturing batches (bs=72 avail_mem=3.61 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.19it/s]Capturing batches (bs=64 avail_mem=3.59 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.19it/s]Capturing batches (bs=64 avail_mem=3.59 GB):  21%|██▏       | 3/14 [00:02<00:06,  1.65it/s]Capturing batches (bs=56 avail_mem=3.58 GB):  21%|██▏       | 3/14 [00:02<00:06,  1.65it/s]Capturing batches (bs=56 avail_mem=3.58 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.05it/s]Capturing batches (bs=48 avail_mem=3.56 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.05it/s]Capturing batches (bs=48 avail_mem=3.56 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.45it/s]Capturing batches (bs=40 avail_mem=3.55 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.45it/s]Capturing batches (bs=40 avail_mem=3.55 GB):  43%|████▎     | 6/14 [00:02<00:02,  2.81it/s]Capturing batches (bs=32 avail_mem=3.51 GB):  43%|████▎     | 6/14 [00:02<00:02,  2.81it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.51 GB):  50%|█████     | 7/14 [00:30<01:04,  9.28s/it]Capturing batches (bs=24 avail_mem=3.50 GB):  50%|█████     | 7/14 [00:30<01:04,  9.28s/it]Capturing batches (bs=24 avail_mem=3.50 GB):  57%|█████▋    | 8/14 [00:49<01:14, 12.36s/it]Capturing batches (bs=16 avail_mem=3.47 GB):  57%|█████▋    | 8/14 [00:49<01:14, 12.36s/it]Capturing batches (bs=16 avail_mem=3.47 GB):  64%|██████▍   | 9/14 [01:12<01:17, 15.60s/it]Capturing batches (bs=12 avail_mem=3.45 GB):  64%|██████▍   | 9/14 [01:12<01:17, 15.60s/it]Capturing batches (bs=12 avail_mem=3.45 GB):  71%|███████▏  | 10/14 [01:35<01:12, 18.04s/it]Capturing batches (bs=8 avail_mem=3.42 GB):  71%|███████▏  | 10/14 [01:35<01:12, 18.04s/it] Capturing batches (bs=8 avail_mem=3.42 GB):  79%|███████▊  | 11/14 [01:59<00:59, 19.88s/it]Capturing batches (bs=4 avail_mem=3.40 GB):  79%|███████▊  | 11/14 [01:59<00:59, 19.88s/it]Capturing batches (bs=4 avail_mem=3.40 GB):  86%|████████▌ | 12/14 [02:24<00:42, 21.41s/it]Capturing batches (bs=2 avail_mem=3.37 GB):  86%|████████▌ | 12/14 [02:24<00:42, 21.41s/it]Capturing batches (bs=2 avail_mem=3.37 GB):  93%|█████████▎| 13/14 [02:49<00:22, 22.43s/it]Capturing batches (bs=1 avail_mem=3.36 GB):  93%|█████████▎| 13/14 [02:49<00:22, 22.43s/it]Capturing batches (bs=1 avail_mem=3.36 GB): 100%|██████████| 14/14 [03:17<00:00, 23.99s/it]Capturing batches (bs=1 avail_mem=3.36 GB): 100%|██████████| 14/14 [03:17<00:00, 14.08s/it]
[2025-10-13 01:28:57 TP0] Capture cuda graph end. Time elapsed: 198.34 s. mem usage=0.37 GB. avail mem=3.33 GB.
[2025-10-13 01:28:57 TP0] max_total_num_tokens=37697, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=3.33 GB
[2025-10-13 01:28:57] INFO:     Started server process [884961]
[2025-10-13 01:28:57] INFO:     Waiting for application startup.
[2025-10-13 01:28:57] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 01:28:57] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 01:28:57] INFO:     Application startup complete.
[2025-10-13 01:28:57] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 01:28:58] INFO:     127.0.0.1:49426 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 01:28:58] Receive: obj=GenerateReqInput(rid='aa018e24e0b84c9e84b9482bb17eb089', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:28:58 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:28:59] Finish: obj=GenerateReqInput(rid='aa018e24e0b84c9e84b9482bb17eb089', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': 'aa018e24e0b84c9e84b9482bb17eb089', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.09110689163208}}
[2025-10-13 01:28:59] INFO:     127.0.0.1:49428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 01:28:59] The server is fired up and ready to roll!
[2025-10-13 01:29:00] INFO:     127.0.0.1:49436 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:29:00] INFO:     127.0.0.1:49438 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 01:29:00] Receive: obj=GenerateReqInput(rid='edef8223628347d7bc9c99b63b2161ce', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:29:00 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:29:19] INFO:     127.0.0.1:52394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 01:29:19] Receive: obj=GenerateReqInput(rid='71af2a3be2b6464b9ceaabc83417f567', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:29:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1001, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:29:20 TP0] Decode batch. #running-req: 1, #token: 1034, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1.75, #queue-req: 0, 
[2025-10-13 01:29:37] INFO:     127.0.0.1:52024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 01:29:37] Receive: obj=GenerateReqInput(rid='9029e7bffb4941df9ecc36f3d60826fd', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:29:37 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1001, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:29:38 TP0] Decode batch. #running-req: 1, #token: 1014, token usage: 0.03, cuda graph: True, gen throughput (token/s): 2.27, #queue-req: 0, 
[2025-10-13 01:29:39 TP0] Decode batch. #running-req: 1, #token: 1054, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.73, #queue-req: 0, 
[2025-10-13 01:36:21] INFO:     127.0.0.1:59124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 01:36:21] Receive: obj=GenerateReqInput(rid='af152b2b410e49bea998c70cc939bf80', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:36:22 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 1001, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:36:22 TP0] Decode batch. #running-req: 1, #token: 1029, token usage: 0.03, cuda graph: True, gen throughput (token/s): 0.10, #queue-req: 0, 
[2025-10-13 01:36:24 TP0] Decode batch. #running-req: 1, #token: 1069, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.32, #queue-req: 0, 
[2025-10-13 01:36:25 TP0] Decode batch. #running-req: 1, #token: 1109, token usage: 0.03, cuda graph: True, gen throughput (token/s): 33.04, #queue-req: 0, 
[2025-10-13 01:36:26 TP0] Decode batch. #running-req: 1, #token: 1149, token usage: 0.03, cuda graph: True, gen throughput (token/s): 33.18, #queue-req: 0, 
[2025-10-13 01:36:27 TP0] Decode batch. #running-req: 1, #token: 1189, token usage: 0.03, cuda graph: True, gen throughput (token/s): 33.00, #queue-req: 0, 
[2025-10-13 01:36:28 TP0] Decode batch. #running-req: 1, #token: 1229, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.76, #queue-req: 0, 
[2025-10-13 01:36:30 TP0] Decode batch. #running-req: 1, #token: 1269, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.86, #queue-req: 0, 
[2025-10-13 01:36:31 TP0] Decode batch. #running-req: 1, #token: 1309, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.86, #queue-req: 0, 
[2025-10-13 01:36:32 TP0] Decode batch. #running-req: 1, #token: 1349, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.87, #queue-req: 0, 
[2025-10-13 01:36:33 TP0] Decode batch. #running-req: 1, #token: 1389, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.76, #queue-req: 0, 
[2025-10-13 01:36:35 TP0] Decode batch. #running-req: 1, #token: 1429, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.69, #queue-req: 0, 
[2025-10-13 01:36:36 TP0] Decode batch. #running-req: 1, #token: 1469, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.72, #queue-req: 0, 
[2025-10-13 01:36:37 TP0] Decode batch. #running-req: 1, #token: 1509, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.66, #queue-req: 0, 
[2025-10-13 01:36:38 TP0] Decode batch. #running-req: 1, #token: 1549, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.67, #queue-req: 0, 
[2025-10-13 01:36:39 TP0] Decode batch. #running-req: 1, #token: 1589, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.66, #queue-req: 0, 
[2025-10-13 01:36:41 TP0] Decode batch. #running-req: 1, #token: 1629, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.67, #queue-req: 0, 
[2025-10-13 01:36:42 TP0] Decode batch. #running-req: 1, #token: 1669, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.68, #queue-req: 0, 
[2025-10-13 01:36:43 TP0] Decode batch. #running-req: 1, #token: 1709, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.60, #queue-req: 0, 
[2025-10-13 01:36:44 TP0] Decode batch. #running-req: 1, #token: 1749, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.58, #queue-req: 0, 
[2025-10-13 01:36:46 TP0] Decode batch. #running-req: 1, #token: 1789, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.61, #queue-req: 0, 
[2025-10-13 01:36:47 TP0] Decode batch. #running-req: 1, #token: 1829, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.59, #queue-req: 0, 
[2025-10-13 01:36:48 TP0] Decode batch. #running-req: 1, #token: 1869, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.58, #queue-req: 0, 
[2025-10-13 01:36:49 TP0] Decode batch. #running-req: 1, #token: 1909, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.51, #queue-req: 0, 
[2025-10-13 01:36:50 TP0] Decode batch. #running-req: 1, #token: 1949, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.52, #queue-req: 0, 
[2025-10-13 01:36:52 TP0] Decode batch. #running-req: 1, #token: 1989, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.51, #queue-req: 0, 
[2025-10-13 01:36:53 TP0] Decode batch. #running-req: 1, #token: 2029, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.56, #queue-req: 0, 
[2025-10-13 01:36:54 TP0] Decode batch. #running-req: 1, #token: 2069, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.52, #queue-req: 0, 
[2025-10-13 01:36:55 TP0] Decode batch. #running-req: 1, #token: 2109, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.46, #queue-req: 0, 
[2025-10-13 01:36:56] Finish: obj=GenerateReqInput(rid='af152b2b410e49bea998c70cc939bf80', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': " The inference speed of a machine learning model, especially a complex one like those supported by the TabbyAPI/ExLLAMAv3 backend, depends on a variety of factors including hardware configuration, software stack, and specific optimizations applied. Let's break down the components and potential modifications you've listed and their expected impacts:\n\n### Hardware Configuration\n1. **6x NVIDIA RTX 3090 GPUs**:\n   - **Raw Performance**: RTX 3090 is known for its high performance in both gaming and machine learning tasks due to its large number of CUDA cores and Tensor cores.\n   - **Memory**: Each 3090 has 24 GB of GDDR6X memory, which is sufficient for large models but would benefit from efficient memory management.\n\n2. **AMD EPYC 73F3 CPU**:\n   - **High Core Count**: This CPU has 16 cores, making it suitable for handling multiple concurrent inference requests and background tasks.\n   - **High IPC**: Good instructions per clock (IPC) performance for CPU-bound tasks.\n\n3. **512 GB DDR4-3200 RAM**:\n   - Sufficient m" ... 'due to improved GPU communication. You might see a 15-20% improvement in inference speed compared to the base configuration.\n\n3. **With NVLink and Modded Drivers, Bare Metal**:\n   - This configuration should yield the best performance. Eliminating virtualization overhead and optimizing GPU communication can result in a 20-30% improvement in inference speed compared to the base configuration.\n\n4. **Overclock Memory and Increased Power Limits (Bare Metal)**:\n   - Slight additional gains, but the improvement may be modest (5-10%), with the trade-off of increased power consumption and thermal output.\n\n### Summary\nFor the best inference speed, running the setup on bare metal with NVLink and modded DirectGPU P2P drivers would be ideal. Overclocking memory and increasing power limits can provide marginal gains but come with trade-offs. Using ESXi VM provides flexibility at the cost of some performance overhead, but with NVLink and modded drivers, it would still perform significantly better than a base configuration.', 'output_ids': [2], 'meta_info': {'id': 'af152b2b410e49bea998c70cc939bf80', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 1002, 'weight_version': 'default', 'completion_tokens': 1137, 'cached_tokens': 1001, 'e2e_latency': 34.826943159103394}}
[2025-10-13 01:36:56] Receive: obj=GenerateReqInput(rid='ba2676fcd9824ca98aca669fc85170c4', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456] ... [5936, 10564, 3708, 1066, 1639, 3243, 2242, 1070, 1102, 29547, 6337, 1769, 1179, 1072, 25484, 1769, 1179, 29491, 781, 1027, 1155, 1387, 10588, 9957, 7834, 29473, 29538, 29502, 29542, 29502, 1427, 29473, 29518, 29549, 23477, 1070, 24985, 5000, 29552, 29582, 5501, 29493, 1458, 1117, 10174, 1122, 3243, 5762, 1330, 1450, 8717, 1245, 9864, 5501, 6179, 29491, 781, 781, 29518, 29491, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 781, 1027, 1155, 1387, 13590, 12965, 4933, 9957, 1619, 15633, 1427, 29473, 29508, 29552, 1769, 1179, 29493, 3260, 1146, 12401, 1122, 13620, 5934, 1147, 3790, 1065, 2871, 10596, 1072, 6182, 10564, 29491, 781, 1027, 1155, 1387, 13590, 1083, 4967, 9957, 6569, 12150, 1428, 10685, 1093, 3433, 29511, 29499, 5165, 1122, 15633, 29501, 8018, 10564, 29491, 781, 781, 29538, 29491, 1387, 29550, 29508, 29518, 23477, 1152, 5000, 29549, 29501, 29538, 29518, 29502, 29502, 28618, 9957, 781, 1027, 1155, 1086, 24653, 5501, 1122, 4590, 5934, 3243, 5762, 1072, 1567, 10564, 1147, 3790, 1114, 29491, 781, 781, 28100, 7351, 20826, 781, 29508, 29491, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 9957, 781, 1027, 1155, 4791, 28919, 29582, 29478, 6080, 8867, 9020, 2605, 17353, 29493, 1146, 4948, 2145, 1509, 21447, 7499, 1066, 13802, 29501, 3011, 1050, 5165, 29491, 781, 781, 29518, 29491, 1387, 29547, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 781, 1027, 1155, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 1117, 1032, 12079, 1072, 13343, 7383, 13586, 29493, 12401, 1122, 4590, 6367, 5936, 1539, 29284, 29491, 781, 781, 29538, 29491, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 9957, 781, 1027, 1155, 3725, 1228, 24195, 1620, 3595, 1122, 4590, 3243, 4610, 5762, 29493, 7765, 2100, 1122, 5165, 29491, 781, 781, 29549, 29491, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 781, 1027, 1155, 7901, 2694, 1032, 2956, 29501, 18877, 5739, 1122, 16865, 1065, 2871, 10564, 29491, 781, 781, 28100, 11418, 3024, 4581, 12748, 781, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 781, 1027, 1155, 1387, 3714, 28238, 5887, 9957, 7901, 2694, 2254, 29501, 13317, 9288, 2212, 20763, 6263, 29493, 17800, 1040, 8486, 29389, 1374, 1122, 1946, 9645, 2212, 20763, 6263, 29491, 10083, 1689, 1208, 5165, 14972, 1309, 1115, 4117, 29491, 781, 1027, 1155, 1387, 22237, 28238, 5887, 9957, 6563, 4464, 2212, 20763, 6263, 1117, 26410, 29493, 1458, 1309, 5856, 1040, 5165, 1070, 5600, 1065, 2871, 10564, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 781, 1027, 1155, 1387, 3714, 4581, 4570, 1152, 1141, 1508, 9957, 2386, 29484, 3001, 14437, 29501, 1300, 29501, 15488, 1093, 29521, 29518, 29521, 29499, 9288, 2212, 20763, 6263, 29493, 13001, 4632, 4807, 2858, 1072, 14725, 17520, 1065, 2871, 5743, 29491, 781, 1027, 1155, 1387, 22237, 4581, 4570, 1152, 1141, 1508, 9957, 13391, 14464, 1761, 1227, 1115, 7765, 2100, 1122, 1135, 29518, 29521, 9288, 29493, 6142, 1066, 5105, 4807, 2858, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 19921, 9957, 781, 1027, 1155, 6003, 11255, 1056, 1040, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 1065, 1509, 5237, 29493, 1330, 1146, 1603, 12863, 2750, 17066, 1072, 19734, 4593, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 781, 1027, 1155, 25145, 4474, 2750, 12424, 6744, 1040, 20763, 6263, 1066, 2250, 1206, 5105, 10685, 28248, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 1330, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 19734, 4593, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 4287, 1070, 28919, 29582, 29478, 19571, 9957, 781, 1027, 1155, 1387, 29528, 1260, 24912, 9957, 2512, 1089, 10986, 1040, 9020, 2605, 21447, 29493, 6142, 1066, 2641, 5165, 1072, 4657, 4807, 2858, 29491, 1619, 1117, 7079, 21926, 1122, 4807, 2858, 29501, 29481, 17188, 9197, 29491, 781, 1027, 1155, 1387, 3023, 29582, 29478, 19571, 9957, 7901, 2694, 23175, 1072, 13569, 1070, 6179, 1330, 4948, 2145, 1509, 5165, 21447, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 781, 781, 29545, 5835, 1040, 2254, 29501, 1255, 12546, 13986, 1072, 24195, 4698, 8000, 29493, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 28248, 29491, 4771, 29510, 29481, 1032, 9422, 12447, 3586, 1124, 1040, 27238, 29515, 781, 781, 29508, 29491, 1387, 22237, 28238, 5887, 1210, 4581, 4570, 1152, 1141, 1508, 1093, 3023, 29582, 29478, 19571, 29499, 9957, 781, 1027, 1155, 1763, 2427, 1800, 1032, 3392, 1065, 2871, 5743, 1738, 4807, 2858, 1117, 7959, 5105, 3708, 1066, 1040, 9020, 2605, 21447, 1072, 26410, 29243, 29501, 1300, 29501, 26572, 9288, 29491, 781, 781, 29518, 29491, 1387, 3714, 28238, 5887, 1072, 4581, 4570, 1152, 1141, 1508, 1093, 3023, 29582, 29478, 19571, 29499, 9957, 781, 1027, 1155, 10083, 1689, 1208, 14156, 1065, 4807, 2858, 3708, 1066, 12725, 29243, 9288, 29491, 1763, 2427, 1800, 1032, 29473, 29508, 29550, 29501, 29518, 29502, 29591, 14972, 1065, 1065, 2871, 5743, 7499, 1066, 1040, 3392, 8195, 29491, 781, 781, 29538, 29491, 1387, 3714, 28238, 5887, 1072, 4581, 4570, 1152, 1141, 1508, 29493, 1133, 1260, 24912, 9957, 781, 1027, 1155, 1619, 8195, 1791, 8877, 1040, 2257, 5165, 29491, 2512, 1089, 12019, 9020, 2605, 21447, 1072, 7765, 4632, 29243, 9288, 1309, 1972, 1065, 1032, 29473, 29518, 29502, 29501, 29538, 29502, 29591, 14972, 1065, 1065, 2871, 5743, 7499, 1066, 1040, 3392, 8195, 29491, 781, 781, 29549, 29491, 1387, 3443, 11255, 19921, 1072, 25145, 2061, 9210, 11377, 1814, 1093, 29528, 1260, 24912, 29499, 9957, 781, 1027, 1155, 1086, 4414, 5638, 22490, 29493, 1330, 1040, 14972, 1761, 1115, 24444, 1093, 29550, 29501, 29508, 29502, 29591, 1325, 1163, 1040, 6994, 29501, 2537, 1070, 8251, 2750, 17066, 1072, 19734, 4593, 29491, 781, 781, 28100, 24044, 781, 3333, 1040, 2257, 1065, 2871, 5743, 29493, 4590, 1040, 10288, 1124, 13802, 8271, 1163, 28238, 5887, 1072, 1736, 4570, 6823, 26572, 1135, 29518, 29521, 14464, 1450, 1115, 8952, 29491, 6003, 11255, 1056, 5501, 1072, 7256, 2750, 12424, 1309, 3852, 26054, 22490, 1330, 2335, 1163, 6994, 29501, 15335, 29491, 10384, 28919, 29582, 29478, 19571, 6080, 23175, 1206, 1040, 3202, 1070, 1509, 5165, 21447, 29493, 1330, 1163, 28238, 5887, 1072, 1736, 4570, 14464, 29493, 1146, 1450, 2077, 2993, 11885, 2641, 1589, 1032, 3392, 8195, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:36:56 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:36:58 TP0] Prefill batch. #new-seq: 1, #new-token: 330, #cached-token: 0, token usage: 0.05, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:37:02 TP0] Decode batch. #running-req: 1, #token: 2389, token usage: 0.06, cuda graph: True, gen throughput (token/s): 5.95, #queue-req: 0, 
[2025-10-13 01:37:03 TP0] Decode batch. #running-req: 1, #token: 2429, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.46, #queue-req: 0, 
[2025-10-13 01:37:05 TP0] Decode batch. #running-req: 1, #token: 2469, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.50, #queue-req: 0, 
[2025-10-13 01:37:05] Finish: obj=GenerateReqInput(rid='ba2676fcd9824ca98aca669fc85170c4', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456] ... [5936, 10564, 3708, 1066, 1639, 3243, 2242, 1070, 1102, 29547, 6337, 1769, 1179, 1072, 25484, 1769, 1179, 29491, 781, 1027, 1155, 1387, 10588, 9957, 7834, 29473, 29538, 29502, 29542, 29502, 1427, 29473, 29518, 29549, 23477, 1070, 24985, 5000, 29552, 29582, 5501, 29493, 1458, 1117, 10174, 1122, 3243, 5762, 1330, 1450, 8717, 1245, 9864, 5501, 6179, 29491, 781, 781, 29518, 29491, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 781, 1027, 1155, 1387, 13590, 12965, 4933, 9957, 1619, 15633, 1427, 29473, 29508, 29552, 1769, 1179, 29493, 3260, 1146, 12401, 1122, 13620, 5934, 1147, 3790, 1065, 2871, 10596, 1072, 6182, 10564, 29491, 781, 1027, 1155, 1387, 13590, 1083, 4967, 9957, 6569, 12150, 1428, 10685, 1093, 3433, 29511, 29499, 5165, 1122, 15633, 29501, 8018, 10564, 29491, 781, 781, 29538, 29491, 1387, 29550, 29508, 29518, 23477, 1152, 5000, 29549, 29501, 29538, 29518, 29502, 29502, 28618, 9957, 781, 1027, 1155, 1086, 24653, 5501, 1122, 4590, 5934, 3243, 5762, 1072, 1567, 10564, 1147, 3790, 1114, 29491, 781, 781, 28100, 7351, 20826, 781, 29508, 29491, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 9957, 781, 1027, 1155, 4791, 28919, 29582, 29478, 6080, 8867, 9020, 2605, 17353, 29493, 1146, 4948, 2145, 1509, 21447, 7499, 1066, 13802, 29501, 3011, 1050, 5165, 29491, 781, 781, 29518, 29491, 1387, 29547, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 781, 1027, 1155, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 1117, 1032, 12079, 1072, 13343, 7383, 13586, 29493, 12401, 1122, 4590, 6367, 5936, 1539, 29284, 29491, 781, 781, 29538, 29491, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 9957, 781, 1027, 1155, 3725, 1228, 24195, 1620, 3595, 1122, 4590, 3243, 4610, 5762, 29493, 7765, 2100, 1122, 5165, 29491, 781, 781, 29549, 29491, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 781, 1027, 1155, 7901, 2694, 1032, 2956, 29501, 18877, 5739, 1122, 16865, 1065, 2871, 10564, 29491, 781, 781, 28100, 11418, 3024, 4581, 12748, 781, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 781, 1027, 1155, 1387, 3714, 28238, 5887, 9957, 7901, 2694, 2254, 29501, 13317, 9288, 2212, 20763, 6263, 29493, 17800, 1040, 8486, 29389, 1374, 1122, 1946, 9645, 2212, 20763, 6263, 29491, 10083, 1689, 1208, 5165, 14972, 1309, 1115, 4117, 29491, 781, 1027, 1155, 1387, 22237, 28238, 5887, 9957, 6563, 4464, 2212, 20763, 6263, 1117, 26410, 29493, 1458, 1309, 5856, 1040, 5165, 1070, 5600, 1065, 2871, 10564, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 781, 1027, 1155, 1387, 3714, 4581, 4570, 1152, 1141, 1508, 9957, 2386, 29484, 3001, 14437, 29501, 1300, 29501, 15488, 1093, 29521, 29518, 29521, 29499, 9288, 2212, 20763, 6263, 29493, 13001, 4632, 4807, 2858, 1072, 14725, 17520, 1065, 2871, 5743, 29491, 781, 1027, 1155, 1387, 22237, 4581, 4570, 1152, 1141, 1508, 9957, 13391, 14464, 1761, 1227, 1115, 7765, 2100, 1122, 1135, 29518, 29521, 9288, 29493, 6142, 1066, 5105, 4807, 2858, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 19921, 9957, 781, 1027, 1155, 6003, 11255, 1056, 1040, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 1065, 1509, 5237, 29493, 1330, 1146, 1603, 12863, 2750, 17066, 1072, 19734, 4593, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 781, 1027, 1155, 25145, 4474, 2750, 12424, 6744, 1040, 20763, 6263, 1066, 2250, 1206, 5105, 10685, 28248, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 1330, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 19734, 4593, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 4287, 1070, 28919, 29582, 29478, 19571, 9957, 781, 1027, 1155, 1387, 29528, 1260, 24912, 9957, 2512, 1089, 10986, 1040, 9020, 2605, 21447, 29493, 6142, 1066, 2641, 5165, 1072, 4657, 4807, 2858, 29491, 1619, 1117, 7079, 21926, 1122, 4807, 2858, 29501, 29481, 17188, 9197, 29491, 781, 1027, 1155, 1387, 3023, 29582, 29478, 19571, 9957, 7901, 2694, 23175, 1072, 13569, 1070, 6179, 1330, 4948, 2145, 1509, 5165, 21447, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 781, 781, 29545, 5835, 1040, 2254, 29501, 1255, 12546, 13986, 1072, 24195, 4698, 8000, 29493, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 28248, 29491, 4771, 29510, 29481, 1032, 9422, 12447, 3586, 1124, 1040, 27238, 29515, 781, 781, 29508, 29491, 1387, 22237, 28238, 5887, 1210, 4581, 4570, 1152, 1141, 1508, 1093, 3023, 29582, 29478, 19571, 29499, 9957, 781, 1027, 1155, 1763, 2427, 1800, 1032, 3392, 1065, 2871, 5743, 1738, 4807, 2858, 1117, 7959, 5105, 3708, 1066, 1040, 9020, 2605, 21447, 1072, 26410, 29243, 29501, 1300, 29501, 26572, 9288, 29491, 781, 781, 29518, 29491, 1387, 3714, 28238, 5887, 1072, 4581, 4570, 1152, 1141, 1508, 1093, 3023, 29582, 29478, 19571, 29499, 9957, 781, 1027, 1155, 10083, 1689, 1208, 14156, 1065, 4807, 2858, 3708, 1066, 12725, 29243, 9288, 29491, 1763, 2427, 1800, 1032, 29473, 29508, 29550, 29501, 29518, 29502, 29591, 14972, 1065, 1065, 2871, 5743, 7499, 1066, 1040, 3392, 8195, 29491, 781, 781, 29538, 29491, 1387, 3714, 28238, 5887, 1072, 4581, 4570, 1152, 1141, 1508, 29493, 1133, 1260, 24912, 9957, 781, 1027, 1155, 1619, 8195, 1791, 8877, 1040, 2257, 5165, 29491, 2512, 1089, 12019, 9020, 2605, 21447, 1072, 7765, 4632, 29243, 9288, 1309, 1972, 1065, 1032, 29473, 29518, 29502, 29501, 29538, 29502, 29591, 14972, 1065, 1065, 2871, 5743, 7499, 1066, 1040, 3392, 8195, 29491, 781, 781, 29549, 29491, 1387, 3443, 11255, 19921, 1072, 25145, 2061, 9210, 11377, 1814, 1093, 29528, 1260, 24912, 29499, 9957, 781, 1027, 1155, 1086, 4414, 5638, 22490, 29493, 1330, 1040, 14972, 1761, 1115, 24444, 1093, 29550, 29501, 29508, 29502, 29591, 1325, 1163, 1040, 6994, 29501, 2537, 1070, 8251, 2750, 17066, 1072, 19734, 4593, 29491, 781, 781, 28100, 24044, 781, 3333, 1040, 2257, 1065, 2871, 5743, 29493, 4590, 1040, 10288, 1124, 13802, 8271, 1163, 28238, 5887, 1072, 1736, 4570, 6823, 26572, 1135, 29518, 29521, 14464, 1450, 1115, 8952, 29491, 6003, 11255, 1056, 5501, 1072, 7256, 2750, 12424, 1309, 3852, 26054, 22490, 1330, 2335, 1163, 6994, 29501, 15335, 29491, 10384, 28919, 29582, 29478, 19571, 6080, 23175, 1206, 1040, 3202, 1070, 1509, 5165, 21447, 29493, 1330, 1163, 28238, 5887, 1072, 1736, 4570, 14464, 29493, 1146, 1450, 2077, 2993, 11885, 2641, 1589, 1032, 3392, 8195, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' ```json\n{\n  "follow_ups": [\n    "How do I actually implement NVLink and the modded DirectGPU P2P drivers on my system?",\n    "What are the risks and benefits of overclocking memory and increasing power limits?",\n    "Can you provide a step-by-step guide to switching from an ESXi VM to a bare-metal setup?"\n  ]\n}\n```', 'output_ids': [13700, 29498, 15973, 29535, 4, 9557, 4139, 781, 29519, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 6428, 1279, 1083, 3358, 5576, 28238, 5887, 1072, 1040, 1736, 4570, 6823, 26572, 1135, 29518, 29521, 14464, 1124, 1354, 2355, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1040, 15447, 1072, 7964, 1070, 1522, 11255, 1056, 5501, 1072, 7256, 2750, 12424, 29572, 1316, 781, 3055, 1113, 7093, 1136, 3852, 1032, 4475, 29501, 2171, 29501, 8459, 9095, 1066, 26207, 1245, 1164, 28919, 29582, 29478, 19571, 1066, 1032, 13802, 29501, 3011, 1050, 10288, 1878, 781, 29473, 5477, 781, 29520, 781, 14708, 29600, 2], 'meta_info': {'id': 'ba2676fcd9824ca98aca669fc85170c4', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 2379, 'weight_version': 'default', 'completion_tokens': 95, 'cached_tokens': 1, 'e2e_latency': 8.3237943649292}}
[2025-10-13 01:37:05] INFO:     127.0.0.1:58194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 01:37:08] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:37:08] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:37:13] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 01:37:21] server_args=ServerArgs(model_path='/home/ivan/models/llama3.3-70B-instruct-abliterated-awq', tokenizer_path='/home/ivan/models/llama3.3-70B-instruct-abliterated-awq', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=812122144, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='llama-3.3-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:37:21] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:37:21] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:37:28 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:37:28 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:37:29 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:37:29 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:37:29 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 01:37:29 TP0] Init torch distributed begin.
[2025-10-13 01:37:29 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:37:29 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:37:29 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:37:29 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:37:30 TP0] sglang is using nccl==2.27.3
[2025-10-13 01:37:30 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:37:30 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:37:30 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:37:30 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:37:30 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 01:37:30 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:37:30 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:37:30 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:37:30 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:37:31 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.70it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:03,  2.12it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.04it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:02,  1.82it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:02<00:01,  2.45it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.11it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:03<00:01,  1.99it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  1.88it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.77it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.92it/s]

[2025-10-13 01:37:36 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 01:37:36 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 01:37:37 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:37:37 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:37:37 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:37:37 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 01:37:37 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:37:37 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 01:37:37 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:14,  1.09s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:14,  1.09s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.46it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.46it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:01<00:05,  2.08it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:01<00:05,  2.08it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:01<00:03,  2.64it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:01<00:03,  2.64it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:02,  3.19it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:02,  3.19it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.70it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.70it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:54<01:59, 17.11s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:54<01:59, 17.11s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:25<02:09, 21.66s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:25<02:09, 21.66s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [02:09<02:23, 28.60s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [02:09<02:23, 28.60s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [02:37<01:53, 28.41s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [02:37<01:53, 28.41s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [03:05<01:25, 28.43s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [03:05<01:25, 28.43s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [03:37<00:58, 29.32s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [03:37<00:58, 29.32s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [04:08<00:29, 29.83s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [04:08<00:29, 29.83s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [04:42<00:00, 31.06s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [04:42<00:00, 20.15s/it]
[2025-10-13 01:42:20 TP0] Capture cuda graph end. Time elapsed: 283.57 s. mem usage=0.41 GB. avail mem=3.23 GB.
[2025-10-13 01:42:21 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.23 GB
[2025-10-13 01:42:21] INFO:     Started server process [1095364]
[2025-10-13 01:42:21] INFO:     Waiting for application startup.
[2025-10-13 01:42:22] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-10-13 01:42:22] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-10-13 01:42:22] INFO:     Application startup complete.
[2025-10-13 01:42:22] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 01:42:23] INFO:     127.0.0.1:49294 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 01:42:23] Receive: obj=GenerateReqInput(rid='bb4f3074351547af9c03e29074c84590', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:42:23 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:42:24] Finish: obj=GenerateReqInput(rid='bb4f3074351547af9c03e29074c84590', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' famous for its stunning architecture, art museums', 'output_ids': [6864, 3363, 315, 9822, 374, 11495, 369, 1202, 20441, 18112, 11, 1989, 51677], 'meta_info': {'id': 'bb4f3074351547af9c03e29074c84590', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.6134719848632812}}
[2025-10-13 01:42:24] INFO:     127.0.0.1:49300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 01:42:24] The server is fired up and ready to roll!
[2025-10-13 01:54:12] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:54:12] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 01:54:13] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 01:54:20] server_args=ServerArgs(model_path='/home/ivan/models/llama3.3-70B-instruct-abliterated-awq', tokenizer_path='/home/ivan/models/llama3.3-70B-instruct-abliterated-awq', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=32419468, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='llama-3.3-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:54:20] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:54:21] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:54:28 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:54:28 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:54:28 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:54:29 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:54:29 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 01:54:29 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:54:29 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:54:30 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 01:54:30 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 01:54:30 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:54:31 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:54:31 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:54:31 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:54:31 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 01:54:31 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 01:54:31 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 01:54:31 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:54:31 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:54:31 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:54:31 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 01:54:32 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.89it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:03,  2.32it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.42it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:01<00:02,  2.23it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:01<00:01,  2.88it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.78it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:02<00:00,  2.55it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:03<00:00,  2.26it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.09it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.30it/s]

[2025-10-13 01:54:36 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 01:54:36 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 01:54:36 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:54:36 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 01:54:36 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:54:37 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:54:37 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 01:54:37 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 01:54:37 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:14,  1.15s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:14,  1.15s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.43it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.43it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:01<00:05,  2.05it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:01<00:05,  2.05it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:01<00:03,  2.60it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:01<00:03,  2.60it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:02,  3.17it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:02,  3.17it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.68it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.68it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:53<01:57, 16.75s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:53<01:57, 16.75s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:10<01:42, 17.11s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:10<01:42, 17.11s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:29<01:27, 17.50s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:29<01:27, 17.50s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [01:48<01:11, 17.93s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [01:48<01:11, 17.93s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [02:09<00:56, 18.92s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [02:09<00:56, 18.92s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [02:31<00:39, 19.95s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [02:31<00:39, 19.95s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [02:55<00:21, 21.08s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [02:55<00:21, 21.08s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:19<00:00, 21.98s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:19<00:00, 14.24s/it]
[2025-10-13 01:57:57 TP0] Capture cuda graph end. Time elapsed: 200.64 s. mem usage=0.41 GB. avail mem=3.23 GB.
[2025-10-13 01:57:58 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.23 GB
[2025-10-13 01:57:58] INFO:     Started server process [1303626]
[2025-10-13 01:57:58] INFO:     Waiting for application startup.
[2025-10-13 01:57:58] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-10-13 01:57:58] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-10-13 01:57:58] INFO:     Application startup complete.
[2025-10-13 01:57:58] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 01:57:59] INFO:     127.0.0.1:41048 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 01:57:59] Receive: obj=GenerateReqInput(rid='27cd320610d34342b4b18bf0bc663921', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:57:59 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:58:00] INFO:     127.0.0.1:41070 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 01:58:00] INFO:     127.0.0.1:41080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 01:58:00] Receive: obj=GenerateReqInput(rid='4b22464fe3d949fc8ddcde1e1804300c', text=None, input_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 1502, 9805, 25, 128009, 128006, 882, 128007, 271, 19182, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 30, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 30, 128009, 128006, 882, 128007, 271, 40, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 30, 128009, 128006, 78191, 128007, 271, 40914, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 13, 128009, 128006, 882, 128007, 271, 3923, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 30, 128009, 128006, 78191, 128007, 271], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.6, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 0.9, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:58:00 TP0] Prefill batch. #new-seq: 1, #new-token: 938, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-13 01:58:02] Finish: obj=GenerateReqInput(rid='27cd320610d34342b4b18bf0bc663921', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' famous for its stunning architecture, art museums', 'output_ids': [6864, 3363, 315, 9822, 374, 11495, 369, 1202, 20441, 18112, 11, 1989, 51677], 'meta_info': {'id': '27cd320610d34342b4b18bf0bc663921', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 2.495633363723755}}
[2025-10-13 01:58:02] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 01:58:02] The server is fired up and ready to roll!
[2025-10-13 01:58:02 TP0] Decode batch. #running-req: 1, #token: 979, token usage: 0.01, cuda graph: True, gen throughput (token/s): 10.08, #queue-req: 0, 
[2025-10-13 01:58:03 TP0] Decode batch. #running-req: 1, #token: 1019, token usage: 0.01, cuda graph: True, gen throughput (token/s): 52.05, #queue-req: 0, 
[2025-10-13 01:58:04 TP0] Decode batch. #running-req: 1, #token: 1059, token usage: 0.01, cuda graph: True, gen throughput (token/s): 52.84, #queue-req: 0, 
[2025-10-13 01:58:05 TP0] Decode batch. #running-req: 1, #token: 1099, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.38, #queue-req: 0, 
[2025-10-13 01:58:05 TP0] Decode batch. #running-req: 1, #token: 1139, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.45, #queue-req: 0, 
[2025-10-13 01:58:06 TP0] Decode batch. #running-req: 1, #token: 1179, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.09, #queue-req: 0, 
[2025-10-13 01:58:07 TP0] Decode batch. #running-req: 1, #token: 1219, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.18, #queue-req: 0, 
[2025-10-13 01:58:08 TP0] Decode batch. #running-req: 1, #token: 1259, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.45, #queue-req: 0, 
[2025-10-13 01:58:08 TP0] Decode batch. #running-req: 1, #token: 1299, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.26, #queue-req: 0, 
[2025-10-13 01:58:09 TP0] Decode batch. #running-req: 1, #token: 1339, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.10, #queue-req: 0, 
[2025-10-13 01:58:10 TP0] Decode batch. #running-req: 1, #token: 1379, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.25, #queue-req: 0, 
[2025-10-13 01:58:11 TP0] Decode batch. #running-req: 1, #token: 1419, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.35, #queue-req: 0, 
[2025-10-13 01:58:12 TP0] Decode batch. #running-req: 1, #token: 1459, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.22, #queue-req: 0, 
[2025-10-13 01:58:12 TP0] Decode batch. #running-req: 1, #token: 1499, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.20, #queue-req: 0, 
[2025-10-13 01:58:13 TP0] Decode batch. #running-req: 1, #token: 1539, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.21, #queue-req: 0, 
[2025-10-13 01:58:14 TP0] Decode batch. #running-req: 1, #token: 1579, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.17, #queue-req: 0, 
[2025-10-13 01:58:15 TP0] Decode batch. #running-req: 1, #token: 1619, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.13, #queue-req: 0, 
[2025-10-13 01:58:15 TP0] Decode batch. #running-req: 1, #token: 1659, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.10, #queue-req: 0, 
[2025-10-13 01:58:16 TP0] Decode batch. #running-req: 1, #token: 1699, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.24, #queue-req: 0, 
[2025-10-13 01:58:17 TP0] Decode batch. #running-req: 1, #token: 1739, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.19, #queue-req: 0, 
[2025-10-13 01:58:18 TP0] Decode batch. #running-req: 1, #token: 1779, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.09, #queue-req: 0, 
[2025-10-13 01:58:18 TP0] Decode batch. #running-req: 1, #token: 1819, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.17, #queue-req: 0, 
[2025-10-13 01:58:19] Finish: obj=GenerateReqInput(rid='4b22464fe3d949fc8ddcde1e1804300c', text=None, input_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 1502, 9805, 25, 128009, 128006, 882, 128007, 271, 19182, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 30, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 30, 128009, 128006, 882, 128007, 271, 40, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 30, 128009, 128006, 78191, 128007, 271, 40914, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 13, 128009, 128006, 882, 128007, 271, 3923, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 30, 128009, 128006, 78191, 128007, 271], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.6, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 0.9, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "What a fascinating setup!\n\nTo provide a rough estimate of inference speed, I'll break down the components and their potential impact:\n\n1. **Hardware**:\n\t* 6x NVIDIA GeForce RTX 3090: These GPUs are powerful, but their performance will depend on the specific workload and optimization.\n\t* AMD EPYC 73F3: A high-end CPU with 32 cores, which should provide sufficient processing power for the backend.\n\t* 512GB DDR4-3200: Ample memory, but the speed might be a bottleneck for certain workloads.\n2. **Software**:\n\t* ESXi 8.0.3uG: A virtualization platform, which might introduce some overhead.\n\t* Ubuntu 24.04 LTS VM: A Linux distribution, which should provide a stable environment for the backend.\n\t* TabbyAPI/ExLLAMAv3 backend: A custom backend, which might have specific optimization requirements.\n\t* Open-webui frontend: A web-based interface, which shouldn't significantly impact inference speed.\n3. **Modifiers**:\n\t* NVLink: This will likely provide a significant boost to inference speed, as it enables direct GPU-to-GPU " ... " than the baseline (around 90-180 ms per sample)\n* Batch size: might be increased due to improved system performance (around 8-16 GB per GPU)\n\nKeep in mind that these estimates are rough and based on general assumptions about the hardware and software setup. The actual performance will depend on various factors, including the specific model, dataset, and optimization techniques used.\n\nTo achieve the best possible performance, I recommend:\n\n1. Enabling NVLink to improve GPU-to-GPU communication.\n2. Using the modded directgpu p2p drivers to optimize GPU-to-GPU communication.\n3. Overclocking memory to improve memory bandwidth.\n4. Removing or adjusting power limits to allow the system to run at higher performance levels.\n5. Considering a bare metal setup to eliminate virtualization overhead.\n\nHowever, be cautious when applying these modifiers, as they may introduce system instability or other issues. Monitor the system's performance, temperature, and power consumption closely to ensure optimal and safe operation.", 'output_ids': [128009], 'meta_info': {'id': '4b22464fe3d949fc8ddcde1e1804300c', 'finish_reason': {'type': 'stop', 'matched': 128009}, 'prompt_tokens': 938, 'weight_version': 'default', 'completion_tokens': 904, 'cached_tokens': 0, 'e2e_latency': 19.012553215026855}}
[2025-10-13 01:58:19] Receive: obj=GenerateReqInput(rid='2041598f0512402f8e0eb706ce1b60c9', text=None, input_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 28653, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 5380, 5045, 3931, 2891, 25, 22691, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 5380, 6584, 25, 358, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 5380, 5045, 3931, 2891, 25, 23371, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 627, 6584, 25, 3639, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 5380, 5045, 3931, 2891, 25, 3639, 264, 27387, 6642, 2268, 1271, 3493, 264, 11413, 16430, 315, 45478, 4732, 11, 358, 3358, 1464, 1523, 279, 6956, 323, 872, 4754, 5536, 1473, 16, 13, 3146, 67962, 334, 512, 197, 9, 220, 21, 87, 34661, 38620, 10860, 55, 220, 15500, 15, 25, 4314, 71503, 527, 8147, 11, 719, 872, 5178, 690, 6904, 389, 279, 3230, 54696, 323, 26329, 627, 197, 9, 25300, 19613, 93944, 220, 5958, 37, 18, 25, 362, 1579, 13368, 14266, 449, 220, 843, 36804, 11, 902, 1288, 3493, 14343, 8863, 2410, 369, 279, 19713, 627, 197, 9, 220, 8358, 5494, 44860, 19, 12, 9588, 15, 25, 3383, 698, 5044, 11, 719, 279, 4732, 2643, 387, 264, 88938, 369, 3738, 990, 33785, 627, 17, 13, 3146, 19805, 334, 512, 197, 9, 19844, 71730, 220, 23, 13, 15, 13, 18, 84, 38, 25, 362, 4200, 2065, 5452, 11, 902, 2643, 19678, 1063, 32115, 627, 197, 9, 36060, 220, 1187, 13, 2371, 80128, 18270, 25, 362, 14677, 8141, 11, 902, 1288, 3493, 264, 15528, 4676, 369, 279, 19713, 627, 197, 9, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 25, 362, 2587, 19713, 11, 902, 2643, 617, 3230, 26329, 8670, 627, 197, 9, 5377, 30531, 2005, 46745, 25, 362, 3566, 6108, 3834, 11, 902, 13434, 956, 12207, 5536, 45478, 4732, 627, 18, 13, 3146, 68037, 334, 512, 197, 9, 25464, 4026, 25, 1115, 690, 4461, 3493, 264, 5199, 7916, 311, 45478, 4732, 11, 439, 433, 20682, 2167, 23501, 4791, 12279, 6459, 10758, 627, 197, 9, 5768, 9988, 2167, 43694, 281, 17, 79, 12050, 25, 4314, 12050, 2643, 7417, 5178, 555, 74142, 23501, 4791, 12279, 6459, 10758, 11, 719, 279, 5536, 690, 6904, 389, 279, 3230, 1005, 1162, 627, 197, 9, 6193, 21321, 287, 5044, 25, 1115, 1436, 13893, 7417, 5178, 11, 719, 433, 1101, 12992, 279, 5326, 315, 1887, 56399, 627, 197, 9, 7572, 13693, 25, 67803, 477, 43468, 2410, 13693, 2643, 2187, 279, 1887, 311, 1629, 520, 5190, 5178, 5990, 11, 719, 433, 1101, 12992, 279, 5326, 315, 72389, 1113, 477, 2410, 15652, 4819, 627, 197, 9, 61892, 9501, 4619, 315, 19844, 71730, 18270, 25, 29125, 279, 1887, 389, 12743, 9501, 1053, 4461, 22472, 1063, 32115, 323, 3493, 2731, 5178, 11, 719, 433, 1101, 7612, 810, 19248, 323, 13709, 382, 5733, 30589, 279, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 374, 34440, 369, 279, 12035, 323, 3241, 6642, 11, 1618, 527, 1063, 11413, 17989, 315, 45478, 4732, 1473, 334, 65711, 320, 30096, 36770, 33395, 1473, 9, 763, 2251, 4732, 25, 2212, 220, 1041, 12, 1049, 10030, 824, 6205, 320, 81580, 389, 279, 1646, 1404, 323, 23965, 340, 9, 35004, 1404, 25, 4461, 7347, 555, 279, 23501, 5044, 320, 20019, 220, 19, 12, 23, 19397, 824, 23501, 696, 334, 2409, 25464, 4026, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 508, 12, 1135, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1490, 12, 3965, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 23501, 4791, 12279, 6459, 10758, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 2409, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 605, 12, 508, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1954, 12, 5245, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 23501, 4791, 12279, 6459, 10758, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 1959, 21321, 287, 5044, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 20, 12, 605, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 2721, 12, 7028, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 5044, 34494, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 64719, 2410, 13693, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 605, 12, 508, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1954, 12, 5245, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 23501, 5178, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 605, 12, 508, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1954, 12, 5245, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 1887, 5178, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 19999, 304, 4059, 430, 1521, 17989, 527, 11413, 323, 3196, 389, 4689, 32946, 922, 279, 12035, 323, 3241, 6642, 13, 578, 5150, 5178, 690, 6904, 389, 5370, 9547, 11, 2737, 279, 3230, 1646, 11, 10550, 11, 323, 26329, 12823, 1511, 382, 1271, 11322, 279, 1888, 3284, 5178, 11, 358, 7079, 1473, 16, 13, 2998, 19295, 25464, 4026, 311, 7417, 23501, 4791, 12279, 6459, 10758, 627, 17, 13, 12362, 279, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 311, 30536, 23501, 4791, 12279, 6459, 10758, 627, 18, 13, 6193, 21321, 287, 5044, 311, 7417, 5044, 34494, 627, 19, 13, 67803, 477, 43468, 2410, 13693, 311, 2187, 279, 1887, 311, 1629, 520, 5190, 5178, 5990, 627, 20, 13, 56877, 264, 12743, 9501, 6642, 311, 22472, 4200, 2065, 32115, 382, 11458, 11, 387, 46878, 994, 19486, 1521, 36770, 11, 439, 814, 1253, 19678, 1887, 56399, 477, 1023, 4819, 13, 24423, 279, 1887, 596, 5178, 11, 9499, 11, 323, 2410, 15652, 15499, 311, 6106, 23669, 323, 6220, 5784, 627, 524, 9884, 20389, 29, 128009, 128006, 78191, 128007, 271], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.6, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 0.9, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 01:58:19 TP0] Prefill batch. #new-seq: 1, #new-token: 2011, #cached-token: 25, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 01:58:22 TP0] Decode batch. #running-req: 1, #token: 2053, token usage: 0.02, cuda graph: True, gen throughput (token/s): 11.14, #queue-req: 0, 
[2025-10-13 01:58:23 TP0] Decode batch. #running-req: 1, #token: 2093, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.63, #queue-req: 0, 
[2025-10-13 01:58:23 TP0] Decode batch. #running-req: 1, #token: 2133, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.02, #queue-req: 0, 
[2025-10-13 01:58:24] Finish: obj=GenerateReqInput(rid='2041598f0512402f8e0eb706ce1b60c9', text=None, input_ids=[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 28653, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 5380, 5045, 3931, 2891, 25, 22691, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 5380, 6584, 25, 358, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 5380, 5045, 3931, 2891, 25, 23371, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 627, 6584, 25, 3639, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 5380, 5045, 3931, 2891, 25, 3639, 264, 27387, 6642, 2268, 1271, 3493, 264, 11413, 16430, 315, 45478, 4732, 11, 358, 3358, 1464, 1523, 279, 6956, 323, 872, 4754, 5536, 1473, 16, 13, 3146, 67962, 334, 512, 197, 9, 220, 21, 87, 34661, 38620, 10860, 55, 220, 15500, 15, 25, 4314, 71503, 527, 8147, 11, 719, 872, 5178, 690, 6904, 389, 279, 3230, 54696, 323, 26329, 627, 197, 9, 25300, 19613, 93944, 220, 5958, 37, 18, 25, 362, 1579, 13368, 14266, 449, 220, 843, 36804, 11, 902, 1288, 3493, 14343, 8863, 2410, 369, 279, 19713, 627, 197, 9, 220, 8358, 5494, 44860, 19, 12, 9588, 15, 25, 3383, 698, 5044, 11, 719, 279, 4732, 2643, 387, 264, 88938, 369, 3738, 990, 33785, 627, 17, 13, 3146, 19805, 334, 512, 197, 9, 19844, 71730, 220, 23, 13, 15, 13, 18, 84, 38, 25, 362, 4200, 2065, 5452, 11, 902, 2643, 19678, 1063, 32115, 627, 197, 9, 36060, 220, 1187, 13, 2371, 80128, 18270, 25, 362, 14677, 8141, 11, 902, 1288, 3493, 264, 15528, 4676, 369, 279, 19713, 627, 197, 9, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 25, 362, 2587, 19713, 11, 902, 2643, 617, 3230, 26329, 8670, 627, 197, 9, 5377, 30531, 2005, 46745, 25, 362, 3566, 6108, 3834, 11, 902, 13434, 956, 12207, 5536, 45478, 4732, 627, 18, 13, 3146, 68037, 334, 512, 197, 9, 25464, 4026, 25, 1115, 690, 4461, 3493, 264, 5199, 7916, 311, 45478, 4732, 11, 439, 433, 20682, 2167, 23501, 4791, 12279, 6459, 10758, 627, 197, 9, 5768, 9988, 2167, 43694, 281, 17, 79, 12050, 25, 4314, 12050, 2643, 7417, 5178, 555, 74142, 23501, 4791, 12279, 6459, 10758, 11, 719, 279, 5536, 690, 6904, 389, 279, 3230, 1005, 1162, 627, 197, 9, 6193, 21321, 287, 5044, 25, 1115, 1436, 13893, 7417, 5178, 11, 719, 433, 1101, 12992, 279, 5326, 315, 1887, 56399, 627, 197, 9, 7572, 13693, 25, 67803, 477, 43468, 2410, 13693, 2643, 2187, 279, 1887, 311, 1629, 520, 5190, 5178, 5990, 11, 719, 433, 1101, 12992, 279, 5326, 315, 72389, 1113, 477, 2410, 15652, 4819, 627, 197, 9, 61892, 9501, 4619, 315, 19844, 71730, 18270, 25, 29125, 279, 1887, 389, 12743, 9501, 1053, 4461, 22472, 1063, 32115, 323, 3493, 2731, 5178, 11, 719, 433, 1101, 7612, 810, 19248, 323, 13709, 382, 5733, 30589, 279, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 374, 34440, 369, 279, 12035, 323, 3241, 6642, 11, 1618, 527, 1063, 11413, 17989, 315, 45478, 4732, 1473, 334, 65711, 320, 30096, 36770, 33395, 1473, 9, 763, 2251, 4732, 25, 2212, 220, 1041, 12, 1049, 10030, 824, 6205, 320, 81580, 389, 279, 1646, 1404, 323, 23965, 340, 9, 35004, 1404, 25, 4461, 7347, 555, 279, 23501, 5044, 320, 20019, 220, 19, 12, 23, 19397, 824, 23501, 696, 334, 2409, 25464, 4026, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 508, 12, 1135, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1490, 12, 3965, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 23501, 4791, 12279, 6459, 10758, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 2409, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 605, 12, 508, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1954, 12, 5245, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 23501, 4791, 12279, 6459, 10758, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 1959, 21321, 287, 5044, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 20, 12, 605, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 2721, 12, 7028, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 5044, 34494, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 64719, 2410, 13693, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 605, 12, 508, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1954, 12, 5245, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 23501, 5178, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 334, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 334, 1473, 9, 763, 2251, 4732, 25, 13893, 220, 605, 12, 508, 4, 10819, 1109, 279, 26954, 320, 20019, 220, 1954, 12, 5245, 10030, 824, 6205, 340, 9, 35004, 1404, 25, 2643, 387, 7319, 4245, 311, 13241, 1887, 5178, 320, 20019, 220, 23, 12, 845, 19397, 824, 23501, 696, 19999, 304, 4059, 430, 1521, 17989, 527, 11413, 323, 3196, 389, 4689, 32946, 922, 279, 12035, 323, 3241, 6642, 13, 578, 5150, 5178, 690, 6904, 389, 5370, 9547, 11, 2737, 279, 3230, 1646, 11, 10550, 11, 323, 26329, 12823, 1511, 382, 1271, 11322, 279, 1888, 3284, 5178, 11, 358, 7079, 1473, 16, 13, 2998, 19295, 25464, 4026, 311, 7417, 23501, 4791, 12279, 6459, 10758, 627, 17, 13, 12362, 279, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 311, 30536, 23501, 4791, 12279, 6459, 10758, 627, 18, 13, 6193, 21321, 287, 5044, 311, 7417, 5044, 34494, 627, 19, 13, 67803, 477, 43468, 2410, 13693, 311, 2187, 279, 1887, 311, 1629, 520, 5190, 5178, 5990, 627, 20, 13, 56877, 264, 12743, 9501, 6642, 311, 22472, 4200, 2065, 32115, 382, 11458, 11, 387, 46878, 994, 19486, 1521, 36770, 11, 439, 814, 1253, 19678, 1887, 56399, 477, 1023, 4819, 13, 24423, 279, 1887, 596, 5178, 11, 9499, 11, 323, 2410, 15652, 15499, 311, 6106, 23669, 323, 6220, 5784, 627, 524, 9884, 20389, 29, 128009, 128006, 78191, 128007, 271], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0.6, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 0.9, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '{"follow_ups": ["Can you provide more specific numbers on the potential performance boost from enabling NVLink?", "How would you recommend optimizing the TabbyAPI/ExLLAMAv3 backend for this hardware setup?", "What are the potential risks and downsides to overclocking memory and removing power limits in this system?", "Would switching to a different virtualization platform or a bare metal setup have a significant impact on inference speed?", "Are there any other potential bottlenecks or areas for optimization in this system that we haven\'t discussed yet?"]}', 'output_ids': [128009, 128006, 78191, 128007, 271, 5018, 19070, 8401, 82, 794, 4482, 6854, 499, 3493, 810, 3230, 5219, 389, 279, 4754, 5178, 7916, 505, 28462, 25464, 4026, 32111, 330, 4438, 1053, 499, 7079, 74142, 279, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 369, 420, 12035, 6642, 32111, 330, 3923, 527, 279, 4754, 15635, 323, 40291, 3422, 311, 81294, 287, 5044, 323, 18054, 2410, 13693, 304, 420, 1887, 32111, 330, 29089, 28865, 311, 264, 2204, 4200, 2065, 5452, 477, 264, 12743, 9501, 6642, 617, 264, 5199, 5536, 389, 45478, 4732, 32111, 330, 11787, 1070, 904, 1023, 4754, 11176, 35291, 14895, 477, 5789, 369, 26329, 304, 420, 1887, 430, 584, 9167, 956, 14407, 3686, 30, 93546, 128009], 'meta_info': {'id': '2041598f0512402f8e0eb706ce1b60c9', 'finish_reason': {'type': 'stop', 'matched': 128009}, 'prompt_tokens': 2036, 'weight_version': 'default', 'completion_tokens': 112, 'cached_tokens': 25, 'e2e_latency': 4.855648994445801}}
[2025-10-13 01:58:24] INFO:     127.0.0.1:38676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 02:01:07] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 02:01:07] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 02:01:09] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:01:17] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=798286495, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:01:17] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:01:17] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:01:25 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:01:25 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:01:25 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:01:25 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:01:25 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:01:26 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:01:26 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:01:26 TP0] Init torch distributed begin.
[2025-10-13 02:01:26 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:01:26 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:01:27 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:01:27 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:01:27 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:01:27 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:01:27 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:01:27 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:01:27 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:01:27 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:01:27 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:01:27 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:01:28 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.93it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:03,  2.18it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.33it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:01<00:02,  2.17it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:01<00:01,  2.91it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.58it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:02<00:00,  2.37it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:03<00:00,  2.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.18it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.30it/s]

[2025-10-13 02:01:33 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 02:01:33 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 02:01:33 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:01:33 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:01:33 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:01:33 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 02:01:33 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:01:33 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 02:01:33 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:15,  1.19s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:15,  1.19s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.37it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.37it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:01<00:05,  1.95it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:01<00:05,  1.95it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.47it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.47it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.97it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.97it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.44it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.44it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:55<02:02, 17.55s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:55<02:02, 17.55s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:15<01:50, 18.38s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:15<01:50, 18.38s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:39<01:40, 20.17s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:39<01:40, 20.17s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [02:03<01:25, 21.34s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [02:03<01:25, 21.34s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [02:31<01:10, 23.36s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [02:31<01:10, 23.36s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [03:00<00:50, 25.02s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [03:00<00:50, 25.02s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [03:29<00:26, 26.06s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [03:29<00:26, 26.06s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:56<00:00, 26.44s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:56<00:00, 16.88s/it]
[2025-10-13 02:05:30 TP0] Capture cuda graph end. Time elapsed: 237.65 s. mem usage=0.41 GB. avail mem=3.23 GB.
[2025-10-13 02:05:31 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.23 GB
[2025-10-13 02:05:32] INFO:     Started server process [1411781]
[2025-10-13 02:05:32] INFO:     Waiting for application startup.
[2025-10-13 02:05:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 02:05:32] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 02:05:32] INFO:     Application startup complete.
[2025-10-13 02:05:32] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 02:05:33] INFO:     127.0.0.1:49386 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 02:05:33] Receive: obj=GenerateReqInput(rid='7462ed4d575e4a18b7741c8924a51cfc', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 02:05:33 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:05:35] Finish: obj=GenerateReqInput(rid='7462ed4d575e4a18b7741c8924a51cfc', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is known for its iconic', 'output_ids': [6864, 3363, 315, 9822, 374, 12366, 13, 1102, 374, 3967, 369, 1202, 27373], 'meta_info': {'id': '7462ed4d575e4a18b7741c8924a51cfc', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.678250789642334}}
[2025-10-13 02:05:35] INFO:     127.0.0.1:49402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 02:05:35] The server is fired up and ready to roll!
[2025-10-13 02:05:36] INFO:     127.0.0.1:49406 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 02:05:36] INFO:     127.0.0.1:49422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 02:05:36] Receive: obj=GenerateReqInput(rid='df667acba71541d5b64e1488688366cd', text=None, input_ids=[128000, 1502, 9805, 1473, 128011, 19182, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 30, 128012, 9906, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 30, 128001, 128011, 40, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 30, 128012, 40914, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 13, 128001, 128011, 3923, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 30, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 02:05:36 TP0] Prefill batch. #new-seq: 1, #new-token: 891, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:05:38 TP0] Decode batch. #running-req: 1, #token: 925, token usage: 0.01, cuda graph: True, gen throughput (token/s): 5.92, #queue-req: 0, 
[2025-10-13 02:05:39 TP0] Decode batch. #running-req: 1, #token: 965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.36, #queue-req: 0, 
[2025-10-13 02:05:39 TP0] Decode batch. #running-req: 1, #token: 1005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.32, #queue-req: 0, 
[2025-10-13 02:05:40 TP0] Decode batch. #running-req: 1, #token: 1045, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.35, #queue-req: 0, 
[2025-10-13 02:05:41 TP0] Decode batch. #running-req: 1, #token: 1085, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.35, #queue-req: 0, 
[2025-10-13 02:05:42 TP0] Decode batch. #running-req: 1, #token: 1125, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.27, #queue-req: 0, 
[2025-10-13 02:05:42 TP0] Decode batch. #running-req: 1, #token: 1165, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.24, #queue-req: 0, 
[2025-10-13 02:05:43 TP0] Decode batch. #running-req: 1, #token: 1205, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.30, #queue-req: 0, 
[2025-10-13 02:05:44 TP0] Decode batch. #running-req: 1, #token: 1245, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.29, #queue-req: 0, 
[2025-10-13 02:05:45 TP0] Decode batch. #running-req: 1, #token: 1285, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.27, #queue-req: 0, 
[2025-10-13 02:05:45 TP0] Decode batch. #running-req: 1, #token: 1325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.22, #queue-req: 0, 
[2025-10-13 02:05:46 TP0] Decode batch. #running-req: 1, #token: 1365, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.30, #queue-req: 0, 
[2025-10-13 02:05:47 TP0] Decode batch. #running-req: 1, #token: 1405, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.19, #queue-req: 0, 
[2025-10-13 02:05:48 TP0] Decode batch. #running-req: 1, #token: 1445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.21, #queue-req: 0, 
[2025-10-13 02:05:48 TP0] Decode batch. #running-req: 1, #token: 1485, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.30, #queue-req: 0, 
[2025-10-13 02:05:49 TP0] Decode batch. #running-req: 1, #token: 1525, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.15, #queue-req: 0, 
[2025-10-13 02:05:50 TP0] Decode batch. #running-req: 1, #token: 1565, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.10, #queue-req: 0, 
[2025-10-13 02:05:51 TP0] Decode batch. #running-req: 1, #token: 1605, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.28, #queue-req: 0, 
[2025-10-13 02:05:51 TP0] Decode batch. #running-req: 1, #token: 1645, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.21, #queue-req: 0, 
[2025-10-13 02:05:52 TP0] Decode batch. #running-req: 1, #token: 1685, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.10, #queue-req: 0, 
[2025-10-13 02:05:53 TP0] Decode batch. #running-req: 1, #token: 1725, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.19, #queue-req: 0, 
[2025-10-13 02:05:54 TP0] Decode batch. #running-req: 1, #token: 1765, token usage: 0.01, cuda graph: True, gen throughput (token/s): 53.15, #queue-req: 0, 
[2025-10-13 02:05:54 TP0] Decode batch. #running-req: 1, #token: 1805, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.04, #queue-req: 0, 
[2025-10-13 02:05:55 TP0] Decode batch. #running-req: 1, #token: 1845, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.13, #queue-req: 0, 
[2025-10-13 02:05:56 TP0] Decode batch. #running-req: 1, #token: 1885, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.22, #queue-req: 0, 
[2025-10-13 02:05:57 TP0] Decode batch. #running-req: 1, #token: 1925, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.08, #queue-req: 0, 
[2025-10-13 02:05:57 TP0] Decode batch. #running-req: 1, #token: 1965, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.03, #queue-req: 0, 
[2025-10-13 02:05:58 TP0] Decode batch. #running-req: 1, #token: 2005, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.19, #queue-req: 0, 
[2025-10-13 02:05:59 TP0] Decode batch. #running-req: 1, #token: 2045, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.05, #queue-req: 0, 
[2025-10-13 02:06:00 TP0] Decode batch. #running-req: 1, #token: 2085, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.96, #queue-req: 0, 
[2025-10-13 02:06:00 TP0] Decode batch. #running-req: 1, #token: 2125, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.03, #queue-req: 0, 
[2025-10-13 02:06:01 TP0] Decode batch. #running-req: 1, #token: 2165, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.06, #queue-req: 0, 
[2025-10-13 02:06:02 TP0] Decode batch. #running-req: 1, #token: 2205, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.96, #queue-req: 0, 
[2025-10-13 02:06:03 TP0] Decode batch. #running-req: 1, #token: 2245, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.04, #queue-req: 0, 
[2025-10-13 02:06:03 TP0] Decode batch. #running-req: 1, #token: 2285, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.05, #queue-req: 0, 
[2025-10-13 02:06:04 TP0] Decode batch. #running-req: 1, #token: 2325, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.06, #queue-req: 0, 
[2025-10-13 02:06:05 TP0] Decode batch. #running-req: 1, #token: 2365, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.99, #queue-req: 0, 
[2025-10-13 02:06:06 TP0] Decode batch. #running-req: 1, #token: 2405, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.00, #queue-req: 0, 
[2025-10-13 02:06:06 TP0] Decode batch. #running-req: 1, #token: 2445, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.05, #queue-req: 0, 
[2025-10-13 02:06:07 TP0] Decode batch. #running-req: 1, #token: 2485, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.96, #queue-req: 0, 
[2025-10-13 02:06:08 TP0] Decode batch. #running-req: 1, #token: 2525, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.92, #queue-req: 0, 
[2025-10-13 02:06:09 TP0] Decode batch. #running-req: 1, #token: 2565, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.06, #queue-req: 0, 
[2025-10-13 02:06:09 TP0] Decode batch. #running-req: 1, #token: 2605, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.95, #queue-req: 0, 
[2025-10-13 02:06:10 TP0] Decode batch. #running-req: 1, #token: 2645, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.91, #queue-req: 0, 
[2025-10-13 02:06:11] Finish: obj=GenerateReqInput(rid='df667acba71541d5b64e1488688366cd', text=None, input_ids=[128000, 1502, 9805, 1473, 128011, 19182, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 30, 128012, 9906, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 30, 128001, 128011, 40, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 30, 128012, 40914, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 13, 128001, 128011, 3923, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 30, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "<think>\nAlright, the user is asking about inference speed in a pretty technical setup. They've got 6x3090s, an Epyc 73F3, 512GB DDR4-3200, running ESXi 8.0.3uG, with an Ubuntu 24.04 LTS VM, using TabbyAPI/ExLLAMAv3 backend and OpenWebUI frontend. They also mentioned some modifiers like nvlink, modded drivers, overclocking, power limits, and bare metal versus VM.\n\nFirst, I need to understand the components involved. The 3090s are powerful GPUs, so they'll handle the compute tasks. The Epyc CPU is a server-grade processor, which is good for multi-threading but might have higher latency compared to consumer CPUs. The RAM is plenty, which is good for large models or datasets.\n\nNow, thinking about the modifiers one by one:\n\n1. **NVLINK vs. PCIe:** NVLink provides higher bandwidth and lower latency between GPUs, which is great for models that require frequent data transfer between GPUs. If the backend uses model parallelism, NVLink will be beneficial. However, if the model is small enough to fit on a single GPU, NV" ... 'runing.\n\n2. **Multi-GPU Inference**:\n   - With 6 GPUs and NVLink, you could achieve **5-10x speedup** for large-scale models compared to single-GPU inference.\n   - Performance will scale roughly linearly with the number of GPUs, assuming the model supports model parallelism.\n\n3. **Bare Metal vs. VM**:\n   - Bare-metal setups can achieve **50-100 ms faster per token generation** compared to VM setups for large workloads.\n\n---\n\n### **Recommendations**\n\n- **Bare Metal**: Strongly recommended for the best performance.\n- **NVLink**: Enable it if you’re using multi-GPU setups.\n- **Modded Drivers**: Install them if you’re running multi-GPU workloads with heavy P2P data transfer.\n- **Overclocking**: Focus on GPU boost clocks rather than memory overclocking.\n- **ESXi VM**: If you must use virtualization, ensure GPU passthrough is fully optimized and test with small models before scaling up.\n\nBy optimizing these factors, you should be able to achieve excellent inference speeds for your TabbyAPI and ExLLAMA v3 workloads.', 'output_ids': [128001], 'meta_info': {'id': 'df667acba71541d5b64e1488688366cd', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 892, 'weight_version': 'default', 'completion_tokens': 1789, 'cached_tokens': 1, 'e2e_latency': 34.979108810424805}}
[2025-10-13 02:06:11] Receive: obj=GenerateReqInput(rid='4f871084e86b4e889ea83e10beada683', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 28653, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 5380, 5045, 3931, 2891, 25, 22691, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 5380, 6584, 25, 358, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 5380, 5045, 3931, 2891, 25, 23371, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 627, 6584, 25, 3639, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11] ... [220, 24, 382, 18, 13, 3146, 8358, 5494, 44860, 19, 12, 9588, 15, 22813, 334, 512, 256, 482, 3383, 698, 5044, 369, 1455, 15592, 990, 33785, 11, 2737, 3544, 4221, 4211, 627, 256, 482, 6193, 21321, 287, 279, 5044, 649, 3493, 32873, 18637, 11, 719, 433, 753, 17821, 311, 387, 264, 88938, 7389, 4401, 1633, 3544, 4211, 382, 19, 13, 3146, 1600, 71730, 220, 23, 13, 15, 13, 18, 84, 18, 334, 512, 256, 482, 29125, 15592, 990, 33785, 304, 264, 18270, 649, 19678, 1063, 32115, 7863, 311, 12743, 89932, 84823, 13, 4452, 11, 279, 5536, 14117, 389, 3508, 279, 18270, 374, 10489, 34440, 369, 23501, 6502, 88001, 323, 12035, 31903, 382, 20, 13, 3146, 76752, 220, 1187, 13, 2371, 80128, 334, 512, 256, 482, 362, 6617, 14677, 8141, 449, 1695, 1862, 369, 23501, 12050, 323, 15592, 49125, 382, 45464, 14711, 3146, 68037, 18825, 57277, 827, 220, 16, 13, 3146, 2409, 14, 27131, 25464, 4026, 1035, 256, 482, 3146, 37426, 4026, 334, 5825, 5190, 34494, 323, 4827, 40370, 369, 828, 8481, 1990, 71503, 13, 1442, 701, 1646, 7720, 505, 7447, 12279, 6459, 4967, 477, 45478, 320, 68, 1326, 2637, 1646, 15638, 2191, 705, 25464, 4026, 690, 12207, 7417, 5178, 627, 256, 482, 17586, 25464, 4026, 11, 91790, 320, 87, 845, 4173, 220, 19, 8, 690, 2103, 3493, 1579, 34494, 11, 719, 40370, 690, 387, 5190, 11, 323, 5178, 1253, 96630, 369, 3738, 990, 33785, 382, 827, 220, 17, 13, 3146, 2409, 14, 27131, 5768, 9988, 7286, 50991, 393, 17, 47, 46837, 1035, 256, 482, 5768, 9988, 12050, 430, 7431, 393, 17, 47, 320, 31988, 4791, 9483, 35856, 8, 5044, 2680, 649, 7417, 5178, 304, 7447, 12279, 6459, 84823, 555, 18189, 279, 32115, 315, 828, 31711, 1990, 71503, 627, 256, 482, 1442, 279, 54696, 17345, 34744, 389, 7447, 12279, 6459, 828, 8481, 11, 420, 1436, 1121, 304, 264, 43426, 4732, 455, 13, 4452, 11, 279, 8935, 1253, 387, 2753, 5199, 369, 3254, 12279, 6459, 45478, 382, 827, 220, 18, 13, 3146, 1959, 21321, 287, 14171, 1035, 256, 482, 6193, 21321, 287, 44860, 19, 12, 9588, 15, 5044, 649, 3493, 264, 2678, 16048, 304, 34494, 11, 902, 2643, 1520, 369, 3544, 29344, 45478, 477, 828, 74666, 990, 33785, 627, 256, 482, 4452, 11, 5044, 34494, 374, 3629, 539, 279, 33994, 8331, 304, 15592, 990, 33785, 11, 902, 527, 810, 12849, 50962, 382, 827, 220, 19, 13, 3146, 15335, 72955, 1035, 256, 482, 445, 18148, 2410, 13693, 6276, 279, 71503, 323, 14266, 311, 14201, 5190, 7916, 51437, 369, 5129, 18852, 627, 256, 482, 1952, 11761, 41327, 71503, 1093, 279, 10860, 55, 220, 15500, 15, 11, 420, 649, 1121, 304, 10284, 2731, 5178, 11, 5423, 304, 29759, 990, 33785, 382, 827, 220, 20, 13, 3146, 33, 548, 19757, 6296, 13, 19844, 71730, 18270, 1035, 256, 482, 29125, 279, 54696, 389, 3146, 55202, 9501, 334, 690, 8965, 387, 10819, 1109, 4401, 433, 304, 264, 18270, 11, 439, 18270, 82, 19678, 32115, 320, 68, 1326, 2637, 9950, 32496, 40370, 11, 358, 17991, 32174, 4390, 256, 482, 1442, 499, 3207, 1701, 3146, 50991, 6502, 88001, 334, 304, 19844, 71730, 11, 5178, 649, 5603, 12743, 89932, 25753, 11, 719, 1070, 1253, 2103, 387, 1063, 32115, 11911, 389, 279, 5696, 323, 18270, 6683, 382, 45464, 14711, 3146, 19430, 21304, 480, 1771, 57277, 12, 3146, 11126, 12279, 6459, 763, 2251, 334, 512, 220, 482, 3161, 279, 10860, 55, 220, 15500, 15, 11, 499, 649, 1755, 1579, 45478, 25753, 369, 4211, 1093, 1398, 4178, 39200, 348, 18, 11, 5423, 422, 1701, 9709, 12, 28281, 320, 11960, 845, 477, 9403, 23, 4390, 220, 482, 6193, 21321, 287, 323, 1491, 9988, 12050, 1253, 539, 3493, 5199, 18637, 369, 3254, 12279, 6459, 45478, 382, 12, 3146, 20981, 12279, 6459, 763, 2251, 320, 68, 1326, 2637, 5008, 50372, 2191, 33395, 512, 220, 482, 3161, 220, 21, 71503, 323, 25464, 4026, 11, 499, 649, 1755, 264, 5199, 4732, 455, 11, 5423, 369, 3544, 4211, 627, 220, 482, 578, 1491, 9988, 393, 17, 47, 12050, 649, 4726, 7417, 7447, 12279, 6459, 5178, 555, 18189, 91790, 8481, 32115, 382, 12, 3146, 33, 548, 19757, 6296, 13, 18270, 334, 512, 220, 482, 61892, 9501, 374, 3685, 311, 704, 29588, 279, 19844, 71730, 18270, 6642, 11, 5423, 369, 7447, 12279, 6459, 990, 33785, 382, 45464, 14711, 3146, 45320, 22018, 95619, 57277, 16, 13, 3146, 11126, 12279, 6459, 763, 2251, 334, 512, 256, 482, 3161, 459, 10860, 55, 220, 15500, 15, 11, 1755, 2212, 3146, 508, 12, 1135, 10030, 824, 4037, 9659, 334, 369, 2678, 311, 11298, 28935, 4211, 320, 68, 1326, 2637, 220, 22, 33, 5852, 4211, 8, 304, 34651, 845, 627, 256, 482, 81908, 4211, 320, 68, 1326, 2637, 220, 2031, 33, 5137, 8, 1253, 1397, 1646, 15638, 2191, 477, 86292, 382, 17, 13, 3146, 20981, 12279, 6459, 763, 2251, 334, 512, 256, 482, 3161, 220, 21, 71503, 323, 25464, 4026, 11, 499, 1436, 11322, 3146, 20, 12, 605, 87, 4732, 455, 334, 369, 3544, 13230, 4211, 7863, 311, 3254, 12279, 6459, 45478, 627, 256, 482, 21304, 690, 5569, 17715, 13790, 398, 449, 279, 1396, 315, 71503, 11, 26619, 279, 1646, 11815, 1646, 15638, 2191, 382, 18, 13, 3146, 33, 548, 19757, 6296, 13, 18270, 334, 512, 256, 482, 61892, 89932, 84823, 649, 11322, 3146, 1135, 12, 1041, 10030, 10819, 824, 4037, 9659, 334, 7863, 311, 18270, 84823, 369, 3544, 990, 33785, 382, 45464, 14711, 3146, 68744, 811, 57277, 12, 3146, 33, 548, 19757, 96618, 27191, 398, 11349, 369, 279, 1888, 5178, 627, 12, 3146, 37426, 4026, 96618, 19078, 433, 422, 499, 3207, 1701, 7447, 12279, 6459, 84823, 627, 12, 3146, 4559, 9988, 46837, 96618, 19796, 1124, 422, 499, 3207, 4401, 7447, 12279, 6459, 990, 33785, 449, 8987, 393, 17, 47, 828, 8481, 627, 12, 3146, 1959, 21321, 287, 96618, 26891, 389, 23501, 7916, 51437, 4856, 1109, 5044, 81294, 287, 627, 12, 3146, 1600, 71730, 18270, 96618, 1442, 499, 2011, 1005, 4200, 2065, 11, 6106, 23501, 6502, 88001, 374, 7373, 34440, 323, 1296, 449, 2678, 4211, 1603, 28041, 709, 382, 1383, 74142, 1521, 9547, 11, 499, 1288, 387, 3025, 311, 11322, 9250, 45478, 25753, 369, 701, 15490, 1729, 7227, 323, 1398, 4178, 39200, 348, 18, 990, 33785, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 02:06:11 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:06:12 TP0] Prefill batch. #new-seq: 1, #new-token: 279, #cached-token: 0, token usage: 0.02, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:06:14 TP0] Decode batch. #running-req: 1, #token: 2332, token usage: 0.02, cuda graph: True, gen throughput (token/s): 10.51, #queue-req: 0, 
[2025-10-13 02:06:15 TP0] Decode batch. #running-req: 1, #token: 2372, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.01, #queue-req: 0, 
[2025-10-13 02:06:16 TP0] Decode batch. #running-req: 1, #token: 2412, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.89, #queue-req: 0, 
[2025-10-13 02:06:16 TP0] Decode batch. #running-req: 1, #token: 2452, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.12, #queue-req: 0, 
[2025-10-13 02:06:17 TP0] Decode batch. #running-req: 1, #token: 2492, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.84, #queue-req: 0, 
[2025-10-13 02:06:18 TP0] Decode batch. #running-req: 1, #token: 2532, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.86, #queue-req: 0, 
[2025-10-13 02:06:19 TP0] Decode batch. #running-req: 1, #token: 2572, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.98, #queue-req: 0, 
[2025-10-13 02:06:19 TP0] Decode batch. #running-req: 1, #token: 2612, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.01, #queue-req: 0, 
[2025-10-13 02:06:20 TP0] Decode batch. #running-req: 1, #token: 2652, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.82, #queue-req: 0, 
[2025-10-13 02:06:21 TP0] Decode batch. #running-req: 1, #token: 2692, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.89, #queue-req: 0, 
[2025-10-13 02:06:22 TP0] Decode batch. #running-req: 1, #token: 2732, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.93, #queue-req: 0, 
[2025-10-13 02:06:22 TP0] Decode batch. #running-req: 1, #token: 2772, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.91, #queue-req: 0, 
[2025-10-13 02:06:23 TP0] Decode batch. #running-req: 1, #token: 2812, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.87, #queue-req: 0, 
[2025-10-13 02:06:24 TP0] Decode batch. #running-req: 1, #token: 2852, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.36, #queue-req: 0, 
[2025-10-13 02:06:25 TP0] Decode batch. #running-req: 1, #token: 2892, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.78, #queue-req: 0, 
[2025-10-13 02:06:25 TP0] Decode batch. #running-req: 1, #token: 2932, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.79, #queue-req: 0, 
[2025-10-13 02:06:26 TP0] Decode batch. #running-req: 1, #token: 2972, token usage: 0.03, cuda graph: True, gen throughput (token/s): 52.85, #queue-req: 0, 
[2025-10-13 02:06:27 TP0] Decode batch. #running-req: 1, #token: 3012, token usage: 0.03, cuda graph: True, gen throughput (token/s): 52.61, #queue-req: 0, 
[2025-10-13 02:06:28 TP0] Decode batch. #running-req: 1, #token: 3052, token usage: 0.03, cuda graph: True, gen throughput (token/s): 52.78, #queue-req: 0, 
[2025-10-13 02:06:28 TP0] Decode batch. #running-req: 1, #token: 3092, token usage: 0.03, cuda graph: True, gen throughput (token/s): 52.77, #queue-req: 0, 
[2025-10-13 02:06:29 TP0] Decode batch. #running-req: 1, #token: 3132, token usage: 0.03, cuda graph: True, gen throughput (token/s): 52.87, #queue-req: 0, 
[2025-10-13 02:06:30 TP0] Decode batch. #running-req: 1, #token: 3172, token usage: 0.03, cuda graph: True, gen throughput (token/s): 52.70, #queue-req: 0, 
[2025-10-13 02:06:30] Finish: obj=GenerateReqInput(rid='4f871084e86b4e889ea83e10beada683', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 28653, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 5380, 5045, 3931, 2891, 25, 22691, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 5380, 6584, 25, 358, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 5380, 5045, 3931, 2891, 25, 23371, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 627, 6584, 25, 3639, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11] ... [220, 24, 382, 18, 13, 3146, 8358, 5494, 44860, 19, 12, 9588, 15, 22813, 334, 512, 256, 482, 3383, 698, 5044, 369, 1455, 15592, 990, 33785, 11, 2737, 3544, 4221, 4211, 627, 256, 482, 6193, 21321, 287, 279, 5044, 649, 3493, 32873, 18637, 11, 719, 433, 753, 17821, 311, 387, 264, 88938, 7389, 4401, 1633, 3544, 4211, 382, 19, 13, 3146, 1600, 71730, 220, 23, 13, 15, 13, 18, 84, 18, 334, 512, 256, 482, 29125, 15592, 990, 33785, 304, 264, 18270, 649, 19678, 1063, 32115, 7863, 311, 12743, 89932, 84823, 13, 4452, 11, 279, 5536, 14117, 389, 3508, 279, 18270, 374, 10489, 34440, 369, 23501, 6502, 88001, 323, 12035, 31903, 382, 20, 13, 3146, 76752, 220, 1187, 13, 2371, 80128, 334, 512, 256, 482, 362, 6617, 14677, 8141, 449, 1695, 1862, 369, 23501, 12050, 323, 15592, 49125, 382, 45464, 14711, 3146, 68037, 18825, 57277, 827, 220, 16, 13, 3146, 2409, 14, 27131, 25464, 4026, 1035, 256, 482, 3146, 37426, 4026, 334, 5825, 5190, 34494, 323, 4827, 40370, 369, 828, 8481, 1990, 71503, 13, 1442, 701, 1646, 7720, 505, 7447, 12279, 6459, 4967, 477, 45478, 320, 68, 1326, 2637, 1646, 15638, 2191, 705, 25464, 4026, 690, 12207, 7417, 5178, 627, 256, 482, 17586, 25464, 4026, 11, 91790, 320, 87, 845, 4173, 220, 19, 8, 690, 2103, 3493, 1579, 34494, 11, 719, 40370, 690, 387, 5190, 11, 323, 5178, 1253, 96630, 369, 3738, 990, 33785, 382, 827, 220, 17, 13, 3146, 2409, 14, 27131, 5768, 9988, 7286, 50991, 393, 17, 47, 46837, 1035, 256, 482, 5768, 9988, 12050, 430, 7431, 393, 17, 47, 320, 31988, 4791, 9483, 35856, 8, 5044, 2680, 649, 7417, 5178, 304, 7447, 12279, 6459, 84823, 555, 18189, 279, 32115, 315, 828, 31711, 1990, 71503, 627, 256, 482, 1442, 279, 54696, 17345, 34744, 389, 7447, 12279, 6459, 828, 8481, 11, 420, 1436, 1121, 304, 264, 43426, 4732, 455, 13, 4452, 11, 279, 8935, 1253, 387, 2753, 5199, 369, 3254, 12279, 6459, 45478, 382, 827, 220, 18, 13, 3146, 1959, 21321, 287, 14171, 1035, 256, 482, 6193, 21321, 287, 44860, 19, 12, 9588, 15, 5044, 649, 3493, 264, 2678, 16048, 304, 34494, 11, 902, 2643, 1520, 369, 3544, 29344, 45478, 477, 828, 74666, 990, 33785, 627, 256, 482, 4452, 11, 5044, 34494, 374, 3629, 539, 279, 33994, 8331, 304, 15592, 990, 33785, 11, 902, 527, 810, 12849, 50962, 382, 827, 220, 19, 13, 3146, 15335, 72955, 1035, 256, 482, 445, 18148, 2410, 13693, 6276, 279, 71503, 323, 14266, 311, 14201, 5190, 7916, 51437, 369, 5129, 18852, 627, 256, 482, 1952, 11761, 41327, 71503, 1093, 279, 10860, 55, 220, 15500, 15, 11, 420, 649, 1121, 304, 10284, 2731, 5178, 11, 5423, 304, 29759, 990, 33785, 382, 827, 220, 20, 13, 3146, 33, 548, 19757, 6296, 13, 19844, 71730, 18270, 1035, 256, 482, 29125, 279, 54696, 389, 3146, 55202, 9501, 334, 690, 8965, 387, 10819, 1109, 4401, 433, 304, 264, 18270, 11, 439, 18270, 82, 19678, 32115, 320, 68, 1326, 2637, 9950, 32496, 40370, 11, 358, 17991, 32174, 4390, 256, 482, 1442, 499, 3207, 1701, 3146, 50991, 6502, 88001, 334, 304, 19844, 71730, 11, 5178, 649, 5603, 12743, 89932, 25753, 11, 719, 1070, 1253, 2103, 387, 1063, 32115, 11911, 389, 279, 5696, 323, 18270, 6683, 382, 45464, 14711, 3146, 19430, 21304, 480, 1771, 57277, 12, 3146, 11126, 12279, 6459, 763, 2251, 334, 512, 220, 482, 3161, 279, 10860, 55, 220, 15500, 15, 11, 499, 649, 1755, 1579, 45478, 25753, 369, 4211, 1093, 1398, 4178, 39200, 348, 18, 11, 5423, 422, 1701, 9709, 12, 28281, 320, 11960, 845, 477, 9403, 23, 4390, 220, 482, 6193, 21321, 287, 323, 1491, 9988, 12050, 1253, 539, 3493, 5199, 18637, 369, 3254, 12279, 6459, 45478, 382, 12, 3146, 20981, 12279, 6459, 763, 2251, 320, 68, 1326, 2637, 5008, 50372, 2191, 33395, 512, 220, 482, 3161, 220, 21, 71503, 323, 25464, 4026, 11, 499, 649, 1755, 264, 5199, 4732, 455, 11, 5423, 369, 3544, 4211, 627, 220, 482, 578, 1491, 9988, 393, 17, 47, 12050, 649, 4726, 7417, 7447, 12279, 6459, 5178, 555, 18189, 91790, 8481, 32115, 382, 12, 3146, 33, 548, 19757, 6296, 13, 18270, 334, 512, 220, 482, 61892, 9501, 374, 3685, 311, 704, 29588, 279, 19844, 71730, 18270, 6642, 11, 5423, 369, 7447, 12279, 6459, 990, 33785, 382, 45464, 14711, 3146, 45320, 22018, 95619, 57277, 16, 13, 3146, 11126, 12279, 6459, 763, 2251, 334, 512, 256, 482, 3161, 459, 10860, 55, 220, 15500, 15, 11, 1755, 2212, 3146, 508, 12, 1135, 10030, 824, 4037, 9659, 334, 369, 2678, 311, 11298, 28935, 4211, 320, 68, 1326, 2637, 220, 22, 33, 5852, 4211, 8, 304, 34651, 845, 627, 256, 482, 81908, 4211, 320, 68, 1326, 2637, 220, 2031, 33, 5137, 8, 1253, 1397, 1646, 15638, 2191, 477, 86292, 382, 17, 13, 3146, 20981, 12279, 6459, 763, 2251, 334, 512, 256, 482, 3161, 220, 21, 71503, 323, 25464, 4026, 11, 499, 1436, 11322, 3146, 20, 12, 605, 87, 4732, 455, 334, 369, 3544, 13230, 4211, 7863, 311, 3254, 12279, 6459, 45478, 627, 256, 482, 21304, 690, 5569, 17715, 13790, 398, 449, 279, 1396, 315, 71503, 11, 26619, 279, 1646, 11815, 1646, 15638, 2191, 382, 18, 13, 3146, 33, 548, 19757, 6296, 13, 18270, 334, 512, 256, 482, 61892, 89932, 84823, 649, 11322, 3146, 1135, 12, 1041, 10030, 10819, 824, 4037, 9659, 334, 7863, 311, 18270, 84823, 369, 3544, 990, 33785, 382, 45464, 14711, 3146, 68744, 811, 57277, 12, 3146, 33, 548, 19757, 96618, 27191, 398, 11349, 369, 279, 1888, 5178, 627, 12, 3146, 37426, 4026, 96618, 19078, 433, 422, 499, 3207, 1701, 7447, 12279, 6459, 84823, 627, 12, 3146, 4559, 9988, 46837, 96618, 19796, 1124, 422, 499, 3207, 4401, 7447, 12279, 6459, 990, 33785, 449, 8987, 393, 17, 47, 828, 8481, 627, 12, 3146, 1959, 21321, 287, 96618, 26891, 389, 23501, 7916, 51437, 4856, 1109, 5044, 81294, 287, 627, 12, 3146, 1600, 71730, 18270, 96618, 1442, 499, 2011, 1005, 4200, 2065, 11, 6106, 23501, 6502, 88001, 374, 7373, 34440, 323, 1296, 449, 2678, 4211, 1603, 28041, 709, 382, 1383, 74142, 1521, 9547, 11, 499, 1288, 387, 3025, 311, 11322, 9250, 45478, 25753, 369, 701, 15490, 1729, 7227, 323, 1398, 4178, 39200, 348, 18, 990, 33785, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\nOkay, so I’m trying to figure out what follow-up questions the user might have after the assistant provided a detailed response. The conversation was about the setup of their AI inference rig with 6x RTX 3090s, an Epyc 73F3, and so on.\n\nFirst, the user talked about modifiers like NVLink, modded drivers, overclocking memory, power limits, and whether to use bare metal or a VM. The assistant gave a breakdown of each component and modifier, explaining how they impact performance.\n\nSo, the user is likely Someone setting up a high-performance AI system, maybe a developer or an engineer working on machine learning models. They’re probably looking to optimize their setup for the best performance, so they’re interested in the specifics of each component and how they can tweak their setup for better results.\n\nLooking at the assistant’s answer, they covered a lot of ground. They talked about the hardware components, the impact of each modifier, and gave some quantitative estimates. Now, the user is probably goi' ... 'Another question could be about the best metrics to measure performance beyond just speed.\n\nI should make sure the questions are clear and specific, focusing on areas the user might naturally explore next after getting the initial detailed response.\n</think>\n\nThe follow-up questions are designed to delve deeper into the user\'s setup optimization, focusing on performance, scalability, and future considerations.\n\n```json\n{\n  "follow_ups": [\n    "What specific benchmarks can I expect when comparing NVLink-enabled setups with standard PCIe configurations?",\n    "Are there any particular optimizations you recommend for the TabbyAPI/ExLLAMAv3 backend to maximize performance?",\n    "Would migrating to a bare-metal setup provide significant enough improvements to justify the effort?",\n    "How can I troubleshoot common issues with the modded P2P drivers in a multi-GPU environment?",\n    "What are the key considerations for scaling this setup to handle significantly larger models or more GPUs in the future?"\n  ]\n}\n```', 'output_ids': [524, 9884, 20389, 29, 128012, 128013, 198, 33413, 11, 779, 358, 4344, 4560, 311, 7216, 704, 1148, 1833, 5352, 4860, 279, 1217, 2643, 617, 1306, 279, 18328, 3984, 264, 11944, 2077, 13, 578, 10652, 574, 922, 279, 6642, 315, 872, 15592, 45478, 13552, 449, 220, 21, 87, 10860, 55, 220, 15500, 15, 82, 11, 459, 469, 3368, 66, 220, 5958, 37, 18, 11, 323, 779, 389, 382, 5451, 11, 279, 1217, 15243, 922, 36770, 1093, 25464, 4026, 11, 1491, 9988, 12050, 11, 81294, 287, 5044, 11, 2410, 13693, 11, 323, 3508, 311, 1005, 12743, 9501, 477, 264, 18270, 13, 578, 18328, 6688, 264, 31085, 315, 1855, 3777, 323, 23575, 11, 26073, 1268, 814, 5536, 5178, 382, 4516, 11, 279, 1217, 374, 4461, 35272, 6376, 709, 264, 1579, 58574, 15592, 1887, 11, 7344, 264, 16131, 477, 459, 24490, 3318, 389, 5780, 6975, 4211, 13, 2435, 3207, 4762, 3411, 311, 30536, 872, 6642, 369, 279, 1888, 5178, 11, 779, 814, 3207, 8173, 304, 279, 49449, 315, 1855, 3777, 323, 1268, 814, 649, 52143, 872, 6642, 369, 2731, 3135, 382, 23274, 520, 279, 18328, 753, 4320, 11, 814, 9960, 264, 2763, 315, 5015, 13, 2435, 15243, 922, 279, 12035, 6956, 11, 279, 5536, 315, 1855, 23575, 11, 323, 6688, 1063, 47616, 17989, 13, 4800, 11, 279, 1217, 374, 4762, 2133, 311, 1390, 311, 30963, 19662, 1139, 3738, 13878, 477, 13488, 1023, 5552, 5789, 382, 96711, 5789, 369, 1833, 5352, 1436, 2997, 1473, 16, 13, 3146, 33, 20345, 15914, 96618, 578, 1217, 2643, 1390, 3230, 5219, 477, 828, 27393, 2204, 84823, 13, 2435, 4691, 922, 45478, 4732, 11, 779, 814, 7070, 4461, 1390, 1972, 31184, 63119, 382, 17, 13, 3146, 22078, 67064, 26788, 96618, 8876, 814, 3207, 1701, 264, 3230, 19713, 320, 8750, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 705, 814, 2643, 1390, 810, 41891, 9650, 389, 74142, 872, 6642, 7953, 1120, 279, 12035, 11, 7344, 3241, 63437, 382, 18, 13, 3146, 15289, 7826, 89729, 18825, 96618, 10926, 814, 3207, 13126, 3508, 3738, 32714, 527, 5922, 279, 9341, 13, 1789, 3187, 11, 374, 7366, 311, 12743, 9501, 5922, 433, 7863, 311, 872, 1510, 18270, 6642, 382, 19, 13, 3146, 3407, 278, 2968, 96618, 2435, 617, 220, 21, 71503, 11, 779, 814, 2643, 387, 8173, 304, 1268, 1664, 872, 6642, 649, 5569, 449, 810, 71503, 477, 8294, 4211, 382, 20, 13, 3146, 91635, 65, 51340, 96618, 2435, 2643, 1629, 1139, 4819, 449, 872, 1510, 6642, 11, 1093, 29487, 81072, 2785, 477, 5696, 5435, 11, 779, 814, 2643, 2610, 922, 1268, 311, 58681, 477, 5155, 1884, 382, 21, 13, 3146, 34587, 449, 7089, 37865, 96618, 2435, 1436, 387, 22999, 1268, 872, 6642, 41050, 709, 2403, 28650, 477, 1023, 33483, 11, 1093, 1701, 2204, 71503, 477, 66575, 382, 22, 13, 3146, 25184, 3216, 23142, 96618, 2435, 2643, 387, 7422, 922, 3938, 60267, 287, 872, 6642, 11, 779, 814, 2643, 2610, 922, 25780, 449, 14827, 12035, 477, 3241, 83787, 382, 23, 13, 3146, 48614, 5008, 21304, 96618, 8876, 814, 9932, 1398, 4178, 39200, 348, 18, 11, 814, 1436, 387, 8173, 304, 1268, 872, 6642, 13777, 430, 3230, 1646, 477, 1023, 4528, 4211, 382, 24, 13, 3146, 57850, 287, 323, 66726, 9744, 96618, 3161, 1579, 13368, 12035, 11, 29487, 4819, 649, 387, 264, 88938, 11, 779, 814, 2643, 2610, 922, 28015, 10105, 477, 1268, 29487, 6373, 22223, 5178, 382, 605, 13, 3146, 47688, 10603, 55290, 96618, 31886, 4732, 11, 814, 2643, 387, 8173, 304, 15374, 11, 1093, 5178, 824, 67272, 477, 2853, 23937, 13071, 382, 22818, 682, 420, 11, 279, 18328, 753, 1833, 5352, 4860, 1288, 3504, 1521, 5789, 2085, 40916, 1148, 596, 2736, 1027, 14407, 13, 2435, 1205, 311, 387, 64694, 323, 6089, 5552, 311, 279, 10652, 382, 4516, 11, 3284, 1833, 5352, 4860, 1436, 2997, 10371, 369, 3230, 29531, 36595, 11, 26329, 10631, 369, 872, 19713, 11, 3508, 7366, 311, 12743, 9501, 374, 5922, 279, 5149, 11, 1268, 311, 3790, 5696, 4819, 449, 7447, 12279, 6459, 84823, 11, 477, 3938, 14234, 13006, 13, 13596, 3488, 1436, 387, 922, 279, 1888, 17150, 311, 6767, 5178, 7953, 1120, 4732, 382, 40, 1288, 1304, 2771, 279, 4860, 527, 2867, 323, 3230, 11, 21760, 389, 5789, 279, 1217, 2643, 18182, 13488, 1828, 1306, 3794, 279, 2926, 11944, 2077, 627, 128014, 271, 791, 1833, 5352, 4860, 527, 6319, 311, 82845, 19662, 1139, 279, 1217, 596, 6642, 26329, 11, 21760, 389, 5178, 11, 94840, 11, 323, 3938, 38864, 382, 74694, 2285, 198, 517, 220, 330, 19070, 8401, 82, 794, 2330, 262, 330, 3923, 3230, 63119, 649, 358, 1755, 994, 27393, 25464, 4026, 55292, 84823, 449, 5410, 91790, 33483, 36818, 262, 330, 11787, 1070, 904, 4040, 82278, 499, 7079, 369, 279, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 311, 35608, 5178, 36818, 262, 330, 29089, 85626, 311, 264, 12743, 89932, 6642, 3493, 5199, 3403, 18637, 311, 9541, 279, 5149, 36818, 262, 330, 4438, 649, 358, 20498, 97508, 4279, 4819, 449, 279, 1491, 9988, 393, 17, 47, 12050, 304, 264, 7447, 12279, 6459, 4676, 36818, 262, 330, 3923, 527, 279, 1401, 38864, 369, 28041, 420, 6642, 311, 3790, 12207, 8294, 4211, 477, 810, 71503, 304, 279, 3938, 48469, 220, 5243, 534, 74694, 128001], 'meta_info': {'id': '4f871084e86b4e889ea83e10beada683', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 2328, 'weight_version': 'default', 'completion_tokens': 848, 'cached_tokens': 1, 'e2e_latency': 19.07525897026062}}
[2025-10-13 02:06:30] INFO:     127.0.0.1:60382 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 02:32:26] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 02:32:26] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 02:32:28] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:32:38] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=738629150, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:32:39] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:32:47 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:32:47 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:32:48 TP0] sglang is using nccl==2.27.3
[2025-10-13 02:32:48 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:32:48 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:32:48 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:32:48 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:32:48 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:32:50 TP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:32:50 TP0] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:32:50 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:32:50 TP1] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:32:50] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:32:50 TP3] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:32:50 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:32:50] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:32:50 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:32:50] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 1853870 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:33:08] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=769047677, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:33:09] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:33:16 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:33:16 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:33:17 TP0] sglang is using nccl==2.27.3
[2025-10-13 02:33:18 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:33:18 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:33:18 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:33:18 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:33:18 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:33:19 TP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:33:19 TP3] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:19 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:19] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:33:19 TP0] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:19 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:19] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:33:19 TP2] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:19 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:19] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:33:19 TP1] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:19 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:19] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 1855216 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:33:37] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=1054314562, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:33:38] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:33:46 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:33:46 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:33:47 TP0] sglang is using nccl==2.27.3
[2025-10-13 02:33:48 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:33:48 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:33:48 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:33:48 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:33:48 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:33:49 TP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:33:49 TP3] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:49 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:49] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:33:49 TP1] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:49 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:49] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:33:49 TP0] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:49 TP2] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:33:49 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:49] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:33:49 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:33:49] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 1856524 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:34:07] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=933129341, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:34:07] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:34:14 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:34:14 TP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:34:16 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:34:17 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:34:17 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:34:17 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:34:17 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:34:17 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:34:18 TP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:34:18 TP2] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:34:18 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:34:18 TP0] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:34:18] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:34:18 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:34:18] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:34:18 TP1] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:34:18 TP3] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:34:18 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:34:18] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:34:18 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:34:18] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 1857840 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:34:36] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=583895597, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:34:36] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:34:44 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:34:44 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:34:45 TP0] sglang is using nccl==2.27.3
[2025-10-13 02:34:46 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:34:46 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:34:46 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:34:46 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:34:46 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:34:47 TP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:34:47 TP3] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:34:47 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:34:47] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:34:47 TP1] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:34:47 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:34:47] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 1859163 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:35:06] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=122633297, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:35:07] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:35:15 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:35:15 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:35:16 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:35:16 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:35:16 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:35:16 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:35:16 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:35:16 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:35:17 TP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:35:17 TP0] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:35:17 TP2] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:35:17 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:35:17] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:35:17 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:35:17] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:35:17 TP1] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:35:17 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:35:17] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:35:17 TP3] Layer 'model.layers.1.mlp.experts' is not supported by AWQMoeMarlin. Falling back to Moe WNA16 kernels.
[2025-10-13 02:35:17 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 576, in load_model
    model = _initialize_model(
            ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 252, in _initialize_model
    return model_class(
           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 753, in __init__
    self.model = Glm4MoeModel(
                 ^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 722, in __init__
    Glm4MoeDecoderLayer(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 637, in __init__
    self.mlp = Glm4MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 432, in __init__
    self.experts = get_moe_impl_class(quant_config)(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py", line 211, in __init__
    self.quant_method = quant_config.get_quant_method(self, prefix)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/awq.py", line 290, in get_quant_method
    return MoeWNA16Config.from_config(self.full_config).get_quant_method(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/layers/quantization/moe_wna16.py", line 146, in from_config
    raise ValueError("moe_wna16 only support gptq and awq.")
ValueError: moe_wna16 only support gptq and awq.

[2025-10-13 02:35:17] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 1860482 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:35:39] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=955384822, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:35:39] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:35:39] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:35:47 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:35:47 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:35:47 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:35:47 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:35:47 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:35:47 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:35:47 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:35:47 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:35:47 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:35:47 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:35:48 TP0] sglang is using nccl==2.27.3
[2025-10-13 02:35:49 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:35:49 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:35:49 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:35:49 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:35:49 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:35:49 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:35:49 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:35:49 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:35:49 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:35:49 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:01<00:14,  1.75s/it]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:02<00:10,  1.43s/it]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:05<00:10,  1.72s/it]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:06<00:08,  1.74s/it]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:07<00:05,  1.46s/it]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:09<00:04,  1.59s/it]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:11<00:03,  1.65s/it]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:13<00:01,  1.70s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:15<00:00,  1.75s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:15<00:00,  1.67s/it]

[2025-10-13 02:36:05 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 02:36:06 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 02:36:06 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:36:06 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:36:06 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:36:06 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 02:36:06 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 02:36:06 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 02:36:06 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:17,  1.37s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:17,  1.37s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.25it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.25it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:02<00:05,  1.84it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:02<00:05,  1.84it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.40it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.40it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.95it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.95it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.49it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.49it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:52<01:55, 16.46s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:52<01:55, 16.46s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:11<01:44, 17.34s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:11<01:44, 17.34s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:29<01:27, 17.56s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:29<01:27, 17.56s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [01:49<01:12, 18.19s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [01:49<01:12, 18.19s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [02:09<00:56, 18.92s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [02:09<00:56, 18.92s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [02:32<00:39, 19.99s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [02:32<00:39, 19.99s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [02:54<00:20, 20.72s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [02:54<00:20, 20.72s/it]/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
usage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,remote,remote_instance}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal] [--revision REVISION]
                        [--model-impl MODEL_IMPL] [--host HOST] [--port PORT]
                        [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3}]
                        [--enable-fp32-lm-head]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--oltp-traces-endpoint OLTP_TRACES_ENDPOINT]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,qwen3,qwen3-thinking,step3}]
                        [--tool-call-parser {deepseekv3,deepseekv31,glm,glm45,gpt-oss,kimi_k2,llama3,mistral,pythonic,qwen,qwen25,qwen3_coder,step3}]
                        [--sampling-defaults {openai,model}]
                        [--tool-server TOOL_SERVER]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-backend {triton,csgmv}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--sampling-backend {flashinfer,pytorch}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn}]
                        [--nsa-prefill {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--nsa-decode {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--radix-eviction-policy {lru,lfu}]
                        [--hicache-io-backend {direct,kernel}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE] [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-nccl-nvls] [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup] [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--debug-tensor-dump-prefill-only]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--enable-pdmux] [--sm-group-num SM_GROUP_NUM]
                        [--enable-deterministic-inference] [--enable-ep-moe]
                        [--enable-deepep-moe]
                        [--enable-flashinfer-cutlass-moe]
                        [--enable-flashinfer-cutedsl-moe]
                        [--enable-flashinfer-trtllm-moe]
                        [--enable-triton-kernel-moe]
                        [--enable-flashinfer-mxfp4-moe] [--config CONFIG]
launch_server.py: error: unrecognized arguments: --enable-expert-parallel
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
usage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,remote,remote_instance}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal] [--revision REVISION]
                        [--model-impl MODEL_IMPL] [--host HOST] [--port PORT]
                        [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3}]
                        [--enable-fp32-lm-head]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--oltp-traces-endpoint OLTP_TRACES_ENDPOINT]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,qwen3,qwen3-thinking,step3}]
                        [--tool-call-parser {deepseekv3,deepseekv31,glm,glm45,gpt-oss,kimi_k2,llama3,mistral,pythonic,qwen,qwen25,qwen3_coder,step3}]
                        [--sampling-defaults {openai,model}]
                        [--tool-server TOOL_SERVER]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-backend {triton,csgmv}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--sampling-backend {flashinfer,pytorch}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn}]
                        [--nsa-prefill {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--nsa-decode {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--radix-eviction-policy {lru,lfu}]
                        [--hicache-io-backend {direct,kernel}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE] [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-nccl-nvls] [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup] [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--debug-tensor-dump-prefill-only]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--enable-pdmux] [--sm-group-num SM_GROUP_NUM]
                        [--enable-deterministic-inference] [--enable-ep-moe]
                        [--enable-deepep-moe]
                        [--enable-flashinfer-cutlass-moe]
                        [--enable-flashinfer-cutedsl-moe]
                        [--enable-flashinfer-trtllm-moe]
                        [--enable-triton-kernel-moe]
                        [--enable-flashinfer-mxfp4-moe] [--config CONFIG]
launch_server.py: error: unrecognized arguments: --enable-expert-parallel
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
usage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,remote,remote_instance}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal] [--revision REVISION]
                        [--model-impl MODEL_IMPL] [--host HOST] [--port PORT]
                        [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3}]
                        [--enable-fp32-lm-head]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--oltp-traces-endpoint OLTP_TRACES_ENDPOINT]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,qwen3,qwen3-thinking,step3}]
                        [--tool-call-parser {deepseekv3,deepseekv31,glm,glm45,gpt-oss,kimi_k2,llama3,mistral,pythonic,qwen,qwen25,qwen3_coder,step3}]
                        [--sampling-defaults {openai,model}]
                        [--tool-server TOOL_SERVER]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-backend {triton,csgmv}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--sampling-backend {flashinfer,pytorch}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn}]
                        [--nsa-prefill {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--nsa-decode {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--radix-eviction-policy {lru,lfu}]
                        [--hicache-io-backend {direct,kernel}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE] [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-nccl-nvls] [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup] [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--debug-tensor-dump-prefill-only]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--enable-pdmux] [--sm-group-num SM_GROUP_NUM]
                        [--enable-deterministic-inference] [--enable-ep-moe]
                        [--enable-deepep-moe]
                        [--enable-flashinfer-cutlass-moe]
                        [--enable-flashinfer-cutedsl-moe]
                        [--enable-flashinfer-trtllm-moe]
                        [--enable-triton-kernel-moe]
                        [--enable-flashinfer-mxfp4-moe] [--config CONFIG]
launch_server.py: error: unrecognized arguments: --enable-expert-parallel
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
usage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,remote,remote_instance}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal] [--revision REVISION]
                        [--model-impl MODEL_IMPL] [--host HOST] [--port PORT]
                        [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3}]
                        [--enable-fp32-lm-head]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--oltp-traces-endpoint OLTP_TRACES_ENDPOINT]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,qwen3,qwen3-thinking,step3}]
                        [--tool-call-parser {deepseekv3,deepseekv31,glm,glm45,gpt-oss,kimi_k2,llama3,mistral,pythonic,qwen,qwen25,qwen3_coder,step3}]
                        [--sampling-defaults {openai,model}]
                        [--tool-server TOOL_SERVER]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-backend {triton,csgmv}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend}]
                        [--sampling-backend {flashinfer,pytorch}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn}]
                        [--nsa-prefill {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--nsa-decode {flashmla_prefill,flashmla_decode,fa3,tilelang,aiter}]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--radix-eviction-policy {lru,lfu}]
                        [--hicache-io-backend {direct,kernel}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE] [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-nccl-nvls] [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup] [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--debug-tensor-dump-prefill-only]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--enable-pdmux] [--sm-group-num SM_GROUP_NUM]
                        [--enable-deterministic-inference] [--enable-ep-moe]
                        [--enable-deepep-moe]
                        [--enable-flashinfer-cutlass-moe]
                        [--enable-flashinfer-cutedsl-moe]
                        [--enable-flashinfer-trtllm-moe]
                        [--enable-triton-kernel-moe]
                        [--enable-flashinfer-mxfp4-moe] [--config CONFIG]
launch_server.py: error: unrecognized arguments: --enable-expert-parallel
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 24: -on-idle: command not found
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3244, in prepare_server_args
    raw_args = parser.parse_args(argv)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1908, in parse_args
    args, argv = self.parse_known_args(args, namespace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1941, in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2162, in _parse_known_args
    start_index = consume_optional(start_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2102, in consume_optional
    take_action(action, args, option_string)
  File "/usr/lib/python3.12/argparse.py", line 2017, in take_action
    action(self, namespace, argument_values, option_string)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3361, in __call__
    raise ValueError(self.help)
ValueError: NOTE: --enable-ep-moe is deprecated. Please set `--ep-size` to the same value as `--tp-size` instead.
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3244, in prepare_server_args
    raw_args = parser.parse_args(argv)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1908, in parse_args
    args, argv = self.parse_known_args(args, namespace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1941, in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2162, in _parse_known_args
    start_index = consume_optional(start_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2102, in consume_optional
    take_action(action, args, option_string)
  File "/usr/lib/python3.12/argparse.py", line 2017, in take_action
    action(self, namespace, argument_values, option_string)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3361, in __call__
    raise ValueError(self.help)
ValueError: NOTE: --enable-ep-moe is deprecated. Please set `--ep-size` to the same value as `--tp-size` instead.
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3244, in prepare_server_args
    raw_args = parser.parse_args(argv)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1908, in parse_args
    args, argv = self.parse_known_args(args, namespace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1941, in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2162, in _parse_known_args
    start_index = consume_optional(start_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2102, in consume_optional
    take_action(action, args, option_string)
  File "/usr/lib/python3.12/argparse.py", line 2017, in take_action
    action(self, namespace, argument_values, option_string)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3361, in __call__
    raise ValueError(self.help)
ValueError: NOTE: --enable-ep-moe is deprecated. Please set `--ep-size` to the same value as `--tp-size` instead.
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3244, in prepare_server_args
    raw_args = parser.parse_args(argv)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1908, in parse_args
    args, argv = self.parse_known_args(args, namespace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1941, in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2162, in _parse_known_args
    start_index = consume_optional(start_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2102, in consume_optional
    take_action(action, args, option_string)
  File "/usr/lib/python3.12/argparse.py", line 2017, in take_action
    action(self, namespace, argument_values, option_string)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3361, in __call__
    raise ValueError(self.help)
ValueError: NOTE: --enable-ep-moe is deprecated. Please set `--ep-size` to the same value as `--tp-size` instead.
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3244, in prepare_server_args
    raw_args = parser.parse_args(argv)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1908, in parse_args
    args, argv = self.parse_known_args(args, namespace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1941, in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2162, in _parse_known_args
    start_index = consume_optional(start_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2102, in consume_optional
    take_action(action, args, option_string)
  File "/usr/lib/python3.12/argparse.py", line 2017, in take_action
    action(self, namespace, argument_values, option_string)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3361, in __call__
    raise ValueError(self.help)
ValueError: NOTE: --enable-ep-moe is deprecated. Please set `--ep-size` to the same value as `--tp-size` instead.
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3244, in prepare_server_args
    raw_args = parser.parse_args(argv)
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1908, in parse_args
    args, argv = self.parse_known_args(args, namespace)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 1941, in parse_known_args
    namespace, args = self._parse_known_args(args, namespace)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2162, in _parse_known_args
    start_index = consume_optional(start_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/argparse.py", line 2102, in consume_optional
    take_action(action, args, option_string)
  File "/usr/lib/python3.12/argparse.py", line 2017, in take_action
    action(self, namespace, argument_values, option_string)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3361, in __call__
    raise ValueError(self.help)
ValueError: NOTE: --enable-ep-moe is deprecated. Please set `--ep-size` to the same value as `--tp-size` instead.
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:42:40] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=967956617, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:42:40] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:42:48 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:42:48 TP0 EP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:42:49 TP0 EP0] sglang is using nccl==2.27.3
[2025-10-13 02:42:49 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:42:49 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:42:49 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:42:49 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:42:49 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:42:51 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:42:51 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:06<01:25,  6.09s/it]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:11<01:16,  5.85s/it]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:17<01:11,  5.94s/it]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:24<01:06,  6.05s/it]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:30<01:00,  6.10s/it]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:36<00:54,  6.06s/it]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:42<00:48,  6.00s/it]
[2025-10-13 02:43:35 TP3 EP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:43:35] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 1986782 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:43:56] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=952471125, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:43:56] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:44:04 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:44:04 TP0 EP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:44:05 TP0 EP0] sglang is using nccl==2.27.3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:44:05 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:05 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:05 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:05 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:44:05 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:44:13] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=313538263, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:44:14] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:44:22 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:44:22 TP0 EP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:44:23 TP0 EP0] sglang is using nccl==2.27.3
[2025-10-13 02:44:23 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:23 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:23 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:23 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:44:23 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:44:24 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:44:24 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:00<00:06,  2.15it/s]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:00<00:06,  1.99it/s]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:01<00:06,  1.94it/s]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:02<00:05,  1.91it/s]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:02<00:05,  1.90it/s]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:03<00:04,  1.90it/s]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.89it/s]
[2025-10-13 02:44:28 TP3 EP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:44:28] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2016613 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:44:46] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=239875844, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:44:46] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:44:54 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:44:54 TP0 EP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:44:55 TP0 EP0] sglang is using nccl==2.27.3
[2025-10-13 02:44:56 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:56 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:56 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:44:56 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:44:56 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:44:57 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:44:57 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:00<00:06,  2.10it/s]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:00<00:06,  2.00it/s]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:01<00:06,  1.94it/s]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:02<00:05,  1.90it/s]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:02<00:05,  1.89it/s]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:03<00:04,  1.88it/s]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.86it/s]
[2025-10-13 02:45:01 TP0 EP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:45:01] Received sigquit from a child process. It usually means the child failed.
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.79it/s]

/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2027320 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:45:19] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=48248783, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:45:20] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:45:28 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:45:28 TP0 EP0] Init torch distributed begin.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:45:29 TP0 EP0] sglang is using nccl==2.27.3
[2025-10-13 02:45:29 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:45:29 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:45:29 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:45:29 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:45:29 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:45:30 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:45:30 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:00<00:06,  2.08it/s]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:01<00:06,  1.98it/s]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:01<00:06,  1.92it/s]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:02<00:05,  1.90it/s]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:02<00:05,  1.88it/s]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:03<00:04,  1.88it/s]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.86it/s]
[2025-10-13 02:45:34 TP3 EP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:45:34] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2037538 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:45:52] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=256079453, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:45:52] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:46:00 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:46:00 TP0 EP0] Init torch distributed begin.
`torch_dtype` is deprecated! Use `dtype` instead!
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:46:01 TP0 EP0] sglang is using nccl==2.27.3
[2025-10-13 02:46:02 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:46:02 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:46:02 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:46:02 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:46:02 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:46:03 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:46:03 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:00<00:06,  2.11it/s]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:00<00:06,  1.99it/s]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:01<00:06,  1.94it/s]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:02<00:05,  1.91it/s]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:02<00:05,  1.89it/s]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:03<00:04,  1.86it/s]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.85it/s]
[2025-10-13 02:46:07 TP2 EP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:46:07] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2047802 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:46:25] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=629549643, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:46:25] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:46:33 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:46:33 TP0 EP0] Init torch distributed begin.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:46:35 TP0 EP0] sglang is using nccl==2.27.3
[2025-10-13 02:46:35 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:46:35 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:46:35 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:46:35 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:46:35 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:46:36 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:46:36 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:00<00:06,  2.13it/s]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:00<00:06,  2.00it/s]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:01<00:06,  1.94it/s]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:02<00:05,  1.90it/s]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:02<00:05,  1.89it/s]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:03<00:04,  1.88it/s]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.87it/s]
[2025-10-13 02:46:40 TP3 EP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:46:40] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:46:40 TP2 EP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:46:40] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2058157 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:46:58] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=651643592, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:46:58] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:47:06 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:47:06 TP0 EP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:47:07 TP0 EP0] sglang is using nccl==2.27.3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:47:08 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:47:08 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:47:08 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:47:08 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:47:08 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:47:09 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:47:09 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:00<00:06,  2.10it/s]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:00<00:06,  1.99it/s]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:01<00:06,  1.93it/s]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:02<00:05,  1.90it/s]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:02<00:05,  1.87it/s]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:03<00:04,  1.87it/s]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.86it/s]
[2025-10-13 02:47:13 TP2 EP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:47:13] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2068484 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:47:31] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=885700006, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:47:31] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:47:39 TP0 EP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:47:39 TP0 EP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:47:40 TP0 EP0] sglang is using nccl==2.27.3
[2025-10-13 02:47:40 TP2 EP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:47:40 TP1 EP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:47:40 TP3 EP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:47:40 TP0 EP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:47:40 TP0 EP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:47:41 TP0 EP0] Load weight begin. avail mem=23.09 GB
[2025-10-13 02:47:41 TP0 EP0] Deepseek and GLM-4.5 or GLM-4.6 can not use shared experts fusion optimization under expert parallelism. Shared experts fusion optimization is disabled.
Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:00<00:06,  2.14it/s]
Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:00<00:06,  2.02it/s]
Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:01<00:06,  1.96it/s]
Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:02<00:05,  1.92it/s]
Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:02<00:05,  1.91it/s]
Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:03<00:04,  1.90it/s]
Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:03<00:04,  1.89it/s]
[2025-10-13 02:47:45 TP2 EP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 346, in initialize
    self.load_model()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 792, in load_model
    self.model = get_model(
                 ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/__init__.py", line 28, in get_model
    return loader.load_model(
           ^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 581, in load_model
    self.load_weights_and_postprocess(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_loader/loader.py", line 589, in load_weights_and_postprocess
    model.load_weights(weights)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/models/glm4_moe.py", line 1011, in load_weights
    param = params_dict[name]
            ~~~~~~~~~~~^^^^^^
KeyError: 'model.layers.45.mlp.experts.w2_weight'

[2025-10-13 02:47:45] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2072981 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:48:03] server_args=ServerArgs(model_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_path='/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=497623353, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='glm-4.5-air-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=4, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:48:03] Using default HuggingFace chat template with detected content format: openai
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:48:11 TP2 EP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 423, in __init__
    self.init_tokenizer()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 720, in init_tokenizer
    self.tokenizer = get_tokenizer(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 354, in get_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 1159, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2097, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2343, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py", line 117, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception: No such file or directory (os error 2)

[2025-10-13 02:48:11] Received sigquit from a child process. It usually means the child failed.
[2025-10-13 02:48:11 TP3 EP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 423, in __init__
    self.init_tokenizer()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 720, in init_tokenizer
    self.tokenizer = get_tokenizer(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 354, in get_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py", line 1159, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2097, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py", line 2343, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py", line 117, in __init__
    fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception: No such file or directory (os error 2)

[2025-10-13 02:48:11] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_glm_tp4_compiled.sh: line 23: 2074330 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix --host 0.0.0.0 --port 8001 --tp 4 --ep-size 4 --context-length 24576 --served-model-name glm-4.5-air-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 02:54:17] server_args=ServerArgs(model_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_path='/home/ivan/models/Mistral-Large-Instruct-2411-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=679675702, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='mistral-large-2411-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:54:17] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:54:17] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:54:25 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:54:25 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:54:25 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:54:25 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 02:54:25 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:54:25 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:54:25 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 02:54:25 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 02:54:25 TP0] Init torch distributed begin.
[2025-10-13 02:54:25 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:54:26 TP0] sglang is using nccl==2.27.3
[2025-10-13 02:54:26 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:54:26 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:54:26 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 02:54:26 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 02:54:26 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 02:54:26 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:54:26 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:54:26 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:54:26 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 02:54:27 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:00<00:08,  1.50it/s]
Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:01<00:08,  1.39it/s]
Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:02<00:08,  1.37it/s]
Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:02<00:07,  1.42it/s]
Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:03<00:06,  1.43it/s]
Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:04<00:05,  1.40it/s]
Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:04<00:04,  1.41it/s]
Loading safetensors checkpoint shards:  57% Completed | 8/14 [00:05<00:03,  1.82it/s]
Loading safetensors checkpoint shards:  64% Completed | 9/14 [00:05<00:02,  1.71it/s]
Loading safetensors checkpoint shards:  71% Completed | 10/14 [00:06<00:02,  1.59it/s]
Loading safetensors checkpoint shards:  79% Completed | 11/14 [00:07<00:01,  1.52it/s]
Loading safetensors checkpoint shards:  86% Completed | 12/14 [00:08<00:01,  1.46it/s]
Loading safetensors checkpoint shards:  93% Completed | 13/14 [00:08<00:00,  1.41it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.39it/s]
Loading safetensors checkpoint shards: 100% Completed | 14/14 [00:09<00:00,  1.47it/s]

[2025-10-13 02:54:38 TP0] Load weight end. type=MistralForCausalLM, dtype=torch.float16, avail mem=7.79 GB, mem usage=15.32 GB.
[2025-10-13 02:54:38 TP0] Using KV cache dtype: torch.float16
[2025-10-13 02:54:38 TP0] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 02:54:38 TP0] Memory pool end. avail mem=4.20 GB
[2025-10-13 02:54:38 TP2] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 02:54:38 TP3] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 02:54:38 TP1] KV Cache is allocated. #tokens: 37697, K size: 1.58 GB, V size: 1.58 GB
[2025-10-13 02:54:38 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.69 GB
[2025-10-13 02:54:38 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.68 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.68 GB):   7%|▋         | 1/14 [00:01<00:14,  1.14s/it]Capturing batches (bs=72 avail_mem=3.61 GB):   7%|▋         | 1/14 [00:01<00:14,  1.14s/it]Capturing batches (bs=72 avail_mem=3.61 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.12it/s]Capturing batches (bs=64 avail_mem=3.59 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.12it/s]Capturing batches (bs=64 avail_mem=3.59 GB):  21%|██▏       | 3/14 [00:02<00:06,  1.58it/s]Capturing batches (bs=56 avail_mem=3.58 GB):  21%|██▏       | 3/14 [00:02<00:06,  1.58it/s]Capturing batches (bs=56 avail_mem=3.58 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.98it/s]Capturing batches (bs=48 avail_mem=3.56 GB):  29%|██▊       | 4/14 [00:02<00:05,  1.98it/s]Capturing batches (bs=48 avail_mem=3.56 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.39it/s]Capturing batches (bs=40 avail_mem=3.55 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.39it/s]Capturing batches (bs=40 avail_mem=3.55 GB):  43%|████▎     | 6/14 [00:03<00:02,  2.76it/s]Capturing batches (bs=32 avail_mem=3.51 GB):  43%|████▎     | 6/14 [00:03<00:02,  2.76it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.51 GB):  50%|█████     | 7/14 [00:22<00:46,  6.67s/it]Capturing batches (bs=24 avail_mem=3.50 GB):  50%|█████     | 7/14 [00:22<00:46,  6.67s/it]Capturing batches (bs=24 avail_mem=3.50 GB):  57%|█████▋    | 8/14 [00:45<01:11, 11.94s/it]Capturing batches (bs=16 avail_mem=3.47 GB):  57%|█████▋    | 8/14 [00:45<01:11, 11.94s/it]Capturing batches (bs=16 avail_mem=3.47 GB):  64%|██████▍   | 9/14 [01:07<01:15, 15.03s/it]Capturing batches (bs=12 avail_mem=3.45 GB):  64%|██████▍   | 9/14 [01:07<01:15, 15.03s/it]Capturing batches (bs=12 avail_mem=3.45 GB):  71%|███████▏  | 10/14 [01:31<01:10, 17.63s/it]Capturing batches (bs=8 avail_mem=3.42 GB):  71%|███████▏  | 10/14 [01:31<01:10, 17.63s/it] Capturing batches (bs=8 avail_mem=3.42 GB):  79%|███████▊  | 11/14 [01:58<01:01, 20.60s/it]Capturing batches (bs=4 avail_mem=3.40 GB):  79%|███████▊  | 11/14 [01:58<01:01, 20.60s/it]Capturing batches (bs=4 avail_mem=3.40 GB):  86%|████████▌ | 12/14 [02:27<00:46, 23.08s/it]Capturing batches (bs=2 avail_mem=3.37 GB):  86%|████████▌ | 12/14 [02:27<00:46, 23.08s/it]Capturing batches (bs=2 avail_mem=3.37 GB):  93%|█████████▎| 13/14 [02:56<00:24, 24.92s/it]Capturing batches (bs=1 avail_mem=3.36 GB):  93%|█████████▎| 13/14 [02:56<00:24, 24.92s/it]Capturing batches (bs=1 avail_mem=3.36 GB): 100%|██████████| 14/14 [03:26<00:00, 26.39s/it]Capturing batches (bs=1 avail_mem=3.36 GB): 100%|██████████| 14/14 [03:26<00:00, 14.73s/it]
[2025-10-13 02:58:05 TP0] Capture cuda graph end. Time elapsed: 207.47 s. mem usage=0.37 GB. avail mem=3.33 GB.
[2025-10-13 02:58:06 TP0] max_total_num_tokens=37697, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2048, context_len=24576, available_gpu_mem=3.33 GB
[2025-10-13 02:58:06] INFO:     Started server process [2136738]
[2025-10-13 02:58:06] INFO:     Waiting for application startup.
[2025-10-13 02:58:06] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 02:58:06] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 02:58:06] INFO:     Application startup complete.
[2025-10-13 02:58:06] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 02:58:07] INFO:     127.0.0.1:37320 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 02:58:07] Receive: obj=GenerateReqInput(rid='f15851cc7a774a4dadbdccec4483727b', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 02:58:07 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:58:09] Finish: obj=GenerateReqInput(rid='f15851cc7a774a4dadbdccec4483727b', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is the most populous', 'output_ids': [6333, 3758, 1070, 5611, 1117, 6233, 29491, 1429, 1117, 1040, 1848, 2620, 10272], 'meta_info': {'id': 'f15851cc7a774a4dadbdccec4483727b', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.452772617340088}}
[2025-10-13 02:58:09] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 02:58:09] The server is fired up and ready to roll!
[2025-10-13 02:58:11] INFO:     127.0.0.1:35620 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 02:58:11] INFO:     127.0.0.1:35624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 02:58:11] Receive: obj=GenerateReqInput(rid='228ed2fec59a4f0a851dee8606f828f8', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 02:58:11 TP0] Prefill batch. #new-seq: 1, #new-token: 1001, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:58:14 TP0] Decode batch. #running-req: 1, #token: 1035, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4.93, #queue-req: 0, 
[2025-10-13 02:58:16 TP0] Decode batch. #running-req: 1, #token: 1075, token usage: 0.03, cuda graph: True, gen throughput (token/s): 33.11, #queue-req: 0, 
[2025-10-13 02:58:17 TP0] Decode batch. #running-req: 1, #token: 1115, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.78, #queue-req: 0, 
[2025-10-13 02:58:18 TP0] Decode batch. #running-req: 1, #token: 1155, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.69, #queue-req: 0, 
[2025-10-13 02:58:19 TP0] Decode batch. #running-req: 1, #token: 1195, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.75, #queue-req: 0, 
[2025-10-13 02:58:20 TP0] Decode batch. #running-req: 1, #token: 1235, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.83, #queue-req: 0, 
[2025-10-13 02:58:22 TP0] Decode batch. #running-req: 1, #token: 1275, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.73, #queue-req: 0, 
[2025-10-13 02:58:23 TP0] Decode batch. #running-req: 1, #token: 1315, token usage: 0.03, cuda graph: True, gen throughput (token/s): 32.69, #queue-req: 0, 
[2025-10-13 02:58:24 TP0] Decode batch. #running-req: 1, #token: 1355, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.75, #queue-req: 0, 
[2025-10-13 02:58:25 TP0] Decode batch. #running-req: 1, #token: 1395, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.71, #queue-req: 0, 
[2025-10-13 02:58:27 TP0] Decode batch. #running-req: 1, #token: 1435, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.66, #queue-req: 0, 
[2025-10-13 02:58:28 TP0] Decode batch. #running-req: 1, #token: 1475, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.62, #queue-req: 0, 
[2025-10-13 02:58:29 TP0] Decode batch. #running-req: 1, #token: 1515, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.67, #queue-req: 0, 
[2025-10-13 02:58:30 TP0] Decode batch. #running-req: 1, #token: 1555, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.65, #queue-req: 0, 
[2025-10-13 02:58:31 TP0] Decode batch. #running-req: 1, #token: 1595, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.59, #queue-req: 0, 
[2025-10-13 02:58:33 TP0] Decode batch. #running-req: 1, #token: 1635, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.63, #queue-req: 0, 
[2025-10-13 02:58:34 TP0] Decode batch. #running-req: 1, #token: 1675, token usage: 0.04, cuda graph: True, gen throughput (token/s): 32.59, #queue-req: 0, 
[2025-10-13 02:58:35 TP0] Decode batch. #running-req: 1, #token: 1715, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.55, #queue-req: 0, 
[2025-10-13 02:58:36 TP0] Decode batch. #running-req: 1, #token: 1755, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.54, #queue-req: 0, 
[2025-10-13 02:58:38 TP0] Decode batch. #running-req: 1, #token: 1795, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.52, #queue-req: 0, 
[2025-10-13 02:58:39 TP0] Decode batch. #running-req: 1, #token: 1835, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.57, #queue-req: 0, 
[2025-10-13 02:58:40 TP0] Decode batch. #running-req: 1, #token: 1875, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.59, #queue-req: 0, 
[2025-10-13 02:58:41 TP0] Decode batch. #running-req: 1, #token: 1915, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.50, #queue-req: 0, 
[2025-10-13 02:58:43 TP0] Decode batch. #running-req: 1, #token: 1955, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.51, #queue-req: 0, 
[2025-10-13 02:58:44 TP0] Decode batch. #running-req: 1, #token: 1995, token usage: 0.05, cuda graph: True, gen throughput (token/s): 32.52, #queue-req: 0, 
[2025-10-13 02:58:44] Finish: obj=GenerateReqInput(rid='228ed2fec59a4f0a851dee8606f828f8', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': " When evaluating the inference speed for a configuration involving multiple high-performance GPUs, a powerful CPU, and large amounts of RAM, there are several factors to consider. Let's break down the expected performance and the impact of various modifiers:\n\n### Base Configuration:\n- **6x NVIDIA RTX 3090 GPUs**: These are powerful consumer-grade GPUs with 24GB of VRAM each, capable of handling demanding AI workloads.\n- **AMD EPYC 73F3 CPU**: A 16-core, 32-thread server-grade CPU with a base clock speed of 3.7 GHz and a boost clock speed of 4.1 GHz. It's well-suited for managing multiple GPUs and handling CPU-bound tasks.\n- **512GB DDR4 RAM at 3200 MHz**: Plenty of memory to ensure that your workloads are not memory-bound.\n- **ESXi 8.0.3uG and Ubuntu 24.04 LTS VM**: Running the workload in a virtualized environment adds some overhead, but modern hypervisors like ESXi are quite efficient.\n\n### Backend and Frontend:\n- **TabbyAPI/ExLLAMAv3 backend**: This is likely optimized for performance on modern GPU hardwar" ... ' (with NVLink, modded drivers, overclocked memory, and increased power limits), you can expect very high inference throughput. However, exact numbers depend on the specific model and workload. For example, with a well-optimized inference pipeline, you might achieve hundreds to thousands of inferences per second on such a high-end setup.\n\n### Recommendations:\n- Use NVLink if possible to maximize communication speed between GPUs.\n- Consider using the modded direct GPU P2P drivers to enhance communication efficiency.\n- Overclocking memory can provide a slight boost but monitor for stability issues.\n- Adjust power limits to balance performance and power consumption.\n- If the virtualization overhead is significant, consider running on bare metal for maximum performance.\n\nIn summary, the configuration you described has the potential for very high inference speeds, especially with the right optimizations. Testing different configurations and monitoring real-world performance will give you the most accurate insights.', 'output_ids': [2], 'meta_info': {'id': '228ed2fec59a4f0a851dee8606f828f8', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 1002, 'weight_version': 'default', 'completion_tokens': 1006, 'cached_tokens': 1, 'e2e_latency': 33.16458797454834}}
[2025-10-13 02:58:44] Receive: obj=GenerateReqInput(rid='bb1146a6f24b4eaabcaf1c5d329cce5b', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456] ... [4287, 1070, 28919, 29582, 29478, 19571, 29572, 781, 5584, 8816, 13506, 29515, 2452, 7316, 1845, 1040, 1065, 2871, 5743, 1122, 1032, 8195, 15739, 5934, 2254, 29501, 1255, 12546, 20763, 6263, 29493, 1032, 7555, 15633, 29493, 1072, 3243, 15606, 1070, 28618, 29493, 1504, 1228, 3624, 9380, 1066, 2685, 29491, 3937, 29510, 29481, 2489, 1828, 1040, 4117, 5165, 1072, 1040, 5856, 1070, 4886, 1736, 12748, 29515, 781, 781, 28100, 8332, 20826, 29515, 781, 29501, 1387, 29552, 29512, 1186, 7568, 8176, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 20763, 6263, 9957, 3725, 1228, 7555, 12845, 29501, 8612, 20763, 6263, 1163, 29473, 29518, 29549, 7671, 1070, 1318, 29522, 3622, 2198, 29493, 11405, 1070, 13620, 22195, 16875, 1539, 29284, 29491, 781, 29501, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 1098, 29473, 29508, 29552, 29501, 3059, 29493, 29473, 29538, 29518, 29501, 6869, 5884, 29501, 8612, 15633, 1163, 1032, 3392, 10685, 5743, 1070, 29473, 29538, 29491, 29555, 1188, 13427, 1072, 1032, 11742, 10685, 5743, 1070, 29473, 29549, 29491, 29508, 1188, 13427, 29491, 1429, 29510, 29481, 1930, 29501, 3681, 2113, 1122, 16865, 5934, 20763, 6263, 1072, 13620, 15633, 29501, 8018, 10564, 29491, 781, 29501, 1387, 29550, 29508, 29518, 7671, 1152, 5000, 29549, 28618, 1206, 29473, 29538, 29518, 29502, 29502, 1119, 13427, 9957, 2409, 4511, 1070, 5501, 1066, 6175, 1137, 1342, 1539, 29284, 1228, 1227, 5501, 29501, 8018, 29491, 781, 29501, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 1072, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 29356, 1040, 1539, 2536, 1065, 1032, 9020, 2100, 5234, 14401, 1509, 21447, 29493, 1330, 5406, 12271, 3993, 1502, 1505, 28919, 29582, 29478, 1228, 4216, 9864, 29491, 781, 781, 28100, 7563, 1184, 1072, 14562, 1184, 29515, 781, 29501, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 9957, 1619, 1117, 4685, 7765, 2100, 1122, 5165, 1124, 5406, 29243, 13986, 29491, 781, 29501, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 16401, 2712, 29493, 1040, 3546, 1184, 1390, 1274, 1032, 14151, 5856, 1124, 1065, 2871, 5743, 7185, 1146, 7523, 1032, 8486, 29389, 1374, 1065, 13620, 2956, 10596, 29491, 781, 781, 28100, 20627, 12540, 1465, 29515, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 781, 1027, 1155, 1387, 3714, 28238, 5887, 9957, 28238, 5887, 6744, 2631, 2254, 29501, 4553, 3385, 9288, 2212, 20763, 6263, 29493, 17800, 4807, 2858, 1072, 17520, 13600, 29491, 1619, 1309, 1115, 21926, 1122, 5600, 4922, 1072, 1065, 2871, 29493, 1738, 1946, 3976, 1066, 1115, 7199, 5145, 2212, 20763, 6263, 29491, 781, 1027, 1155, 1387, 22237, 28238, 5887, 9957, 1167, 1491, 1056, 1124, 19622, 29474, 1073, 9550, 1122, 9288, 2212, 20763, 6263, 29493, 1458, 1761, 13865, 1509, 4807, 2858, 1072, 8411, 8312, 1827, 1527, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 781, 1027, 1155, 1387, 3714, 4581, 4570, 1152, 1141, 1508, 9957, 10384, 1040, 2631, 29243, 1135, 29518, 29521, 14464, 1245, 1040, 4625, 3830, 1309, 11885, 5684, 14437, 29501, 1300, 29501, 15488, 9288, 2212, 20763, 6263, 29493, 17800, 21447, 1072, 4807, 2858, 29491, 781, 1027, 1155, 1387, 22237, 4581, 4570, 1152, 1141, 1508, 9957, 1763, 1450, 15955, 1124, 5555, 14464, 29493, 1458, 2427, 1227, 1115, 1158, 7765, 2100, 1122, 29243, 29501, 1300, 29501, 26572, 9288, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 19921, 9957, 781, 1027, 1155, 1387, 3443, 1280, 7982, 19921, 9957, 6003, 11255, 1056, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 29493, 7079, 1281, 1342, 1539, 2536, 1117, 5501, 29501, 8018, 29491, 3761, 29493, 1040, 22490, 2427, 1115, 26054, 1072, 2335, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 5396, 16905, 5150, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 781, 1027, 1155, 1387, 24210, 1054, 9210, 11377, 1814, 9957, 25145, 4474, 1040, 2750, 4375, 1093, 29506, 11253, 29499, 1309, 2682, 1040, 20763, 6263, 1066, 2250, 1206, 5105, 28673, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 29491, 3761, 29493, 1224, 1390, 6015, 2750, 17066, 1072, 7369, 10560, 1276, 1120, 29491, 781, 1027, 1155, 1387, 4926, 9210, 11377, 1814, 9957, 29356, 1206, 3137, 2750, 12424, 1390, 6175, 16905, 1072, 17109, 1258, 29496, 1240, 1330, 1761, 4375, 1040, 7996, 5165, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8477, 29491, 28919, 29582, 29478, 19571, 9957, 781, 1027, 1155, 1387, 29528, 1260, 24912, 9957, 29356, 5858, 1124, 13802, 8271, 1390, 21313, 1040, 9020, 2605, 21447, 29493, 14725, 6142, 1066, 2641, 5165, 29491, 781, 1027, 1155, 1387, 3023, 29582, 29478, 19571, 9957, 29356, 1065, 1032, 19571, 14401, 1509, 21447, 1330, 6519, 7964, 1505, 26018, 29493, 7857, 6179, 29493, 1072, 23175, 29491, 14718, 12271, 3993, 1502, 1228, 4216, 9864, 29493, 1347, 1040, 5165, 4835, 2427, 1227, 1115, 1158, 6632, 1158, 1065, 1040, 3377, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 29515, 781, 29545, 5835, 1040, 3392, 8195, 1072, 17115, 16167, 7240, 1093, 4183, 28238, 5887, 29493, 1736, 4570, 14464, 29493, 1522, 1280, 7982, 5501, 29493, 1072, 8251, 2750, 12424, 1325, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 1827, 1527, 29491, 3761, 29493, 4227, 6319, 4057, 1124, 1040, 3716, 2997, 1072, 1539, 2536, 29491, 2031, 3525, 29493, 1163, 1032, 1930, 29501, 19193, 2100, 1065, 2871, 17660, 29493, 1136, 2427, 7387, 11292, 1066, 9345, 1070, 1065, 9599, 1428, 2444, 1124, 2027, 1032, 2254, 29501, 1184, 10288, 29491, 781, 781, 28100, 2066, 2573, 1184, 1465, 29515, 781, 29501, 6706, 28238, 5887, 1281, 3340, 1066, 16118, 1421, 9288, 5743, 2212, 20763, 6263, 29491, 781, 29501, 12540, 2181, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1066, 12744, 9288, 13600, 29491, 781, 29501, 6003, 11255, 1056, 5501, 1309, 3852, 1032, 6311, 11742, 1330, 9777, 1122, 16905, 5150, 29491, 781, 29501, 2732, 4790, 2750, 12424, 1066, 8641, 5165, 1072, 2750, 17066, 29491, 781, 29501, 1815, 1040, 9020, 2605, 21447, 1117, 6632, 29493, 2685, 4590, 1124, 13802, 8271, 1122, 7996, 5165, 29491, 781, 781, 1425, 14828, 29493, 1040, 8195, 1136, 6165, 1427, 1040, 5396, 1122, 1983, 2254, 1065, 2871, 28248, 29493, 4780, 1163, 1040, 1871, 7765, 14577, 29491, 4503, 1056, 2349, 25759, 1072, 17650, 2121, 29501, 10239, 5165, 1390, 2879, 1136, 1040, 1848, 11997, 21483, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 02:58:44 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:58:46 TP0] Prefill batch. #new-seq: 1, #new-token: 199, #cached-token: 0, token usage: 0.05, #running-req: 0, #queue-req: 0, 
[2025-10-13 02:58:50 TP0] Decode batch. #running-req: 1, #token: 2275, token usage: 0.06, cuda graph: True, gen throughput (token/s): 6.24, #queue-req: 0, 
[2025-10-13 02:58:51 TP0] Decode batch. #running-req: 1, #token: 2315, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.74, #queue-req: 0, 
[2025-10-13 02:58:53 TP0] Decode batch. #running-req: 1, #token: 2355, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.35, #queue-req: 0, 
[2025-10-13 02:58:54 TP0] Decode batch. #running-req: 1, #token: 2395, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.34, #queue-req: 0, 
[2025-10-13 02:58:54] Finish: obj=GenerateReqInput(rid='bb1146a6f24b4eaabcaf1c5d329cce5b', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 781, 5584, 8816, 13506, 29515, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456] ... [4287, 1070, 28919, 29582, 29478, 19571, 29572, 781, 5584, 8816, 13506, 29515, 2452, 7316, 1845, 1040, 1065, 2871, 5743, 1122, 1032, 8195, 15739, 5934, 2254, 29501, 1255, 12546, 20763, 6263, 29493, 1032, 7555, 15633, 29493, 1072, 3243, 15606, 1070, 28618, 29493, 1504, 1228, 3624, 9380, 1066, 2685, 29491, 3937, 29510, 29481, 2489, 1828, 1040, 4117, 5165, 1072, 1040, 5856, 1070, 4886, 1736, 12748, 29515, 781, 781, 28100, 8332, 20826, 29515, 781, 29501, 1387, 29552, 29512, 1186, 7568, 8176, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 20763, 6263, 9957, 3725, 1228, 7555, 12845, 29501, 8612, 20763, 6263, 1163, 29473, 29518, 29549, 7671, 1070, 1318, 29522, 3622, 2198, 29493, 11405, 1070, 13620, 22195, 16875, 1539, 29284, 29491, 781, 29501, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 1098, 29473, 29508, 29552, 29501, 3059, 29493, 29473, 29538, 29518, 29501, 6869, 5884, 29501, 8612, 15633, 1163, 1032, 3392, 10685, 5743, 1070, 29473, 29538, 29491, 29555, 1188, 13427, 1072, 1032, 11742, 10685, 5743, 1070, 29473, 29549, 29491, 29508, 1188, 13427, 29491, 1429, 29510, 29481, 1930, 29501, 3681, 2113, 1122, 16865, 5934, 20763, 6263, 1072, 13620, 15633, 29501, 8018, 10564, 29491, 781, 29501, 1387, 29550, 29508, 29518, 7671, 1152, 5000, 29549, 28618, 1206, 29473, 29538, 29518, 29502, 29502, 1119, 13427, 9957, 2409, 4511, 1070, 5501, 1066, 6175, 1137, 1342, 1539, 29284, 1228, 1227, 5501, 29501, 8018, 29491, 781, 29501, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 1072, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 29356, 1040, 1539, 2536, 1065, 1032, 9020, 2100, 5234, 14401, 1509, 21447, 29493, 1330, 5406, 12271, 3993, 1502, 1505, 28919, 29582, 29478, 1228, 4216, 9864, 29491, 781, 781, 28100, 7563, 1184, 1072, 14562, 1184, 29515, 781, 29501, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 9957, 1619, 1117, 4685, 7765, 2100, 1122, 5165, 1124, 5406, 29243, 13986, 29491, 781, 29501, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 16401, 2712, 29493, 1040, 3546, 1184, 1390, 1274, 1032, 14151, 5856, 1124, 1065, 2871, 5743, 7185, 1146, 7523, 1032, 8486, 29389, 1374, 1065, 13620, 2956, 10596, 29491, 781, 781, 28100, 20627, 12540, 1465, 29515, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 781, 1027, 1155, 1387, 3714, 28238, 5887, 9957, 28238, 5887, 6744, 2631, 2254, 29501, 4553, 3385, 9288, 2212, 20763, 6263, 29493, 17800, 4807, 2858, 1072, 17520, 13600, 29491, 1619, 1309, 1115, 21926, 1122, 5600, 4922, 1072, 1065, 2871, 29493, 1738, 1946, 3976, 1066, 1115, 7199, 5145, 2212, 20763, 6263, 29491, 781, 1027, 1155, 1387, 22237, 28238, 5887, 9957, 1167, 1491, 1056, 1124, 19622, 29474, 1073, 9550, 1122, 9288, 2212, 20763, 6263, 29493, 1458, 1761, 13865, 1509, 4807, 2858, 1072, 8411, 8312, 1827, 1527, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 781, 1027, 1155, 1387, 3714, 4581, 4570, 1152, 1141, 1508, 9957, 10384, 1040, 2631, 29243, 1135, 29518, 29521, 14464, 1245, 1040, 4625, 3830, 1309, 11885, 5684, 14437, 29501, 1300, 29501, 15488, 9288, 2212, 20763, 6263, 29493, 17800, 21447, 1072, 4807, 2858, 29491, 781, 1027, 1155, 1387, 22237, 4581, 4570, 1152, 1141, 1508, 9957, 1763, 1450, 15955, 1124, 5555, 14464, 29493, 1458, 2427, 1227, 1115, 1158, 7765, 2100, 1122, 29243, 29501, 1300, 29501, 26572, 9288, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 19921, 9957, 781, 1027, 1155, 1387, 3443, 1280, 7982, 19921, 9957, 6003, 11255, 1056, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 29493, 7079, 1281, 1342, 1539, 2536, 1117, 5501, 29501, 8018, 29491, 3761, 29493, 1040, 22490, 2427, 1115, 26054, 1072, 2335, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 5396, 16905, 5150, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 781, 1027, 1155, 1387, 24210, 1054, 9210, 11377, 1814, 9957, 25145, 4474, 1040, 2750, 4375, 1093, 29506, 11253, 29499, 1309, 2682, 1040, 20763, 6263, 1066, 2250, 1206, 5105, 28673, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 29491, 3761, 29493, 1224, 1390, 6015, 2750, 17066, 1072, 7369, 10560, 1276, 1120, 29491, 781, 1027, 1155, 1387, 4926, 9210, 11377, 1814, 9957, 29356, 1206, 3137, 2750, 12424, 1390, 6175, 16905, 1072, 17109, 1258, 29496, 1240, 1330, 1761, 4375, 1040, 7996, 5165, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8477, 29491, 28919, 29582, 29478, 19571, 9957, 781, 1027, 1155, 1387, 29528, 1260, 24912, 9957, 29356, 5858, 1124, 13802, 8271, 1390, 21313, 1040, 9020, 2605, 21447, 29493, 14725, 6142, 1066, 2641, 5165, 29491, 781, 1027, 1155, 1387, 3023, 29582, 29478, 19571, 9957, 29356, 1065, 1032, 19571, 14401, 1509, 21447, 1330, 6519, 7964, 1505, 26018, 29493, 7857, 6179, 29493, 1072, 23175, 29491, 14718, 12271, 3993, 1502, 1228, 4216, 9864, 29493, 1347, 1040, 5165, 4835, 2427, 1227, 1115, 1158, 6632, 1158, 1065, 1040, 3377, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 29515, 781, 29545, 5835, 1040, 3392, 8195, 1072, 17115, 16167, 7240, 1093, 4183, 28238, 5887, 29493, 1736, 4570, 14464, 29493, 1522, 1280, 7982, 5501, 29493, 1072, 8251, 2750, 12424, 1325, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 1827, 1527, 29491, 3761, 29493, 4227, 6319, 4057, 1124, 1040, 3716, 2997, 1072, 1539, 2536, 29491, 2031, 3525, 29493, 1163, 1032, 1930, 29501, 19193, 2100, 1065, 2871, 17660, 29493, 1136, 2427, 7387, 11292, 1066, 9345, 1070, 1065, 9599, 1428, 2444, 1124, 2027, 1032, 2254, 29501, 1184, 10288, 29491, 781, 781, 28100, 2066, 2573, 1184, 1465, 29515, 781, 29501, 6706, 28238, 5887, 1281, 3340, 1066, 16118, 1421, 9288, 5743, 2212, 20763, 6263, 29491, 781, 29501, 12540, 2181, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1066, 12744, 9288, 13600, 29491, 781, 29501, 6003, 11255, 1056, 5501, 1309, 3852, 1032, 6311, 11742, 1330, 9777, 1122, 16905, 5150, 29491, 781, 29501, 2732, 4790, 2750, 12424, 1066, 8641, 5165, 1072, 2750, 17066, 29491, 781, 29501, 1815, 1040, 9020, 2605, 21447, 1117, 6632, 29493, 2685, 4590, 1124, 13802, 8271, 1122, 7996, 5165, 29491, 781, 781, 1425, 14828, 29493, 1040, 8195, 1136, 6165, 1427, 1040, 5396, 1122, 1983, 2254, 1065, 2871, 28248, 29493, 4780, 1163, 1040, 1871, 7765, 14577, 29491, 4503, 1056, 2349, 25759, 1072, 17650, 2121, 29501, 10239, 5165, 1390, 2879, 1136, 1040, 1848, 11997, 21483, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' ```json\n{\n  "follow_ups": [\n    "Can you provide any benchmarks or real-world examples of similar setups for comparison?",\n    "What are the potential downsides or risks of overclocking the GPU memory in this setup?",\n    "How does the performance of AMD EPYC 73F3 compare to alternative CPUs for this kind of workload?",\n    "Are there any specific optimizations or configurations in ESXi that could mitigate the virtualization overhead?",\n    "What are some common pitfalls or issues to look out for when implementing NVLink in a multi-GPU setup?"\n  ]\n}\n```', 'output_ids': [13700, 29498, 15973, 29535, 4, 9557, 4139, 781, 29519, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 7093, 1136, 3852, 1475, 12752, 17949, 1210, 2121, 29501, 10239, 10022, 1070, 4452, 1576, 9791, 1122, 11135, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1040, 5396, 1828, 29481, 2694, 1210, 15447, 1070, 1522, 11255, 1056, 1040, 29243, 5501, 1065, 1224, 10288, 29572, 1316, 781, 3055, 1113, 6428, 2003, 1040, 5165, 1070, 28605, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 10352, 1066, 10053, 17339, 6263, 1122, 1224, 2880, 1070, 1539, 2536, 29572, 1316, 781, 3055, 1113, 6753, 1504, 1475, 3716, 7765, 14577, 1210, 25759, 1065, 28919, 29582, 29478, 1137, 1597, 3135, 12358, 1040, 9020, 2605, 21447, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1509, 4066, 12033, 17536, 1210, 5150, 1066, 1681, 1343, 1122, 1507, 27836, 28238, 5887, 1065, 1032, 6847, 29501, 26572, 10288, 1878, 781, 29473, 5477, 781, 29520, 781, 14708, 29600, 2], 'meta_info': {'id': 'bb1146a6f24b4eaabcaf1c5d329cce5b', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 2248, 'weight_version': 'default', 'completion_tokens': 148, 'cached_tokens': 1, 'e2e_latency': 9.694871187210083}}
[2025-10-13 02:58:54] INFO:     127.0.0.1:51836 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 03:11:06] INFO:     127.0.0.1:57046 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 03:11:06] Receive: obj=GenerateReqInput(rid='3dbf1fa18af94992bec811e6357134b3', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4, 2452, 7316, 1845, 1040, 1065, 2871, 5743, 1122, 1032, 8195, 15739, 5934, 2254, 29501, 1255, 12546, 20763, 6263, 29493, 1032, 7555, 15633] ... [29491, 3937, 29510, 29481, 2489, 1828, 1040, 4117, 5165, 1072, 1040, 5856, 1070, 4886, 1736, 12748, 29515, 781, 781, 28100, 8332, 20826, 29515, 781, 29501, 1387, 29552, 29512, 1186, 7568, 8176, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 20763, 6263, 9957, 3725, 1228, 7555, 12845, 29501, 8612, 20763, 6263, 1163, 29473, 29518, 29549, 7671, 1070, 1318, 29522, 3622, 2198, 29493, 11405, 1070, 13620, 22195, 16875, 1539, 29284, 29491, 781, 29501, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 1098, 29473, 29508, 29552, 29501, 3059, 29493, 29473, 29538, 29518, 29501, 6869, 5884, 29501, 8612, 15633, 1163, 1032, 3392, 10685, 5743, 1070, 29473, 29538, 29491, 29555, 1188, 13427, 1072, 1032, 11742, 10685, 5743, 1070, 29473, 29549, 29491, 29508, 1188, 13427, 29491, 1429, 29510, 29481, 1930, 29501, 3681, 2113, 1122, 16865, 5934, 20763, 6263, 1072, 13620, 15633, 29501, 8018, 10564, 29491, 781, 29501, 1387, 29550, 29508, 29518, 7671, 1152, 5000, 29549, 28618, 1206, 29473, 29538, 29518, 29502, 29502, 1119, 13427, 9957, 2409, 4511, 1070, 5501, 1066, 6175, 1137, 1342, 1539, 29284, 1228, 1227, 5501, 29501, 8018, 29491, 781, 29501, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 1072, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 29356, 1040, 1539, 2536, 1065, 1032, 9020, 2100, 5234, 14401, 1509, 21447, 29493, 1330, 5406, 12271, 3993, 1502, 1505, 28919, 29582, 29478, 1228, 4216, 9864, 29491, 781, 781, 28100, 7563, 1184, 1072, 14562, 1184, 29515, 781, 29501, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 9957, 1619, 1117, 4685, 7765, 2100, 1122, 5165, 1124, 5406, 29243, 13986, 29491, 781, 29501, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 16401, 2712, 29493, 1040, 3546, 1184, 1390, 1274, 1032, 14151, 5856, 1124, 1065, 2871, 5743, 7185, 1146, 7523, 1032, 8486, 29389, 1374, 1065, 13620, 2956, 10596, 29491, 781, 781, 28100, 20627, 12540, 1465, 29515, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 781, 1027, 1155, 1387, 3714, 28238, 5887, 9957, 28238, 5887, 6744, 2631, 2254, 29501, 4553, 3385, 9288, 2212, 20763, 6263, 29493, 17800, 4807, 2858, 1072, 17520, 13600, 29491, 1619, 1309, 1115, 21926, 1122, 5600, 4922, 1072, 1065, 2871, 29493, 1738, 1946, 3976, 1066, 1115, 7199, 5145, 2212, 20763, 6263, 29491, 781, 1027, 1155, 1387, 22237, 28238, 5887, 9957, 1167, 1491, 1056, 1124, 19622, 29474, 1073, 9550, 1122, 9288, 2212, 20763, 6263, 29493, 1458, 1761, 13865, 1509, 4807, 2858, 1072, 8411, 8312, 1827, 1527, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 781, 1027, 1155, 1387, 3714, 4581, 4570, 1152, 1141, 1508, 9957, 10384, 1040, 2631, 29243, 1135, 29518, 29521, 14464, 1245, 1040, 4625, 3830, 1309, 11885, 5684, 14437, 29501, 1300, 29501, 15488, 9288, 2212, 20763, 6263, 29493, 17800, 21447, 1072, 4807, 2858, 29491, 781, 1027, 1155, 1387, 22237, 4581, 4570, 1152, 1141, 1508, 9957, 1763, 1450, 15955, 1124, 5555, 14464, 29493, 1458, 2427, 1227, 1115, 1158, 7765, 2100, 1122, 29243, 29501, 1300, 29501, 26572, 9288, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 19921, 9957, 781, 1027, 1155, 1387, 3443, 1280, 7982, 19921, 9957, 6003, 11255, 1056, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 29493, 7079, 1281, 1342, 1539, 2536, 1117, 5501, 29501, 8018, 29491, 3761, 29493, 1040, 22490, 2427, 1115, 26054, 1072, 2335, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 5396, 16905, 5150, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 781, 1027, 1155, 1387, 24210, 1054, 9210, 11377, 1814, 9957, 25145, 4474, 1040, 2750, 4375, 1093, 29506, 11253, 29499, 1309, 2682, 1040, 20763, 6263, 1066, 2250, 1206, 5105, 28673, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 29491, 3761, 29493, 1224, 1390, 6015, 2750, 17066, 1072, 7369, 10560, 1276, 1120, 29491, 781, 1027, 1155, 1387, 4926, 9210, 11377, 1814, 9957, 29356, 1206, 3137, 2750, 12424, 1390, 6175, 16905, 1072, 17109, 1258, 29496, 1240, 1330, 1761, 4375, 1040, 7996, 5165, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8477, 29491, 28919, 29582, 29478, 19571, 9957, 781, 1027, 1155, 1387, 29528, 1260, 24912, 9957, 29356, 5858, 1124, 13802, 8271, 1390, 21313, 1040, 9020, 2605, 21447, 29493, 14725, 6142, 1066, 2641, 5165, 29491, 781, 1027, 1155, 1387, 3023, 29582, 29478, 19571, 9957, 29356, 1065, 1032, 19571, 14401, 1509, 21447, 1330, 6519, 7964, 1505, 26018, 29493, 7857, 6179, 29493, 1072, 23175, 29491, 14718, 12271, 3993, 1502, 1228, 4216, 9864, 29493, 1347, 1040, 5165, 4835, 2427, 1227, 1115, 1158, 6632, 1158, 1065, 1040, 3377, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 29515, 781, 29545, 5835, 1040, 3392, 8195, 1072, 17115, 16167, 7240, 1093, 4183, 28238, 5887, 29493, 1736, 4570, 14464, 29493, 1522, 1280, 7982, 5501, 29493, 1072, 8251, 2750, 12424, 1325, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 1827, 1527, 29491, 3761, 29493, 4227, 6319, 4057, 1124, 1040, 3716, 2997, 1072, 1539, 2536, 29491, 2031, 3525, 29493, 1163, 1032, 1930, 29501, 19193, 2100, 1065, 2871, 17660, 29493, 1136, 2427, 7387, 11292, 1066, 9345, 1070, 1065, 9599, 1428, 2444, 1124, 2027, 1032, 2254, 29501, 1184, 10288, 29491, 781, 781, 28100, 2066, 2573, 1184, 1465, 29515, 781, 29501, 6706, 28238, 5887, 1281, 3340, 1066, 16118, 1421, 9288, 5743, 2212, 20763, 6263, 29491, 781, 29501, 12540, 2181, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1066, 12744, 9288, 13600, 29491, 781, 29501, 6003, 11255, 1056, 5501, 1309, 3852, 1032, 6311, 11742, 1330, 9777, 1122, 16905, 5150, 29491, 781, 29501, 2732, 4790, 2750, 12424, 1066, 8641, 5165, 1072, 2750, 17066, 29491, 781, 29501, 1815, 1040, 9020, 2605, 21447, 1117, 6632, 29493, 2685, 4590, 1124, 13802, 8271, 1122, 7996, 5165, 29491, 781, 781, 1425, 14828, 29493, 1040, 8195, 1136, 6165, 1427, 1040, 5396, 1122, 1983, 2254, 1065, 2871, 28248, 29493, 4780, 1163, 1040, 1871, 7765, 14577, 29491, 4503, 1056, 2349, 25759, 1072, 17650, 2121, 29501, 10239, 5165, 1390, 2879, 1136, 1040, 1848, 11997, 21483, 29491, 2, 3, 1083, 1544, 1576, 1136, 1350, 1124, 1137, 10288, 29493, 1330, 1163, 1086, 5495, 1370, 1083, 1597, 1633, 1706, 29473, 29549, 29512, 29538, 29502, 29542, 29502, 29481, 29491, 781, 1027, 781, 2081, 15717, 1354, 13434, 29516, 29481, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 1066, 29473, 29538, 29552, 11946, 29516, 29481, 29576, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 03:11:06 TP0] Prefill batch. #new-seq: 1, #new-token: 54, #cached-token: 2007, token usage: 0.05, #running-req: 0, #queue-req: 0, 
[2025-10-13 03:11:07 TP0] Decode batch. #running-req: 1, #token: 2100, token usage: 0.06, cuda graph: True, gen throughput (token/s): 0.05, #queue-req: 0, 
[2025-10-13 03:11:08 TP0] Decode batch. #running-req: 1, #token: 2140, token usage: 0.06, cuda graph: True, gen throughput (token/s): 33.23, #queue-req: 0, 
[2025-10-13 03:11:10 TP0] Decode batch. #running-req: 1, #token: 2180, token usage: 0.06, cuda graph: True, gen throughput (token/s): 33.15, #queue-req: 0, 
[2025-10-13 03:11:11 TP0] Decode batch. #running-req: 1, #token: 2220, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.80, #queue-req: 0, 
[2025-10-13 03:11:12 TP0] Decode batch. #running-req: 1, #token: 2260, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.91, #queue-req: 0, 
[2025-10-13 03:11:13 TP0] Decode batch. #running-req: 1, #token: 2300, token usage: 0.06, cuda graph: True, gen throughput (token/s): 33.03, #queue-req: 0, 
[2025-10-13 03:11:14 TP0] Decode batch. #running-req: 1, #token: 2340, token usage: 0.06, cuda graph: True, gen throughput (token/s): 33.00, #queue-req: 0, 
[2025-10-13 03:11:16 TP0] Decode batch. #running-req: 1, #token: 2380, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.97, #queue-req: 0, 
[2025-10-13 03:11:17 TP0] Decode batch. #running-req: 1, #token: 2420, token usage: 0.06, cuda graph: True, gen throughput (token/s): 32.90, #queue-req: 0, 
[2025-10-13 03:11:18 TP0] Decode batch. #running-req: 1, #token: 2460, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.86, #queue-req: 0, 
[2025-10-13 03:11:19 TP0] Decode batch. #running-req: 1, #token: 2500, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.86, #queue-req: 0, 
[2025-10-13 03:11:20 TP0] Decode batch. #running-req: 1, #token: 2540, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.84, #queue-req: 0, 
[2025-10-13 03:11:22 TP0] Decode batch. #running-req: 1, #token: 2580, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.79, #queue-req: 0, 
[2025-10-13 03:11:23 TP0] Decode batch. #running-req: 1, #token: 2620, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.75, #queue-req: 0, 
[2025-10-13 03:11:24 TP0] Decode batch. #running-req: 1, #token: 2660, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.79, #queue-req: 0, 
[2025-10-13 03:11:25 TP0] Decode batch. #running-req: 1, #token: 2700, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.75, #queue-req: 0, 
[2025-10-13 03:11:27 TP0] Decode batch. #running-req: 1, #token: 2740, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.73, #queue-req: 0, 
[2025-10-13 03:11:28 TP0] Decode batch. #running-req: 1, #token: 2780, token usage: 0.07, cuda graph: True, gen throughput (token/s): 32.66, #queue-req: 0, 
[2025-10-13 03:11:28] Finish: obj=GenerateReqInput(rid='3dbf1fa18af94992bec811e6357134b3', text=None, input_ids=[1, 16, 2015, 15036, 29515, 781, 781, 17, 3, 17930, 1504, 6137, 1804, 1804, 9540, 9540, 9540, 2370, 10897, 1279, 1030, 29572, 4, 23325, 29576, 1083, 29510, 29487, 27713, 1158, 9694, 29493, 7747, 1136, 29491, 2370, 1452, 1136, 29572, 2370, 29510, 29481, 1342, 2138, 2172, 29572, 2, 3, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 4, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 2, 3, 2592, 1450, 1136, 2443, 1065, 2871, 5743, 1450, 1115, 1124, 29473, 29552, 29512, 29538, 29502, 29542, 29502, 29481, 29493, 1085, 3602, 29485, 29473, 29555, 29538, 29490, 29538, 29493, 29473, 29550, 29508, 29518, 16649, 1049, 3690, 29549, 29501, 29538, 29518, 29502, 29502, 29493, 1805, 5765, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 29493, 4590, 1164, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 29493, 15087, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 29493, 6397, 29501, 5079, 2492, 3546, 1184, 29491, 781, 2469, 12748, 1450, 3792, 29491, 3094, 29516, 21563, 26730, 3171, 781, 4183, 29516, 21563, 1736, 4570, 2631, 16230, 1052, 29518, 29488, 14464, 3366, 2004, 29515, 5217, 2276, 6990, 29491, 1443, 29516, 25917, 7104, 29516, 4892, 29501, 16230, 29501, 12462, 29501, 8272, 29516, 7011, 29516, 29550, 29550, 29502, 29491, 29550, 29549, 29491, 29508, 29550, 29501, 29488, 29518, 29488, 781, 3443, 11255, 2394, 29572, 781, 12550, 12424, 29572, 781, 29528, 1260, 8271, 4287, 1070, 28919, 29582, 29478, 19571, 29572, 4, 2452, 7316, 1845, 1040, 1065, 2871, 5743, 1122, 1032, 8195, 15739, 5934, 2254, 29501, 1255, 12546, 20763, 6263, 29493, 1032, 7555, 15633] ... [29491, 3937, 29510, 29481, 2489, 1828, 1040, 4117, 5165, 1072, 1040, 5856, 1070, 4886, 1736, 12748, 29515, 781, 781, 28100, 8332, 20826, 29515, 781, 29501, 1387, 29552, 29512, 1186, 7568, 8176, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 20763, 6263, 9957, 3725, 1228, 7555, 12845, 29501, 8612, 20763, 6263, 1163, 29473, 29518, 29549, 7671, 1070, 1318, 29522, 3622, 2198, 29493, 11405, 1070, 13620, 22195, 16875, 1539, 29284, 29491, 781, 29501, 1387, 3622, 29525, 1181, 12287, 29511, 29473, 29555, 29538, 29533, 29538, 15633, 9957, 1098, 29473, 29508, 29552, 29501, 3059, 29493, 29473, 29538, 29518, 29501, 6869, 5884, 29501, 8612, 15633, 1163, 1032, 3392, 10685, 5743, 1070, 29473, 29538, 29491, 29555, 1188, 13427, 1072, 1032, 11742, 10685, 5743, 1070, 29473, 29549, 29491, 29508, 1188, 13427, 29491, 1429, 29510, 29481, 1930, 29501, 3681, 2113, 1122, 16865, 5934, 20763, 6263, 1072, 13620, 15633, 29501, 8018, 10564, 29491, 781, 29501, 1387, 29550, 29508, 29518, 7671, 1152, 5000, 29549, 28618, 1206, 29473, 29538, 29518, 29502, 29502, 1119, 13427, 9957, 2409, 4511, 1070, 5501, 1066, 6175, 1137, 1342, 1539, 29284, 1228, 1227, 5501, 29501, 8018, 29491, 781, 29501, 1387, 3023, 29582, 29478, 29473, 29551, 29491, 29502, 29491, 29538, 29486, 29545, 1072, 1268, 29494, 3562, 29486, 29473, 29518, 29549, 29491, 29502, 29549, 1161, 8764, 19571, 9957, 29356, 1040, 1539, 2536, 1065, 1032, 9020, 2100, 5234, 14401, 1509, 21447, 29493, 1330, 5406, 12271, 3993, 1502, 1505, 28919, 29582, 29478, 1228, 4216, 9864, 29491, 781, 781, 28100, 7563, 1184, 1072, 14562, 1184, 29515, 781, 29501, 1387, 8328, 2171, 7456, 29516, 1734, 6060, 29509, 2660, 29496, 29538, 25095, 9957, 1619, 1117, 4685, 7765, 2100, 1122, 5165, 1124, 5406, 29243, 13986, 29491, 781, 29501, 1387, 7261, 29501, 5079, 2492, 3546, 1184, 9957, 16401, 2712, 29493, 1040, 3546, 1184, 1390, 1274, 1032, 14151, 5856, 1124, 1065, 2871, 5743, 7185, 1146, 7523, 1032, 8486, 29389, 1374, 1065, 13620, 2956, 10596, 29491, 781, 781, 28100, 20627, 12540, 1465, 29515, 781, 29508, 29491, 1387, 3714, 29516, 22237, 28238, 5887, 9957, 781, 1027, 1155, 1387, 3714, 28238, 5887, 9957, 28238, 5887, 6744, 2631, 2254, 29501, 4553, 3385, 9288, 2212, 20763, 6263, 29493, 17800, 4807, 2858, 1072, 17520, 13600, 29491, 1619, 1309, 1115, 21926, 1122, 5600, 4922, 1072, 1065, 2871, 29493, 1738, 1946, 3976, 1066, 1115, 7199, 5145, 2212, 20763, 6263, 29491, 781, 1027, 1155, 1387, 22237, 28238, 5887, 9957, 1167, 1491, 1056, 1124, 19622, 29474, 1073, 9550, 1122, 9288, 2212, 20763, 6263, 29493, 1458, 1761, 13865, 1509, 4807, 2858, 1072, 8411, 8312, 1827, 1527, 29491, 781, 781, 29518, 29491, 1387, 3714, 29516, 22237, 4581, 4570, 6823, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 781, 1027, 1155, 1387, 3714, 4581, 4570, 1152, 1141, 1508, 9957, 10384, 1040, 2631, 29243, 1135, 29518, 29521, 14464, 1245, 1040, 4625, 3830, 1309, 11885, 5684, 14437, 29501, 1300, 29501, 15488, 9288, 2212, 20763, 6263, 29493, 17800, 21447, 1072, 4807, 2858, 29491, 781, 1027, 1155, 1387, 22237, 4581, 4570, 1152, 1141, 1508, 9957, 1763, 1450, 15955, 1124, 5555, 14464, 29493, 1458, 2427, 1227, 1115, 1158, 7765, 2100, 1122, 29243, 29501, 1300, 29501, 26572, 9288, 29491, 781, 781, 29538, 29491, 1387, 3443, 11255, 19921, 9957, 781, 1027, 1155, 1387, 3443, 1280, 7982, 19921, 9957, 6003, 11255, 1056, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 29493, 7079, 1281, 1342, 1539, 2536, 1117, 5501, 29501, 8018, 29491, 3761, 29493, 1040, 22490, 2427, 1115, 26054, 1072, 2335, 1206, 1040, 3202, 1070, 5105, 2750, 17066, 1072, 5396, 16905, 5150, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 781, 1027, 1155, 1387, 24210, 1054, 9210, 11377, 1814, 9957, 25145, 4474, 1040, 2750, 4375, 1093, 29506, 11253, 29499, 1309, 2682, 1040, 20763, 6263, 1066, 2250, 1206, 5105, 28673, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 29491, 3761, 29493, 1224, 1390, 6015, 2750, 17066, 1072, 7369, 10560, 1276, 1120, 29491, 781, 1027, 1155, 1387, 4926, 9210, 11377, 1814, 9957, 29356, 1206, 3137, 2750, 12424, 1390, 6175, 16905, 1072, 17109, 1258, 29496, 1240, 1330, 1761, 4375, 1040, 7996, 5165, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8477, 29491, 28919, 29582, 29478, 19571, 9957, 781, 1027, 1155, 1387, 29528, 1260, 24912, 9957, 29356, 5858, 1124, 13802, 8271, 1390, 21313, 1040, 9020, 2605, 21447, 29493, 14725, 6142, 1066, 2641, 5165, 29491, 781, 1027, 1155, 1387, 3023, 29582, 29478, 19571, 9957, 29356, 1065, 1032, 19571, 14401, 1509, 21447, 1330, 6519, 7964, 1505, 26018, 29493, 7857, 6179, 29493, 1072, 23175, 29491, 14718, 12271, 3993, 1502, 1228, 4216, 9864, 29493, 1347, 1040, 5165, 4835, 2427, 1227, 1115, 1158, 6632, 1158, 1065, 1040, 3377, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 29515, 781, 29545, 5835, 1040, 3392, 8195, 1072, 17115, 16167, 7240, 1093, 4183, 28238, 5887, 29493, 1736, 4570, 14464, 29493, 1522, 1280, 7982, 5501, 29493, 1072, 8251, 2750, 12424, 1325, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 1827, 1527, 29491, 3761, 29493, 4227, 6319, 4057, 1124, 1040, 3716, 2997, 1072, 1539, 2536, 29491, 2031, 3525, 29493, 1163, 1032, 1930, 29501, 19193, 2100, 1065, 2871, 17660, 29493, 1136, 2427, 7387, 11292, 1066, 9345, 1070, 1065, 9599, 1428, 2444, 1124, 2027, 1032, 2254, 29501, 1184, 10288, 29491, 781, 781, 28100, 2066, 2573, 1184, 1465, 29515, 781, 29501, 6706, 28238, 5887, 1281, 3340, 1066, 16118, 1421, 9288, 5743, 2212, 20763, 6263, 29491, 781, 29501, 12540, 2181, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1066, 12744, 9288, 13600, 29491, 781, 29501, 6003, 11255, 1056, 5501, 1309, 3852, 1032, 6311, 11742, 1330, 9777, 1122, 16905, 5150, 29491, 781, 29501, 2732, 4790, 2750, 12424, 1066, 8641, 5165, 1072, 2750, 17066, 29491, 781, 29501, 1815, 1040, 9020, 2605, 21447, 1117, 6632, 29493, 2685, 4590, 1124, 13802, 8271, 1122, 7996, 5165, 29491, 781, 781, 1425, 14828, 29493, 1040, 8195, 1136, 6165, 1427, 1040, 5396, 1122, 1983, 2254, 1065, 2871, 28248, 29493, 4780, 1163, 1040, 1871, 7765, 14577, 29491, 4503, 1056, 2349, 25759, 1072, 17650, 2121, 29501, 10239, 5165, 1390, 2879, 1136, 1040, 1848, 11997, 21483, 29491, 2, 3, 1083, 1544, 1576, 1136, 1350, 1124, 1137, 10288, 29493, 1330, 1163, 1086, 5495, 1370, 1083, 1597, 1633, 1706, 29473, 29549, 29512, 29538, 29502, 29542, 29502, 29481, 29491, 781, 1027, 781, 2081, 15717, 1354, 13434, 29516, 29481, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 1066, 29473, 29538, 29552, 11946, 29516, 29481, 29576, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': " That's a significant improvement in token generation speed! Jumping from 12.99 tok/s to 36 tok/s is a substantial gain and highlights the benefits of leveraging multiple high-performance GPUs. Let's break down some of the factors contributing to this performance boost and how you might further optimize your setup:\n\n### Factors Contributing to Performance Boost:\n1. **Additional GPUs**: Adding more GPUs (from a single or fewer GPUs to 4x RTX 3090s) significantly increases the available computational power. Each RTX 3090 has a large number of CUDA cores and high bandwidth memory, which are crucial for the heavy matrix operations involved in AI inference.\n\n2. **Parallelization**: The use of multiple GPUs allows for parallel processing, which can dramatically speed up tasks that can be split across multiple devices. Modern AI frameworks are optimized to take advantage of such parallelism.\n\n3. **SGLang**: If SGLang is providing optimizations specifically for GPU computing, it might be contributing to the improved " ... "e benefits.\n\n6. **Optimized Software Stack**: Ensure that your software stack, including your AI frameworks and libraries, is fully optimized for multi-GPU setups. This might involve using specific libraries or versions that are known to perform well with multiple GPUs.\n\n7. **Batch Size and Pipeline Optimization**: Optimize your batch size and pipeline to ensure that each GPU is utilized to its fullest potential. Sometimes, increasing the batch size can lead to better hardware utilization.\n\n### Conclusion:\nYour jump from 12.99 tok/s to 36 tok/s is a clear indicator that your setup is capable of handling high-performance AI workloads effectively. By adding more GPUs and leveraging SGLang, you've seen a substantial improvement. Further optimizations, such as enabling NVLink, using modded drivers, and fine-tuning system settings, could potentially push your performance even higher.\n\nGood luck with your continued optimization efforts! If you have more specific questions or need further guidance, feel free to ask.", 'output_ids': [2], 'meta_info': {'id': '3dbf1fa18af94992bec811e6357134b3', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 2061, 'weight_version': 'default', 'completion_tokens': 724, 'cached_tokens': 2007, 'e2e_latency': 22.176150798797607}}
[2025-10-13 03:11:28] Receive: obj=GenerateReqInput(rid='d8d901eaac014d7c91ece71f757a38b5', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 781, 11891, 29515, 2592, 1450, 1136] ... [1158, 6632, 1158, 1065, 1040, 3377, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 29515, 781, 29545, 5835, 1040, 3392, 8195, 1072, 17115, 16167, 7240, 1093, 4183, 28238, 5887, 29493, 1736, 4570, 14464, 29493, 1522, 1280, 7982, 5501, 29493, 1072, 8251, 2750, 12424, 1325, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 1827, 1527, 29491, 3761, 29493, 4227, 6319, 4057, 1124, 1040, 3716, 2997, 1072, 1539, 2536, 29491, 2031, 3525, 29493, 1163, 1032, 1930, 29501, 19193, 2100, 1065, 2871, 17660, 29493, 1136, 2427, 7387, 11292, 1066, 9345, 1070, 1065, 9599, 1428, 2444, 1124, 2027, 1032, 2254, 29501, 1184, 10288, 29491, 781, 781, 28100, 2066, 2573, 1184, 1465, 29515, 781, 29501, 6706, 28238, 5887, 1281, 3340, 1066, 16118, 1421, 9288, 5743, 2212, 20763, 6263, 29491, 781, 29501, 12540, 2181, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1066, 12744, 9288, 13600, 29491, 781, 29501, 6003, 11255, 1056, 5501, 1309, 3852, 1032, 6311, 11742, 1330, 9777, 1122, 16905, 5150, 29491, 781, 29501, 2732, 4790, 2750, 12424, 1066, 8641, 5165, 1072, 2750, 17066, 29491, 781, 29501, 1815, 1040, 9020, 2605, 21447, 1117, 6632, 29493, 2685, 4590, 1124, 13802, 8271, 1122, 7996, 5165, 29491, 781, 781, 1425, 14828, 29493, 1040, 8195, 1136, 6165, 1427, 1040, 5396, 1122, 1983, 2254, 1065, 2871, 28248, 29493, 4780, 1163, 1040, 1871, 7765, 14577, 29491, 4503, 1056, 2349, 25759, 1072, 17650, 2121, 29501, 10239, 5165, 1390, 2879, 1136, 1040, 1848, 11997, 21483, 29491, 781, 11891, 29515, 1083, 1544, 1576, 1136, 1350, 1124, 1137, 10288, 29493, 1330, 1163, 1086, 5495, 1370, 1083, 1597, 1633, 1706, 29473, 29549, 29512, 29538, 29502, 29542, 29502, 29481, 29491, 781, 1027, 781, 2081, 15717, 1354, 13434, 29516, 29481, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 1066, 29473, 29538, 29552, 11946, 29516, 29481, 29576, 781, 5584, 8816, 13506, 29515, 2493, 29510, 29481, 1032, 6632, 14972, 1065, 6797, 9110, 5743, 29576, 1243, 2572, 1056, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 13434, 29516, 29481, 1066, 29473, 29538, 29552, 13434, 29516, 29481, 1117, 1032, 15813, 9124, 1072, 23857, 1040, 7964, 1070, 23367, 4822, 5934, 2254, 29501, 1255, 12546, 20763, 6263, 29491, 3937, 29510, 29481, 2489, 1828, 1509, 1070, 1040, 9380, 26653, 1066, 1224, 5165, 11742, 1072, 1678, 1136, 2427, 4397, 7765, 1421, 1342, 10288, 29515, 781, 781, 28100, 1169, 1340, 1502, 26783, 1056, 1066, 20627, 4036, 1272, 29515, 781, 29508, 29491, 1387, 2759, 3350, 20763, 6263, 9957, 4069, 1056, 1448, 20763, 6263, 1093, 3979, 1032, 3460, 1210, 16898, 20763, 6263, 1066, 29473, 29549, 29512, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 29481, 29499, 11885, 12863, 1040, 3400, 4401, 2017, 2750, 29491, 7834, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 1427, 1032, 3243, 2242, 1070, 1102, 29547, 6337, 1769, 1179, 1072, 2254, 4796, 3385, 5501, 29493, 1458, 1228, 13808, 1122, 1040, 6685, 7879, 7701, 6058, 1065, 16875, 1065, 2871, 29491, 781, 781, 29518, 29491, 1387, 27317, 2605, 9957, 1183, 1706, 1070, 5934, 20763, 6263, 6744, 1122, 13403, 10225, 29493, 1458, 1309, 26565, 5743, 1350, 10564, 1137, 1309, 1115, 8693, 3441, 5934, 9077, 29491, 14718, 16875, 11550, 29481, 1228, 7765, 2100, 1066, 2156, 8627, 1070, 2027, 13403, 2211, 29491, 781, 781, 29538, 29491, 1387, 29503, 5495, 1370, 9957, 1815, 1086, 5495, 1370, 1117, 8269, 7765, 14577, 10875, 1122, 29243, 22031, 29493, 1146, 2427, 1115, 26653, 1066, 1040, 12725, 5165, 1254, 1448, 24231, 29232, 1040, 3400, 13986, 6591, 29491, 781, 781, 28100, 11418, 3024, 10212, 28036, 14577, 29515, 781, 29508, 29491, 1387, 16874, 5887, 9957, 1815, 1136, 6257, 1717, 29510, 29475, 1274, 28238, 5887, 10419, 2212, 1342, 20763, 6263, 29493, 2685, 9601, 1146, 29491, 28238, 5887, 6080, 2254, 29501, 13317, 1559, 7950, 2212, 20763, 6263, 29493, 17800, 4807, 2858, 1072, 17520, 8312, 5165, 29491, 781, 781, 29518, 29491, 1387, 8514, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 27024, 1056, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1597, 4397, 12744, 9288, 13600, 2212, 20763, 6263, 29493, 14725, 6142, 1066, 5638, 5165, 22490, 29491, 781, 781, 29538, 29491, 1387, 10588, 6003, 11255, 1056, 9957, 6003, 11255, 1056, 1040, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 29491, 3761, 29493, 1146, 29510, 29481, 3046, 1066, 9777, 2355, 16905, 1072, 7369, 10560, 1276, 1120, 1507, 1522, 11255, 1056, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 25145, 4474, 1040, 2750, 12424, 1093, 29506, 11253, 29499, 1122, 1342, 20763, 6263, 1309, 2682, 1474, 1066, 2250, 1206, 5105, 28673, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 29491, 3761, 29493, 1224, 1390, 6015, 2750, 17066, 1072, 7369, 10560, 1276, 1120, 29493, 1347, 6175, 1136, 1274, 23961, 23333, 1072, 2750, 8770, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8477, 29491, 19571, 9957, 1815, 1136, 1228, 4590, 1224, 10288, 1124, 1032, 9020, 6367, 29493, 2685, 9028, 1146, 1124, 13802, 8271, 1066, 21313, 1475, 9020, 2605, 21447, 29491, 1619, 2427, 3852, 5638, 5165, 7964, 29491, 781, 781, 29552, 29491, 1387, 14973, 1089, 2100, 7351, 26087, 9957, 25985, 1137, 1342, 4698, 8000, 29493, 3258, 1342, 16875, 11550, 29481, 1072, 24901, 29493, 1117, 6662, 7765, 2100, 1122, 6847, 29501, 26572, 1576, 9791, 29491, 1619, 2427, 18284, 2181, 3716, 24901, 1210, 11799, 1137, 1228, 3419, 1066, 2993, 1930, 1163, 5934, 20763, 6263, 29491, 781, 781, 29555, 29491, 1387, 15481, 14677, 1072, 1135, 9152, 28036, 2605, 9957, 28036, 1421, 1342, 12154, 2437, 1072, 17660, 1066, 6175, 1137, 2198, 29243, 1117, 28829, 1066, 1639, 2941, 1142, 5396, 29491, 10506, 29493, 7256, 1040, 12154, 2437, 1309, 2504, 1066, 2641, 13986, 5247, 2605, 29491, 781, 781, 28100, 2093, 17481, 29515, 781, 11927, 7251, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 13434, 29516, 29481, 1066, 29473, 29538, 29552, 13434, 29516, 29481, 1117, 1032, 3849, 25260, 1137, 1342, 10288, 1117, 11405, 1070, 13620, 2254, 29501, 1255, 12546, 16875, 1539, 29284, 12234, 29491, 3231, 9601, 1448, 20763, 6263, 1072, 23367, 4822, 1086, 5495, 1370, 29493, 1136, 29510, 1101, 3366, 1032, 15813, 14972, 29491, 10212, 7765, 14577, 29493, 2027, 1158, 26516, 28238, 5887, 29493, 2181, 1736, 4570, 14464, 29493, 1072, 5201, 29501, 29475, 26290, 2355, 7240, 29493, 1597, 14725, 6464, 1342, 5165, 1787, 5105, 29491, 781, 781, 11372, 7344, 1163, 1342, 6085, 19354, 9048, 29576, 1815, 1136, 1274, 1448, 3716, 4992, 1210, 1695, 4397, 16756, 29493, 2369, 2701, 1066, 2228, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 03:11:28 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 229, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-10-13 03:11:30 TP0] Prefill batch. #new-seq: 1, #new-token: 709, #cached-token: 0, token usage: 0.06, #running-req: 0, #queue-req: 0, 
[2025-10-13 03:11:35 TP0] Decode batch. #running-req: 1, #token: 3021, token usage: 0.08, cuda graph: True, gen throughput (token/s): 5.34, #queue-req: 0, 
[2025-10-13 03:11:36 TP0] Decode batch. #running-req: 1, #token: 3061, token usage: 0.08, cuda graph: True, gen throughput (token/s): 32.49, #queue-req: 0, 
[2025-10-13 03:11:38 TP0] Decode batch. #running-req: 1, #token: 3101, token usage: 0.08, cuda graph: True, gen throughput (token/s): 32.49, #queue-req: 0, 
[2025-10-13 03:11:38] Finish: obj=GenerateReqInput(rid='d8d901eaac014d7c91ece71f757a38b5', text=None, input_ids=[1, 3, 1542, 11058, 29515, 781, 29503, 17707, 29473, 29538, 29501, 29550, 9366, 2140, 29501, 1483, 4992, 1210, 2925, 11498, 1137, 1040, 2956, 2427, 13496, 2228, 2447, 1065, 1224, 7882, 1158, 1032, 1387, 2606, 5557, 3586, 1124, 1040, 11474, 4108, 29493, 1066, 2084, 4456, 1210, 4302, 1037, 1040, 9155, 29491, 781, 28100, 3248, 15787, 29515, 781, 29501, 12786, 1312, 2140, 29501, 1483, 4992, 1245, 1040, 2956, 29577, 29481, 2073, 1070, 2972, 29493, 11434, 1066, 1040, 14660, 29491, 781, 29501, 7514, 4992, 3846, 1632, 29493, 3849, 29493, 1072, 5858, 5970, 1066, 1040, 10719, 9835, 29500, 29481, 1377, 781, 29501, 7120, 4165, 2140, 29501, 9791, 1137, 1806, 4135, 2846, 1040, 11474, 3804, 1072, 1279, 1227, 14518, 1535, 1171, 2909, 7591, 29491, 781, 29501, 1815, 1040, 7882, 1117, 1983, 3253, 1210, 1227, 3716, 29493, 4165, 1448, 3720, 1093, 3159, 9366, 29499, 2140, 29501, 9791, 1040, 2956, 2427, 2228, 29491, 781, 29501, 6706, 1040, 7882, 29510, 29481, 7026, 4610, 29513, 3137, 1066, 5068, 1281, 3299, 6476, 1608, 29491, 781, 29501, 12875, 2348, 1115, 1032, 10060, 3061, 1070, 12040, 29493, 1476, 4978, 3013, 1210, 1989, 15526, 29491, 781, 28100, 16753, 29515, 781, 9206, 5800, 29515, 1139, 1113, 24719, 29498, 9791, 2032, 8135, 25762, 29473, 29508, 29572, 1316, 1113, 25762, 29473, 29518, 29572, 1316, 1113, 25762, 29473, 29538, 1878, 29561, 1211, 781, 28100, 24549, 7634, 29515, 781, 29557, 13700, 29498, 15973, 29535, 781, 11891, 29515, 1083, 29577, 29487, 3710, 1066, 1800, 1678, 4870, 1040, 6797, 9110, 1117, 29493, 1309, 1136, 8372, 1343, 1032, 29473, 29508, 3652, 3667, 1254, 5444, 29572, 6681, 29508, 29502, 29502, 29502, 17014, 29572, 781, 5584, 8816, 13506, 29515, 13643, 29493, 1083, 29510, 29483, 1115, 5378, 1066, 2084, 1163, 1137, 29491, 4771, 29510, 29481, 1032, 7585, 23862, 1070, 1040, 4108, 1070, 19046, 11663, 29493, 1458, 1791, 1115, 2169, 29473, 29508, 29502, 29502, 29502, 17014, 29515, 781, 781, 11131, 15541, 23859, 1093, 12509, 29499, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29491, 1183, 2618, 1113, 1212, 15541, 11663, 29507, 1171, 1675, 1769, 2079, 1254, 2983, 10808, 4384, 29492, 1065, 29473, 29508, 29542, 29550, 29552, 29493, 1507, 1168, 4104, 1040, 1675, 12628, 10655, 1124, 1040, 4585, 1206, 1152, 1212, 20891, 6904, 29491, 1183, 3703, 2970, 1070, 16875, 1422, 7542, 1163, 7765, 2211, 29493, 1163, 22003, 1172, 1505, 2239, 9598, 1119, 27747, 7550, 1056, 1137, 12923, 1450, 1915, 4887, 3698, 11663, 1065, 1544, 1032, 2432, 10841, 29491, 781, 781, 28723, 29493, 1040, 4510, 7765, 2211, 1171, 4171, 5794, 1054, 1254, 1040, 7708, 1070, 1040, 4406, 29491, 1183, 29473, 29508, 29542, 29555, 29502, 29481, 1072, 29473, 29508, 29542, 29551, 29502, 29481, 3440, 1032, 6632, 4712, 4022, 1065, 16875, 4100, 29493, 1032, 3984, 3419, 1158, 1040, 1113, 12509, 9307, 1379, 1619, 1171, 3708, 1066, 1032, 10238, 1070, 9380, 29493, 3258, 1040, 10426, 1070, 3703, 19286, 1072, 1040, 15194, 1070, 17060, 12923, 1066, 3148, 4997, 4610, 29491, 781, 781, 1782, 29473, 29508, 29542, 29542, 29502, 29481, 11495, 1032, 9296, 2073, 1122, 16875, 29493, 1163, 1040, 10755, 1070, 1040, 8633, 1072, 1040, 7256, 19626, 1070, 3243, 27650, 29491, 1619, 4931, 1066, 1032, 6907, 1065, 4000, 5851, 6367, 5936, 29493, 1032, 20572, 1070, 16875, 1137, 15425, 4922, 19307, 1124, 1946, 1066, 1806, 21364, 1210, 10317, 29491, 3155, 1070, 1040, 2713, 2489, 15736, 29481, 1065, 6367, 5936, 1171, 1040, 4867, 1070, 26494, 12935, 29493, 1458, 1228, 2997, 1054, 1792, 1040, 3698, 7155, 1072, 1309, 3590, 1245, 1946, 1065, 1032, 1837, 1137, 7830, 19307, 4341, 29491, 781, 781, 1782, 29473, 29518, 29502, 29502, 29502, 29481, 3440, 4397, 28273, 1065, 16875, 29493, 1163, 1040, 4867, 1070, 4302, 5936, 10572, 1137, 5438, 26494, 12935, 1066, 1115, 11666, 1124, 1787, 6852, 27650, 29491, 1619, 4931, 1066, 6632, 20432, 1065, 1032, 6103, 3587, 1070, 16875, 9197, 29493, 1245, 4237, 1072, 9434, 14596, 1066, 4997, 4610, 10225, 1072, 19169, 1831, 29491, 781, 781, 1425, 6159, 2035, 29493, 16875, 1427, 3495, 13875, 16415, 1546, 1581, 7558, 5389, 29493, 1245, 9020, 6799, 2317, 1505, 9119, 29478, 1072, 5781, 29476, 1066, 1776, 29501, 29483, 17750, 9068, 1072, 26845, 19307, 1124, 18768, 4113, 29491, 3761, 29493, 1224, 15446, 1427, 1603, 7101, 3046, 27092, 1072, 26517, 1050, 4992, 29493, 2027, 1158, 1040, 5396, 1122, 16875, 1066, 1207, 9940, 29494, 1148, 7362, 1065, 8086, 1986, 1210, 1066, 1115, 2075, 1122, 6893, 11341, 11468, 29491, 781, 781, 6935, 1070, 1040, 2713, 11137, 12749, 16875, 3922, 1117, 1040, 1695, 1122, 8050, 1290, 16875, 29493, 1458, 16422, 1066, 1040, 6305, 1070, 16875, 5686, 1066, 8050, 1420, 10317, 1065, 1032, 1837, 1137, 11357, 1309, 3148, 29491, 1619, 1117, 7079, 3046, 1065, 5788, 1505, 16008, 1072, 12160, 11522, 29493, 1738, 1040, 1109, 2361, 1228, 2254, 1072, 1040, 10317, 2037, 1254, 16875, 5686, 1309, 1274, 6632, 14184, 29491, 781, 781, 3588, 1807, 8803, 1117, 1040, 1695, 1066, 6175, 1137, 16875, 1117, 6970, 1072, 2075, 1065, 1032, 1837, 1137, 1117, 27092, 1072, 8100, 29491, 1619, 6300, 25411, 5150, 1505, 16885, 1065, 16875, 19307, 29493, 1458, 1309, 6032, 1507, 1040, 1946, 2075, 1066, 6603, 1040, 19307, 1117, 1227, 16080, 1070, 1040, 5657, 1158, 1032, 3662, 29491, 1429, 1603, 6300, 20851, 1137, 16875, 1117, 2075, 1065, 1032, 1837, 1137, 3884, 29481, 13685, 1072, 3698, 5263, 29491, 781, 781, 3483, 5763, 1935, 11137, 29493, 1040, 4205, 1070, 16875, 5442, 7601, 29491, 11590, 3001, 1065, 5788, 1505, 11683, 22031, 1072, 24938, 23399, 14088, 3168, 1040, 9849, 1070, 1787, 1448, 7555, 16875, 5686, 29493, 2080, 1040, 7256, 19626, 1070, 1946, 1072, 22031, 2750, 1117, 3260, 16875, 1448, 15262, 1589, 3038, 1927, 29491, 781, 781, 1425, 13654, 29493, 16875, 1427, 2335, 1032, 1811, 1837, 2622, 1639, 1065, 2045, 1065, 1040, 5493, 29501, 29518, 29502, 1130, 6213, 29493, 1072, 1146, 11120, 1066, 1451, 6071, 1206, 1032, 9189, 14449, 29491, 4791, 1504, 1228, 6553, 11137, 1066, 1115, 17483, 29493, 1040, 5396, 7964, 1070, 16875, 1228, 16090, 29493, 1072, 1146, 1117, 4685, 1066, 1924, 1164, 13875, 3046, 4673, 1065, 1581, 5389, 1065, 1040, 2035, 1066, 2335, 29491, 1904, 1246, 4456, 1066, 2718, 1072, 16026, 16875, 5686, 29493, 1146, 1117, 8742, 1137, 1246, 1279, 1347, 1065, 1032, 1837, 1137, 1117, 27092, 29493, 8100, 29493, 1072, 26341, 29493, 1347, 1137, 1040, 7964, 1070, 16875, 1309, 1115, 7199, 1254, 1312, 29491, 781, 11891, 29515, 2592, 1450, 1136] ... [1158, 6632, 1158, 1065, 1040, 3377, 29491, 781, 781, 28100, 2297, 4500, 1328, 2871, 20354, 29515, 781, 29545, 5835, 1040, 3392, 8195, 1072, 17115, 16167, 7240, 1093, 4183, 28238, 5887, 29493, 1736, 4570, 14464, 29493, 1522, 1280, 7982, 5501, 29493, 1072, 8251, 2750, 12424, 1325, 1136, 1309, 2443, 1983, 2254, 1065, 2871, 1827, 1527, 29491, 3761, 29493, 4227, 6319, 4057, 1124, 1040, 3716, 2997, 1072, 1539, 2536, 29491, 2031, 3525, 29493, 1163, 1032, 1930, 29501, 19193, 2100, 1065, 2871, 17660, 29493, 1136, 2427, 7387, 11292, 1066, 9345, 1070, 1065, 9599, 1428, 2444, 1124, 2027, 1032, 2254, 29501, 1184, 10288, 29491, 781, 781, 28100, 2066, 2573, 1184, 1465, 29515, 781, 29501, 6706, 28238, 5887, 1281, 3340, 1066, 16118, 1421, 9288, 5743, 2212, 20763, 6263, 29491, 781, 29501, 12540, 2181, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1066, 12744, 9288, 13600, 29491, 781, 29501, 6003, 11255, 1056, 5501, 1309, 3852, 1032, 6311, 11742, 1330, 9777, 1122, 16905, 5150, 29491, 781, 29501, 2732, 4790, 2750, 12424, 1066, 8641, 5165, 1072, 2750, 17066, 29491, 781, 29501, 1815, 1040, 9020, 2605, 21447, 1117, 6632, 29493, 2685, 4590, 1124, 13802, 8271, 1122, 7996, 5165, 29491, 781, 781, 1425, 14828, 29493, 1040, 8195, 1136, 6165, 1427, 1040, 5396, 1122, 1983, 2254, 1065, 2871, 28248, 29493, 4780, 1163, 1040, 1871, 7765, 14577, 29491, 4503, 1056, 2349, 25759, 1072, 17650, 2121, 29501, 10239, 5165, 1390, 2879, 1136, 1040, 1848, 11997, 21483, 29491, 781, 11891, 29515, 1083, 1544, 1576, 1136, 1350, 1124, 1137, 10288, 29493, 1330, 1163, 1086, 5495, 1370, 1083, 1597, 1633, 1706, 29473, 29549, 29512, 29538, 29502, 29542, 29502, 29481, 29491, 781, 1027, 781, 2081, 15717, 1354, 13434, 29516, 29481, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 1066, 29473, 29538, 29552, 11946, 29516, 29481, 29576, 781, 5584, 8816, 13506, 29515, 2493, 29510, 29481, 1032, 6632, 14972, 1065, 6797, 9110, 5743, 29576, 1243, 2572, 1056, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 13434, 29516, 29481, 1066, 29473, 29538, 29552, 13434, 29516, 29481, 1117, 1032, 15813, 9124, 1072, 23857, 1040, 7964, 1070, 23367, 4822, 5934, 2254, 29501, 1255, 12546, 20763, 6263, 29491, 3937, 29510, 29481, 2489, 1828, 1509, 1070, 1040, 9380, 26653, 1066, 1224, 5165, 11742, 1072, 1678, 1136, 2427, 4397, 7765, 1421, 1342, 10288, 29515, 781, 781, 28100, 1169, 1340, 1502, 26783, 1056, 1066, 20627, 4036, 1272, 29515, 781, 29508, 29491, 1387, 2759, 3350, 20763, 6263, 9957, 4069, 1056, 1448, 20763, 6263, 1093, 3979, 1032, 3460, 1210, 16898, 20763, 6263, 1066, 29473, 29549, 29512, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 29481, 29499, 11885, 12863, 1040, 3400, 4401, 2017, 2750, 29491, 7834, 1167, 8724, 29473, 29538, 29502, 29542, 29502, 1427, 1032, 3243, 2242, 1070, 1102, 29547, 6337, 1769, 1179, 1072, 2254, 4796, 3385, 5501, 29493, 1458, 1228, 13808, 1122, 1040, 6685, 7879, 7701, 6058, 1065, 16875, 1065, 2871, 29491, 781, 781, 29518, 29491, 1387, 27317, 2605, 9957, 1183, 1706, 1070, 5934, 20763, 6263, 6744, 1122, 13403, 10225, 29493, 1458, 1309, 26565, 5743, 1350, 10564, 1137, 1309, 1115, 8693, 3441, 5934, 9077, 29491, 14718, 16875, 11550, 29481, 1228, 7765, 2100, 1066, 2156, 8627, 1070, 2027, 13403, 2211, 29491, 781, 781, 29538, 29491, 1387, 29503, 5495, 1370, 9957, 1815, 1086, 5495, 1370, 1117, 8269, 7765, 14577, 10875, 1122, 29243, 22031, 29493, 1146, 2427, 1115, 26653, 1066, 1040, 12725, 5165, 1254, 1448, 24231, 29232, 1040, 3400, 13986, 6591, 29491, 781, 781, 28100, 11418, 3024, 10212, 28036, 14577, 29515, 781, 29508, 29491, 1387, 16874, 5887, 9957, 1815, 1136, 6257, 1717, 29510, 29475, 1274, 28238, 5887, 10419, 2212, 1342, 20763, 6263, 29493, 2685, 9601, 1146, 29491, 28238, 5887, 6080, 2254, 29501, 13317, 1559, 7950, 2212, 20763, 6263, 29493, 17800, 4807, 2858, 1072, 17520, 8312, 5165, 29491, 781, 781, 29518, 29491, 1387, 8514, 26572, 1135, 29518, 29521, 1152, 1141, 1508, 9957, 27024, 1056, 1040, 1736, 4570, 2631, 29243, 1135, 29518, 29521, 14464, 1597, 4397, 12744, 9288, 13600, 2212, 20763, 6263, 29493, 14725, 6142, 1066, 5638, 5165, 22490, 29491, 781, 781, 29538, 29491, 1387, 10588, 6003, 11255, 1056, 9957, 6003, 11255, 1056, 1040, 29243, 5501, 1309, 3852, 1032, 6311, 5165, 11742, 29491, 3761, 29493, 1146, 29510, 29481, 3046, 1066, 9777, 2355, 16905, 1072, 7369, 10560, 1276, 1120, 1507, 1522, 11255, 1056, 29491, 781, 781, 29549, 29491, 1387, 12550, 11377, 1814, 9957, 25145, 4474, 1040, 2750, 12424, 1093, 29506, 11253, 29499, 1122, 1342, 20763, 6263, 1309, 2682, 1474, 1066, 2250, 1206, 5105, 28673, 1122, 4542, 16540, 29493, 14725, 17520, 5165, 29491, 3761, 29493, 1224, 1390, 6015, 2750, 17066, 1072, 7369, 10560, 1276, 1120, 29493, 1347, 6175, 1136, 1274, 23961, 23333, 1072, 2750, 8770, 29491, 781, 781, 29550, 29491, 1387, 29528, 1260, 24912, 8477, 29491, 19571, 9957, 1815, 1136, 1228, 4590, 1224, 10288, 1124, 1032, 9020, 6367, 29493, 2685, 9028, 1146, 1124, 13802, 8271, 1066, 21313, 1475, 9020, 2605, 21447, 29491, 1619, 2427, 3852, 5638, 5165, 7964, 29491, 781, 781, 29552, 29491, 1387, 14973, 1089, 2100, 7351, 26087, 9957, 25985, 1137, 1342, 4698, 8000, 29493, 3258, 1342, 16875, 11550, 29481, 1072, 24901, 29493, 1117, 6662, 7765, 2100, 1122, 6847, 29501, 26572, 1576, 9791, 29491, 1619, 2427, 18284, 2181, 3716, 24901, 1210, 11799, 1137, 1228, 3419, 1066, 2993, 1930, 1163, 5934, 20763, 6263, 29491, 781, 781, 29555, 29491, 1387, 15481, 14677, 1072, 1135, 9152, 28036, 2605, 9957, 28036, 1421, 1342, 12154, 2437, 1072, 17660, 1066, 6175, 1137, 2198, 29243, 1117, 28829, 1066, 1639, 2941, 1142, 5396, 29491, 10506, 29493, 7256, 1040, 12154, 2437, 1309, 2504, 1066, 2641, 13986, 5247, 2605, 29491, 781, 781, 28100, 2093, 17481, 29515, 781, 11927, 7251, 1245, 29473, 29508, 29518, 29491, 29542, 29542, 13434, 29516, 29481, 1066, 29473, 29538, 29552, 13434, 29516, 29481, 1117, 1032, 3849, 25260, 1137, 1342, 10288, 1117, 11405, 1070, 13620, 2254, 29501, 1255, 12546, 16875, 1539, 29284, 12234, 29491, 3231, 9601, 1448, 20763, 6263, 1072, 23367, 4822, 1086, 5495, 1370, 29493, 1136, 29510, 1101, 3366, 1032, 15813, 14972, 29491, 10212, 7765, 14577, 29493, 2027, 1158, 26516, 28238, 5887, 29493, 2181, 1736, 4570, 14464, 29493, 1072, 5201, 29501, 29475, 26290, 2355, 7240, 29493, 1597, 14725, 6464, 1342, 5165, 1787, 5105, 29491, 781, 781, 11372, 7344, 1163, 1342, 6085, 19354, 9048, 29576, 1815, 1136, 1274, 1448, 3716, 4992, 1210, 1695, 4397, 16756, 29493, 2369, 2701, 1066, 2228, 29491, 781, 1468, 13700, 29498, 15973, 29535, 4], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' ```json\n{\n  "follow_ups": [\n    "Can you provide more details on the NVLink setup and its expected improvements?",\n    "What are the potential risks and benefits of overclocking the GPU memory?",\n    "How can I optimize my software stack for multi-GPU setups?",\n    "What are the steps to transition from a VM to a bare metal setup for better performance?",\n    "Can you explain more about batch size optimization and its impact on inference speed?"\n  ]\n}\n```', 'output_ids': [13700, 29498, 15973, 29535, 4, 9557, 4139, 781, 29519, 781, 29473, 1113, 24719, 29498, 9791, 2032, 1501, 781, 3055, 1113, 7093, 1136, 3852, 1448, 4930, 1124, 1040, 28238, 5887, 10288, 1072, 1639, 4117, 20432, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1040, 5396, 15447, 1072, 7964, 1070, 1522, 11255, 1056, 1040, 29243, 5501, 29572, 1316, 781, 3055, 1113, 6428, 1309, 1083, 7765, 1421, 1354, 4698, 8000, 1122, 6847, 29501, 26572, 1576, 9791, 29572, 1316, 781, 3055, 1113, 3963, 1228, 1040, 6712, 1066, 9033, 1245, 1032, 19571, 1066, 1032, 13802, 8271, 10288, 1122, 2641, 5165, 29572, 1316, 781, 3055, 1113, 7093, 1136, 8050, 1448, 1452, 12154, 2437, 19354, 1072, 1639, 5856, 1124, 1065, 2871, 5743, 1878, 781, 29473, 5477, 781, 29520, 781, 14708, 29600, 2], 'meta_info': {'id': 'd8d901eaac014d7c91ece71f757a38b5', 'finish_reason': {'type': 'stop', 'matched': 2}, 'prompt_tokens': 2986, 'weight_version': 'default', 'completion_tokens': 119, 'cached_tokens': 229, 'e2e_latency': 9.882030248641968}}
[2025-10-13 03:11:38] INFO:     127.0.0.1:33540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 06:51:52] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 06:51:52] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 06:51:53] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 06:52:00] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=190218093, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 06:52:00] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 06:52:01] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 06:52:08 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 06:52:08 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 06:52:08 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 06:52:09 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 06:52:09 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 06:52:09 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 06:52:09 TP0] Init torch distributed begin.
[2025-10-13 06:52:09 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 06:52:09 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 06:52:09 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 06:52:09 TP0] sglang is using nccl==2.27.3
[2025-10-13 06:52:10 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 06:52:10 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 06:52:10 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 06:52:10 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 06:52:10 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 06:52:10 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 06:52:10 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 06:52:10 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 06:52:10 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 06:52:11 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:01<00:15,  1.92s/it]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:03<00:11,  1.58s/it]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:05<00:11,  1.96s/it]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:07<00:09,  1.94s/it]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:09<00:07,  1.95s/it]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:11<00:05,  1.85s/it]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:11<00:02,  1.43s/it]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:12<00:01,  1.22s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:14<00:00,  1.42s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:14<00:00,  1.60s/it]

[2025-10-13 06:52:26 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 06:52:26 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 06:52:26 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 06:52:26 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 06:52:26 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 06:52:26 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 06:52:26 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 06:52:26 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 06:52:26 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:16,  1.24s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:16,  1.24s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.25it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.25it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:01<00:05,  1.85it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:01<00:05,  1.85it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.41it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.41it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.97it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.97it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.50it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.50it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:52<01:56, 16.65s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:52<01:56, 16.65s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:10<01:42, 17.01s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:10<01:42, 17.01s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:28<01:26, 17.37s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:28<01:26, 17.37s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [01:47<01:11, 17.84s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [01:47<01:11, 17.84s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [02:08<00:55, 18.58s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [02:08<00:55, 18.58s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [02:29<00:38, 19.45s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [02:29<00:38, 19.45s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [02:51<00:20, 20.28s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [02:51<00:20, 20.28s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:14<00:00, 21.18s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:14<00:00, 13.92s/it]
[2025-10-13 06:55:43 TP0] Capture cuda graph end. Time elapsed: 196.13 s. mem usage=0.41 GB. avail mem=3.23 GB.
[2025-10-13 06:55:43 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.23 GB
[2025-10-13 06:55:43] INFO:     Started server process [1926536]
[2025-10-13 06:55:43] INFO:     Waiting for application startup.
[2025-10-13 06:55:43] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 06:55:44] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 06:55:44] INFO:     Application startup complete.
[2025-10-13 06:55:44] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 06:55:45] INFO:     127.0.0.1:41322 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 06:55:45] Receive: obj=GenerateReqInput(rid='d8c29e96211c40bf9bd2adf7d669892c', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 06:55:45 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 06:55:45] INFO:     127.0.0.1:41344 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 06:55:45] Receive: obj=GenerateReqInput(rid='b5cec747ef8a41acb10587daa98df1c8', text=None, input_ids=[128000, 16892, 14173, 25, 62853, 609, 794, 330, 2984, 13473, 20432, 498, 330, 4789, 794, 2990, 77, 286, 2522, 20432, 323, 1920, 264, 3566, 2199, 1701, 436, 1190, 2259, 41483, 1734, 1734, 286, 3755, 330, 14105, 794, 5324, 13495, 794, 5324, 1103, 794, 5324, 4789, 794, 330, 791, 5665, 315, 279, 3566, 2199, 311, 58228, 10684, 330, 1337, 794, 330, 928, 9388, 2186, 330, 6413, 794, 4482, 1103, 8073, 330, 1337, 794, 330, 1735, 32075, 2595, 7927, 3465, 374, 311, 5268, 323, 471, 279, 4495, 5507, 1161, 8, 505, 279, 1160, 315, 2561, 7526, 3196, 389, 279, 3319, 13, 11359, 1521, 17959, 1473, 12, 3494, 1193, 279, 4823, 1665, 11, 2085, 904, 5217, 1495, 477, 16540, 382, 12, 1442, 912, 7526, 2489, 279, 3319, 11, 471, 459, 4384, 1358, 25, 720, 256, 341, 257, 330, 14506, 46736, 794, 4260, 256, 557, 12, 1442, 832, 477, 810, 7526, 2489, 279, 3319, 11, 9429, 264, 4823, 2077, 8649, 264, 330, 14506, 46736, 1, 1358, 449, 6302, 430, 2997, 512, 256, 482, 330, 609, 794, 578, 5507, 596, 836, 627, 256, 482, 330, 14105, 794, 362, 11240, 315, 2631, 5137, 323, 872, 12435, 2819, 382, 791, 3645, 369, 279, 4823, 2077, 374, 26549, 512, 517, 220, 330, 14506, 46736, 794, 2330, 262, 5324, 609, 794, 330, 14506, 678, 16, 498, 330, 14105, 794, 5324, 798, 16, 794, 330, 970, 16, 49185, 262, 5324, 609, 794, 330, 14506, 678, 17, 498, 330, 14105, 794, 5324, 798, 17, 794, 330, 970, 17, 96742, 220, 5243, 92, 128011, 2929, 25, 11346, 512, 5045, 3931, 2891, 25, 4304, 29815, 389, 279, 2262, 88937, 505, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 13650, 11, 21513, 11, 323, 2218, 30994, 29068, 291, 311, 26029, 1888, 12659, 11, 5195, 596, 12384, 11, 323, 88278, 2778, 28849, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 25, 330, 5479, 220, 605, 26029, 56619, 311, 34507, 4718, 8184, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 10631, 11, 220, 2366, 18, 18845, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 384, 42685, 35490, 271, 45464, 14711, 220, 17, 13, 3146, 3936, 25, 330, 4438, 311, 35602, 4718, 8184, 30241, 34711, 53435, 82, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 44483, 11920, 922, 7528, 4868, 271, 45464, 14711, 220, 18, 13, 3146, 3936, 25, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 18, 8661, 11, 15592, 18052, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 9721, 3046, 304, 279, 7528, 3634, 271, 45464, 14711, 220, 19, 13, 3146, 3936, 25, 330, 6109, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 1, 1035, 256, 482, 3146, 44942, 68063, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 16406, 7980, 11, 384, 42685, 9873, 11, 3566, 26897, 10968, 389, 18899, 1217, 3217, 271, 45464, 14711, 220, 20, 13, 3146, 3936, 25, 330, 4301, 19723, 25, 2650, 97693, 5364, 847, 11880, 34507, 291, 264, 17829, 39514, 596, 34711, 17476, 1, 1035, 256, 482, 3146, 44942, 68063, 11799, 4007, 11, 62542, 2450, 11, 21210, 6650, 11, 97693, 5364, 847, 11880, 11, 2626, 1162, 198, 256, 482, 3146, 6531, 66000, 68063, 17829, 44483, 11, 5597, 76170, 11, 9873, 3411, 369, 62542, 10105, 271, 45464, 14711, 220, 21, 13, 3146, 3936, 25, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 8184, 13346, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 5597, 76170, 44929, 922, 26029, 753, 51752, 271, 45464, 14711, 220, 22, 13, 3146, 3936, 25, 330, 1061, 12, 99584, 18729, 25, 2650, 33527, 3053, 15583, 4718, 28845, 1, 1035, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 28975, 11, 8446, 18475, 11, 2626, 6650, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 3411, 311, 33164, 828, 271, 45464, 14711, 220, 23, 13, 3146, 3936, 25, 330, 791, 7572, 315, 16835, 15457, 83, 6427, 304, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 68063, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 8661, 7411, 11, 6883, 20258, 10968, 389, 4857, 264, 29722, 7275, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 7275, 5045, 3931, 2891, 25, 4304, 40914, 0, 2057, 7068, 7701, 9959, 323, 7524, 5117, 1772, 13650, 369, 97693, 5364, 847, 11880, 4340, 11, 584, 690, 5357, 389, 279, 6332, 3600, 323, 94436, 27463, 389, 872, 3997, 25, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 13, 5810, 527, 220, 605, 29722, 5117, 1772, 13650, 11, 3235, 449, 872, 20081, 21513, 323, 2218, 30994, 1473, 14711, 220, 16, 13, 3146, 3936, 25, 330, 11060, 287, 26029, 304, 220, 2366, 18, 25, 7054, 56619, 369, 15344, 71190, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2678, 9873, 11, 220, 2366, 18, 26029, 11, 2778, 4817, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 2626, 7980, 11, 7528, 55572, 11, 35490, 271, 14711, 220, 17, 13, 3146, 3936, 25, 330, 791, 19121, 13002, 311, 34711, 17476, 369, 18766, 71190, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 8641, 11, 6617, 9873, 11, 21516, 18208, 11, 828, 9313, 11, 2626, 4868, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 15749, 11, 21210, 48727, 271, 14711, 220, 18, 13, 3146, 3936, 25, 330, 43, 2099, 4210, 15592, 369, 62549, 14434, 18729, 18591, 1, 1035, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 8661, 11, 15592, 5557, 11, 24872, 3135, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 15592, 43448, 271, 14711, 220, 19, 13, 3146, 3936, 25, 330, 5479, 5000, 7127, 50730, 311, 34507, 4718, 43609, 48076, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 5000, 2955, 18845, 11, 14747, 7969, 11, 220, 2366, 18, 2955, 11, 62593, 75983, 11, 3997, 26329, 198, 256, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 384, 42685, 9873, 11, 3997, 7980, 271, 14711, 220, 20, 13, 3146, 3936, 25, 330, 4301, 19723, 25, 55664, 1113, 34711, 17476, 369, 264, 17829, 39514, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 1162, 4007, 11, 13312, 21210, 11, 4868, 27886, 11, 97693, 5364, 847, 11880, 11, 2626, 2450, 198, 256, 482, 3146, 6531, 66000, 68063, 17829, 44483, 11, 2626, 7980, 11, 62542, 15749, 271, 14711, 220, 21, 13, 3146, 3936, 25, 330, 791, 5843, 9469, 4289, 39195, 315, 93696, 304, 26029, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 7720, 11, 7528, 8661, 11, 2626, 6650, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 6020, 5597, 76170, 271, 14711, 220, 22, 13, 3146, 3936, 25, 330, 6102, 318, 4954, 51752, 449, 2956, 12, 99584, 18729, 56619, 1, 1035, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 51752, 11, 8661, 15174, 11, 28975, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 271, 14711, 220, 23, 13, 3146, 3936, 25, 330, 4438, 61844, 8871, 8471, 3053, 15583, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 68063, 61844, 8871, 3600, 11, 2626, 18475, 11, 8871, 10105, 11, 13312, 1862, 11, 26206, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 6278, 3157, 40919, 271, 14711, 220, 24, 13, 3146, 3936, 25, 330, 53463, 287, 4718, 9983, 7972, 79723, 25, 33257, 26788, 369, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 9983, 3772, 9546, 11, 6335, 10631, 11, 220, 2366, 18, 3674, 3772, 11, 2626, 20392, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 9983, 3772, 20258, 11, 2678, 2626, 7980, 11, 8661, 50184, 271, 14711, 220, 605, 13, 3146, 3936, 25, 330, 791, 29680, 315, 62593, 7127, 389, 4718, 26029, 83899, 1, 1035, 262, 482, 3146, 44942, 68063, 62593, 2955, 11, 26029, 33407, 11, 5536, 389, 26029, 11, 1217, 3217, 11, 3997, 26329, 198, 262, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 26029, 35416, 11, 7528, 55572, 11, 2626, 7980, 271, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 6319, 311, 5398, 449, 279, 6332, 4634, 6072, 323, 3600, 315, 97693, 5364, 847, 11880, 4340, 13, 3296, 52913, 9959, 21513, 11, 1521, 8158, 690, 9504, 279, 1314, 10877, 323, 6678, 23222, 20392, 13, 9062, 8712, 374, 17550, 7119, 264, 3230, 38462, 11125, 26793, 1139, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 11, 47594, 279, 2816, 596, 26029, 5178, 323, 1217, 20392, 14781, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 7275, 2929, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 06:55:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1753, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-13 06:55:48] Finish: obj=GenerateReqInput(rid='d8c29e96211c40bf9bd2adf7d669892c', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is known for its iconic', 'output_ids': [6864, 3363, 315, 9822, 374, 12366, 13, 1102, 374, 3967, 369, 1202, 27373], 'meta_info': {'id': 'd8c29e96211c40bf9bd2adf7d669892c', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 3.2384302616119385}}
[2025-10-13 06:55:48] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 06:55:48] The server is fired up and ready to roll!
[2025-10-13 06:55:49 TP0] Decode batch. #running-req: 1, #token: 1794, token usage: 0.02, cuda graph: True, gen throughput (token/s): 8.66, #queue-req: 0, 
[2025-10-13 06:55:49 TP0] Decode batch. #running-req: 1, #token: 1834, token usage: 0.02, cuda graph: True, gen throughput (token/s): 51.80, #queue-req: 0, 
[2025-10-13 06:55:50 TP0] Decode batch. #running-req: 1, #token: 1874, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.49, #queue-req: 0, 
[2025-10-13 06:55:51 TP0] Decode batch. #running-req: 1, #token: 1914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.01, #queue-req: 0, 
[2025-10-13 06:55:51] Finish: obj=GenerateReqInput(rid='b5cec747ef8a41acb10587daa98df1c8', text=None, input_ids=[128000, 16892, 14173, 25, 62853, 609, 794, 330, 2984, 13473, 20432, 498, 330, 4789, 794, 2990, 77, 286, 2522, 20432, 323, 1920, 264, 3566, 2199, 1701, 436, 1190, 2259, 41483, 1734, 1734, 286, 3755, 330, 14105, 794, 5324, 13495, 794, 5324, 1103, 794, 5324, 4789, 794, 330, 791, 5665, 315, 279, 3566, 2199, 311, 58228, 10684, 330, 1337, 794, 330, 928, 9388, 2186, 330, 6413, 794, 4482, 1103, 8073, 330, 1337, 794, 330, 1735, 32075, 2595, 7927, 3465, 374, 311, 5268, 323, 471, 279, 4495, 5507, 1161, 8, 505, 279, 1160, 315, 2561, 7526, 3196, 389, 279, 3319, 13, 11359, 1521, 17959, 1473, 12, 3494, 1193, 279, 4823, 1665, 11, 2085, 904, 5217, 1495, 477, 16540, 382, 12, 1442, 912, 7526, 2489, 279, 3319, 11, 471, 459, 4384, 1358, 25, 720, 256, 341, 257, 330, 14506, 46736, 794, 4260, 256, 557, 12, 1442, 832, 477, 810, 7526, 2489, 279, 3319, 11, 9429, 264, 4823, 2077, 8649, 264, 330, 14506, 46736, 1, 1358, 449, 6302, 430, 2997, 512, 256, 482, 330, 609, 794, 578, 5507, 596, 836, 627, 256, 482, 330, 14105, 794, 362, 11240, 315, 2631, 5137, 323, 872, 12435, 2819, 382, 791, 3645, 369, 279, 4823, 2077, 374, 26549, 512, 517, 220, 330, 14506, 46736, 794, 2330, 262, 5324, 609, 794, 330, 14506, 678, 16, 498, 330, 14105, 794, 5324, 798, 16, 794, 330, 970, 16, 49185, 262, 5324, 609, 794, 330, 14506, 678, 17, 498, 330, 14105, 794, 5324, 798, 17, 794, 330, 970, 17, 96742, 220, 5243, 92, 128011, 2929, 25, 11346, 512, 5045, 3931, 2891, 25, 4304, 29815, 389, 279, 2262, 88937, 505, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 13650, 11, 21513, 11, 323, 2218, 30994, 29068, 291, 311, 26029, 1888, 12659, 11, 5195, 596, 12384, 11, 323, 88278, 2778, 28849, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 25, 330, 5479, 220, 605, 26029, 56619, 311, 34507, 4718, 8184, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 10631, 11, 220, 2366, 18, 18845, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 384, 42685, 35490, 271, 45464, 14711, 220, 17, 13, 3146, 3936, 25, 330, 4438, 311, 35602, 4718, 8184, 30241, 34711, 53435, 82, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 44483, 11920, 922, 7528, 4868, 271, 45464, 14711, 220, 18, 13, 3146, 3936, 25, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 18, 8661, 11, 15592, 18052, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 9721, 3046, 304, 279, 7528, 3634, 271, 45464, 14711, 220, 19, 13, 3146, 3936, 25, 330, 6109, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 1, 1035, 256, 482, 3146, 44942, 68063, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 16406, 7980, 11, 384, 42685, 9873, 11, 3566, 26897, 10968, 389, 18899, 1217, 3217, 271, 45464, 14711, 220, 20, 13, 3146, 3936, 25, 330, 4301, 19723, 25, 2650, 97693, 5364, 847, 11880, 34507, 291, 264, 17829, 39514, 596, 34711, 17476, 1, 1035, 256, 482, 3146, 44942, 68063, 11799, 4007, 11, 62542, 2450, 11, 21210, 6650, 11, 97693, 5364, 847, 11880, 11, 2626, 1162, 198, 256, 482, 3146, 6531, 66000, 68063, 17829, 44483, 11, 5597, 76170, 11, 9873, 3411, 369, 62542, 10105, 271, 45464, 14711, 220, 21, 13, 3146, 3936, 25, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 8184, 13346, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 5597, 76170, 44929, 922, 26029, 753, 51752, 271, 45464, 14711, 220, 22, 13, 3146, 3936, 25, 330, 1061, 12, 99584, 18729, 25, 2650, 33527, 3053, 15583, 4718, 28845, 1, 1035, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 28975, 11, 8446, 18475, 11, 2626, 6650, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 3411, 311, 33164, 828, 271, 45464, 14711, 220, 23, 13, 3146, 3936, 25, 330, 791, 7572, 315, 16835, 15457, 83, 6427, 304, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 68063, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 8661, 7411, 11, 6883, 20258, 10968, 389, 4857, 264, 29722, 7275, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 7275, 5045, 3931, 2891, 25, 4304, 40914, 0, 2057, 7068, 7701, 9959, 323, 7524, 5117, 1772, 13650, 369, 97693, 5364, 847, 11880, 4340, 11, 584, 690, 5357, 389, 279, 6332, 3600, 323, 94436, 27463, 389, 872, 3997, 25, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 13, 5810, 527, 220, 605, 29722, 5117, 1772, 13650, 11, 3235, 449, 872, 20081, 21513, 323, 2218, 30994, 1473, 14711, 220, 16, 13, 3146, 3936, 25, 330, 11060, 287, 26029, 304, 220, 2366, 18, 25, 7054, 56619, 369, 15344, 71190, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2678, 9873, 11, 220, 2366, 18, 26029, 11, 2778, 4817, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 2626, 7980, 11, 7528, 55572, 11, 35490, 271, 14711, 220, 17, 13, 3146, 3936, 25, 330, 791, 19121, 13002, 311, 34711, 17476, 369, 18766, 71190, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 8641, 11, 6617, 9873, 11, 21516, 18208, 11, 828, 9313, 11, 2626, 4868, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 15749, 11, 21210, 48727, 271, 14711, 220, 18, 13, 3146, 3936, 25, 330, 43, 2099, 4210, 15592, 369, 62549, 14434, 18729, 18591, 1, 1035, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 8661, 11, 15592, 5557, 11, 24872, 3135, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 15592, 43448, 271, 14711, 220, 19, 13, 3146, 3936, 25, 330, 5479, 5000, 7127, 50730, 311, 34507, 4718, 43609, 48076, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 5000, 2955, 18845, 11, 14747, 7969, 11, 220, 2366, 18, 2955, 11, 62593, 75983, 11, 3997, 26329, 198, 256, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 384, 42685, 9873, 11, 3997, 7980, 271, 14711, 220, 20, 13, 3146, 3936, 25, 330, 4301, 19723, 25, 55664, 1113, 34711, 17476, 369, 264, 17829, 39514, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 1162, 4007, 11, 13312, 21210, 11, 4868, 27886, 11, 97693, 5364, 847, 11880, 11, 2626, 2450, 198, 256, 482, 3146, 6531, 66000, 68063, 17829, 44483, 11, 2626, 7980, 11, 62542, 15749, 271, 14711, 220, 21, 13, 3146, 3936, 25, 330, 791, 5843, 9469, 4289, 39195, 315, 93696, 304, 26029, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 7720, 11, 7528, 8661, 11, 2626, 6650, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 6020, 5597, 76170, 271, 14711, 220, 22, 13, 3146, 3936, 25, 330, 6102, 318, 4954, 51752, 449, 2956, 12, 99584, 18729, 56619, 1, 1035, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 51752, 11, 8661, 15174, 11, 28975, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 271, 14711, 220, 23, 13, 3146, 3936, 25, 330, 4438, 61844, 8871, 8471, 3053, 15583, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 68063, 61844, 8871, 3600, 11, 2626, 18475, 11, 8871, 10105, 11, 13312, 1862, 11, 26206, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 6278, 3157, 40919, 271, 14711, 220, 24, 13, 3146, 3936, 25, 330, 53463, 287, 4718, 9983, 7972, 79723, 25, 33257, 26788, 369, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 9983, 3772, 9546, 11, 6335, 10631, 11, 220, 2366, 18, 3674, 3772, 11, 2626, 20392, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 9983, 3772, 20258, 11, 2678, 2626, 7980, 11, 8661, 50184, 271, 14711, 220, 605, 13, 3146, 3936, 25, 330, 791, 29680, 315, 62593, 7127, 389, 4718, 26029, 83899, 1, 1035, 262, 482, 3146, 44942, 68063, 62593, 2955, 11, 26029, 33407, 11, 5536, 389, 26029, 11, 1217, 3217, 11, 3997, 26329, 198, 262, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 26029, 35416, 11, 7528, 55572, 11, 2626, 7980, 271, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 6319, 311, 5398, 449, 279, 6332, 4634, 6072, 323, 3600, 315, 97693, 5364, 847, 11880, 4340, 13, 3296, 52913, 9959, 21513, 11, 1521, 8158, 690, 9504, 279, 1314, 10877, 323, 6678, 23222, 20392, 13, 9062, 8712, 374, 17550, 7119, 264, 3230, 38462, 11125, 26793, 1139, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 11, 47594, 279, 2816, 596, 26029, 5178, 323, 1217, 20392, 14781, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 7275, 2929, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\nAlright, I need to figure out which tool to use based on the user\'s query. The user is asking to scrape a website and generate blog post topics, keywords, and target audience. Looking at the available tools, there\'s "web_scrape" which is designed to scrape and process web pages. That seems perfect for this task.\n\nI should choose the "web_scrape" tool and provide the URL the user specified. So the tool call will include the name "web_scrape" and the parameter "url" set to "https://elevated-marketing.io/".\n\nI think that\'s all. No other tool fits this requirement, so the response will only include this one tool call.\n</think>\n\n{\n  "tool_calls": [\n    {"name": "web_scrape", "parameters": {"url": "https://elevated-marketing.io/"}}\n  ]\n}', 'output_ids': [30004, 11880, 4340, 3747, 128012, 128013, 198, 72586, 11, 358, 1205, 311, 7216, 704, 902, 5507, 311, 1005, 3196, 389, 279, 1217, 596, 3319, 13, 578, 1217, 374, 10371, 311, 58228, 264, 3997, 323, 7068, 5117, 1772, 13650, 11, 21513, 11, 323, 2218, 10877, 13, 21815, 520, 279, 2561, 7526, 11, 1070, 596, 330, 2984, 13473, 20432, 1, 902, 374, 6319, 311, 58228, 323, 1920, 3566, 6959, 13, 3011, 5084, 4832, 369, 420, 3465, 382, 40, 1288, 5268, 279, 330, 2984, 13473, 20432, 1, 5507, 323, 3493, 279, 5665, 279, 1217, 5300, 13, 2100, 279, 5507, 1650, 690, 2997, 279, 836, 330, 2984, 13473, 20432, 1, 323, 279, 5852, 330, 1103, 1, 743, 311, 330, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 11474, 382, 40, 1781, 430, 596, 682, 13, 2360, 1023, 5507, 18809, 420, 16686, 11, 779, 279, 2077, 690, 1193, 2997, 420, 832, 5507, 1650, 627, 128014, 271, 517, 220, 330, 14506, 46736, 794, 2330, 262, 5324, 609, 794, 330, 2984, 13473, 20432, 498, 330, 14105, 794, 5324, 1103, 794, 330, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 11474, 11498, 220, 5243, 92, 128001], 'meta_info': {'id': 'b5cec747ef8a41acb10587daa98df1c8', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 1753, 'weight_version': 'default', 'completion_tokens': 183, 'cached_tokens': 0, 'e2e_latency': 6.456451177597046}}
[2025-10-13 06:55:51] INFO:     127.0.0.1:41346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 06:55:56] INFO:     127.0.0.1:58988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 06:55:56] Receive: obj=GenerateReqInput(rid='72e9889dff97465db596bb2f303eb636', text=None, input_ids=[128000, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 128012, 198, 14711, 14496, 3962, 41994, 11, 56795, 11, 323, 13791, 66000, 271, 29815, 389, 279, 2262, 323, 5357, 315, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 6848, 41891, 369, 26029, 1888, 12659, 323, 6319, 311, 9504, 28849, 323, 20392, 1473, 45464, 14711, 220, 16, 13, 3146, 27504, 25, 330, 605, 48833, 26029, 56619, 311, 55664, 349, 4718, 8184, 304, 220, 2366, 19, 1, 1035, 256, 482, 3146, 44942, 96618, 26029, 15174, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 10631, 11, 220, 2366, 19, 18845, 627, 256, 482, 3146, 54049, 1873, 96618, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 323, 384, 42685, 35490, 3411, 311, 7417, 872, 2930, 9546, 382, 45464, 14711, 220, 17, 13, 3146, 27504, 25, 330, 57815, 655, 17476, 220, 4645, 25, 2650, 311, 34763, 4718, 8184, 505, 14434, 53435, 82, 1, 1035, 256, 482, 3146, 44942, 96618, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 627, 256, 482, 3146, 54049, 1873, 96618, 8184, 7980, 11, 8871, 20258, 11, 323, 44483, 11920, 922, 41577, 872, 7528, 12032, 382, 45464, 14711, 220, 18, 13, 3146, 27504, 25, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 19, 1, 1035, 256, 482, 3146, 44942, 96618, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 19, 8661, 11, 15592, 18052, 11, 2626, 6650, 627, 256, 482, 3146, 54049, 1873, 96618, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 323, 9721, 3046, 304, 279, 7528, 3634, 382, 45464, 14711, 220, 19, 13, 3146, 27504, 25, 330, 4438, 311, 8012, 264, 27191, 16835, 15457, 17331, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 96618, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 627, 256, 482, 3146, 54049, 1873, 96618, 42759, 16941, 1759, 11, 8661, 7411, 11, 323, 6883, 20258, 10968, 389, 4857, 264, 29722, 6883, 9764, 382, 45464, 14711, 220, 20, 13, 3146, 27504, 25, 330, 5479, 220, 20, 5000, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 1, 1035, 256, 482, 3146, 44942, 96618, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 627, 256, 482, 3146, 54049, 1873, 96618, 16406, 7980, 11, 384, 42685, 9873, 11, 323, 3566, 26897, 3411, 311, 7417, 1217, 3217, 382, 45464, 14711, 220, 21, 13, 3146, 27504, 25, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 96618, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 627, 256, 482, 3146, 54049, 1873, 96618, 8184, 7980, 11, 8661, 20258, 11, 323, 5597, 76170, 44929, 922, 26029, 596, 51752, 382, 45464, 14711, 220, 22, 13, 3146, 27504, 25, 330, 20, 40961, 311, 4621, 1711, 4718, 16406, 30241, 34711, 53435, 82, 1, 1035, 256, 482, 3146, 44942, 96618, 16406, 4868, 11, 21516, 18208, 11, 62542, 7504, 11, 2930, 9313, 11, 2626, 7296, 627, 256, 482, 3146, 54049, 1873, 96618, 15344, 2626, 7980, 11, 3997, 38212, 11, 323, 8871, 15749, 382, 45464, 14711, 220, 23, 13, 3146, 27504, 25, 330, 4438, 311, 22991, 279, 10291, 14434, 18729, 16784, 369, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 96618, 14434, 8661, 9266, 11, 2626, 6650, 11, 8661, 8427, 11, 26029, 3600, 11, 8871, 10105, 627, 256, 482, 3146, 54049, 1873, 96618, 42759, 16941, 1759, 11, 2626, 7980, 11, 323, 55572, 3411, 311, 704, 2484, 872, 7528, 15174, 382, 45464, 14711, 220, 24, 13, 3146, 27504, 25, 330, 791, 94100, 315, 2956, 12, 99584, 41525, 25274, 304, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 96618, 2956, 32505, 8661, 11, 5597, 3339, 11, 7528, 15174, 11, 2626, 6650, 11, 28975, 627, 256, 482, 3146, 54049, 1873, 96618, 18729, 15749, 11, 2626, 31499, 11, 323, 828, 1355, 402, 14029, 35490, 382, 45464, 14711, 220, 605, 13, 3146, 27504, 25, 330, 75392, 26788, 369, 29757, 9151, 4718, 9983, 7972, 18729, 28845, 1, 1035, 256, 482, 3146, 44942, 96618, 9983, 3772, 8661, 11, 8446, 10631, 11, 2626, 6650, 11, 20392, 11, 7528, 8661, 627, 256, 482, 3146, 54049, 1873, 96618, 9983, 3772, 20258, 11, 2678, 2626, 7980, 11, 323, 55572, 3411, 311, 7417, 872, 3674, 3772, 9546, 382, 45464, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 6319, 311, 5398, 449, 279, 3600, 323, 19248, 315, 97693, 5364, 847, 11880, 4340, 1418, 25103, 3230, 30994, 449, 9959, 21513, 13, 3296, 21760, 389, 26029, 11, 62542, 11, 323, 7528, 8661, 11, 279, 2262, 690, 89986, 449, 2626, 7980, 323, 55572, 11125, 92178, 26793, 369, 6650, 13, 128001, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 128012, 198, 2, 14496, 3962, 41994, 369, 97693, 5364, 847, 11880, 4340, 271, 16, 13, 3146, 3936, 68063, 7054, 220, 605, 26029, 26788, 369, 15344, 71190, 304, 220, 2366, 18, 2355, 256, 482, 3146, 44942, 68063, 26029, 10631, 11, 2678, 9873, 11, 220, 2366, 18, 18845, 2355, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 64758, 7980, 19124, 17, 13, 3146, 3936, 68063, 2650, 311, 34763, 4718, 8184, 505, 34711, 53435, 82, 304, 220, 2366, 18, 2355, 256, 482, 3146, 44942, 68063, 8184, 9313, 11, 21516, 18208, 11, 2930, 4868, 2355, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 15749, 19124, 18, 13, 3146, 3936, 68063, 578, 15766, 315, 8871, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220] ... [3532, 349, 701, 7528, 9546, 2930, 271, 8906, 603, 3432, 369, 264, 12929, 198, 28914, 271, 36, 3532, 349, 271, 456, 311, 1440, 603, 271, 36, 3532, 660, 5364, 847, 11880, 25, 4718, 32413, 304, 14434, 58240, 627, 9411, 72637, 50779, 4718, 16835, 596, 8797, 55057, 449, 97693, 5364, 847, 11880, 271, 1688, 97693, 5364, 847, 11880, 11, 584, 4510, 430, 1475, 6883, 706, 264, 5016, 3446, 8748, 311, 387, 3309, 13, 5751, 11782, 15174, 323, 86282, 54077, 690, 1520, 499, 4667, 449, 701, 10877, 1093, 2646, 1603, 382, 827, 14434, 8661, 271, 1041, 15804, 48341, 46899, 198, 70143, 27414, 7972, 18729, 612, 47423, 271, 47743, 7224, 271, 1816, 35883, 198, 5272, 10669, 9, 256, 2052, 7, 22, 340, 9, 256, 16835, 287, 7, 20, 340, 9, 256, 34711, 8398, 7, 18, 340, 9, 256, 41147, 7, 18, 340, 9, 256, 7694, 8364, 58839, 7, 20, 340, 9, 256, 5000, 7127, 7, 21, 696, 21362, 1945, 220, 16, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 4119, 14148, 2595, 21362, 1945, 220, 17, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 6726, 15189, 2595, 21362, 1945, 220, 18, 25, 86811, 2712, 10766, 47049, 5487, 73, 88, 27515, 22, 39911, 88, 27515, 22, 39911, 85514, 2595, 21362, 1945, 220, 19, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 931, 23929, 2595, 21362, 1945, 220, 20, 25, 220, 21251, 21033, 16, 65, 1481, 64, 6365, 12, 19, 544, 19, 12, 24, 66, 24, 68, 7561, 291, 15, 77576, 19711, 15, 68, 2595, 21362, 1945, 220, 21, 25, 53873, 320, 16, 28871, 21362, 1945, 220, 22, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 931, 9756, 2595, 11296, 11302, 271, 33947, 97693, 5364, 847, 11880, 6444, 512, 1434, 45464, 1, 35078, 449, 20851, 445, 3550, 574, 459, 10973, 1847, 11843, 4091, 369, 1057, 85243, 2626, 13, 5414, 26029, 15174, 3952, 603, 505, 7600, 3750, 311, 279, 1948, 315, 5195, 2778, 3135, 304, 271, 827, 21077, 26224, 271, 13084, 271, 48240, 279, 6506, 315, 264, 13312, 21210, 11, 62542, 374, 2744, 520, 279, 52301, 315, 856, 4059, 13, 42608, 539, 1193, 27528, 1057, 3997, 2403, 4754, 18208, 719, 1101, 271, 827, 8096, 11605, 271, 13084, 271, 1, 48341, 46899, 706, 1027, 459, 10973, 9513, 311, 1057, 8661, 9045, 13, 5414, 11782, 15174, 369, 3674, 3772, 20392, 617, 9087, 603, 5662, 264, 1790, 22622, 10877, 11, 323, 271, 827, 35266, 17200, 271, 13084, 271, 1, 35078, 449, 20851, 445, 3550, 574, 459, 10973, 1847, 11843, 4091, 369, 1057, 85243, 2626, 13, 5414, 26029, 15174, 3952, 603, 505, 7600, 3750, 311, 279, 1948, 315, 5195, 2778, 3135, 304, 271, 827, 21077, 26224, 271, 13084, 271, 48240, 279, 6506, 315, 264, 13312, 21210, 11, 62542, 374, 2744, 520, 279, 52301, 315, 856, 4059, 13, 42608, 539, 1193, 27528, 1057, 3997, 2403, 4754, 18208, 719, 1101, 271, 827, 8096, 11605, 271, 13084, 271, 1, 48341, 46899, 706, 1027, 459, 10973, 9513, 311, 1057, 8661, 9045, 13, 5414, 11782, 15174, 369, 3674, 3772, 20392, 617, 9087, 603, 5662, 264, 1790, 22622, 10877, 11, 323, 271, 827, 35266, 17200, 271, 13084, 271, 1, 35078, 449, 20851, 445, 3550, 574, 459, 10973, 1847, 11843, 4091, 369, 1057, 85243, 2626, 13, 5414, 26029, 15174, 3952, 603, 505, 7600, 3750, 311, 279, 1948, 315, 5195, 2778, 3135, 304, 271, 827, 21077, 26224, 271, 13084, 271, 10348, 7720, 271, 10445, 1472, 1288, 5268, 97693, 5364, 847, 11880, 5380, 1434, 70143, 8140, 19248, 45395, 5370, 5151, 2737, 26029, 3600, 11, 3566, 4500, 11, 62542, 11, 8871, 14054, 11, 828, 3133, 6373, 11, 8661, 15174, 11, 323, 15592, 18052, 13, 1226, 527, 11411, 311, 10043, 6650, 323, 47594, 20392, 1555, 18699, 7528, 10105, 382, 21362, 1945, 220, 23, 25, 4758, 315, 97693, 18729, 596, 8871, 323, 26029, 10105, 430, 18885, 323, 69730, 16097, 36284, 9, 256, 19758, 11278, 271, 9, 256, 96658, 9365, 271, 9, 256, 3799, 3536, 315, 21460, 271, 9, 256, 18591, 2999, 2116, 26891, 271, 1687, 527, 5644, 311, 55664, 349, 271, 701, 2626, 198, 1434, 72637, 41651, 6335, 1274, 271, 414, 2128, 3697, 198, 776, 271, 9, 256, 26029, 5664, 198, 9, 256, 18729, 56619, 198, 9, 256, 34711, 17476, 271, 9, 256, 27370, 33257, 26029, 23508, 271, 85131, 927, 1403, 11026, 315, 3217, 304, 26029, 11, 97693, 5364, 847, 11880, 28421, 17033, 15174, 430, 6678, 17808, 9629, 323, 7916, 701, 2930, 9546, 1210, 720, 9, 256, 27370, 2956, 12, 99584, 18591, 271, 52825, 828, 32505, 5603, 26420, 5190, 2778, 4817, 33407, 11, 51647, 810, 15337, 11767, 323, 7859, 701, 6130, 2385, 1210, 4815, 14711, 1226, 617, 927, 220, 508, 1667, 315, 3217, 271, 58, 4518, 4497, 2595, 21362, 1945, 220, 24, 25, 97693, 18729, 12708, 67908, 6883, 27255, 15174, 323, 6335, 8871, 612, 26029, 3600, 369, 2626, 6650, 36284, 9, 256, 27370, 16542, 27697, 60463, 271, 78299, 3532, 349, 701, 6883, 449, 97693, 5364, 847, 11880, 596, 18699, 8661, 15174, 11, 6319, 311, 35608, 20392, 323, 6678, 6650, 4028, 5361, 19647, 1210, 720, 9, 256, 27370, 13620, 6427, 16835, 15457, 83, 6427, 271, 1, 3915, 86282, 9302, 2262, 311, 29722, 48929, 11, 1057, 8661, 2128, 28421, 3135, 430, 1977, 6883, 17985, 323, 31087, 6130, 32883, 1210, 4815, 14711, 1226, 617, 927, 220, 508, 1667, 315, 3217, 271, 58, 4518, 4497, 2595, 21362, 1945, 220, 605, 25, 68535, 24206, 315, 97693, 18729, 67908, 6883, 27255, 323, 6335, 8871, 612, 26029, 3600, 311, 18885, 2626, 24035, 36284, 9, 256, 27370, 1252, 37834, 34711, 19721, 271, 1, 62647, 701, 2930, 12032, 449, 97693, 5364, 847, 11880, 596, 22514, 62542, 10105, 11, 22542, 555, 12387, 46331, 220, 1041, 5220, 1210, 720, 9, 256, 27370, 1322, 3104, 8398, 73656, 271, 67410, 2642, 82971, 701, 3997, 2403, 4754, 18208, 323, 4774, 9096, 315, 4059, 449, 1057, 16195, 4868, 11193, 41891, 311, 701, 2626, 3966, 1210, 4815, 14711, 1226, 617, 927, 220, 972, 1667, 315, 3217, 271, 58, 4518, 4497, 2595, 21362, 1945, 220, 806, 25, 2418, 42118, 7528, 828, 389, 4264, 3345, 5352, 2595, 36, 3532, 349, 18729, 5918, 11951, 369, 701, 2626, 13, 510, 10086, 4718, 12761, 2595, 414, 5117, 8158, 271, 14710, 612, 29461, 198, 63832, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 06:55:56 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 06:55:57 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.02, #running-req: 0, #queue-req: 0, 
[2025-10-13 06:56:00 TP0] Prefill batch. #new-seq: 1, #new-token: 1926, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[2025-10-13 06:56:05 TP0] Decode batch. #running-req: 1, #token: 6041, token usage: 0.05, cuda graph: True, gen throughput (token/s): 2.90, #queue-req: 0, 
[2025-10-13 06:56:05 TP0] Decode batch. #running-req: 1, #token: 6081, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.69, #queue-req: 0, 
[2025-10-13 06:56:06 TP0] Decode batch. #running-req: 1, #token: 6121, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.86, #queue-req: 0, 
[2025-10-13 06:56:07 TP0] Decode batch. #running-req: 1, #token: 6161, token usage: 0.05, cuda graph: True, gen throughput (token/s): 52.07, #queue-req: 0, 
[2025-10-13 06:56:08 TP0] Decode batch. #running-req: 1, #token: 6201, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.97, #queue-req: 0, 
[2025-10-13 06:56:08 TP0] Decode batch. #running-req: 1, #token: 6241, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.77, #queue-req: 0, 
[2025-10-13 06:56:09 TP0] Decode batch. #running-req: 1, #token: 6281, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.86, #queue-req: 0, 
[2025-10-13 06:56:10 TP0] Decode batch. #running-req: 1, #token: 6321, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.88, #queue-req: 0, 
[2025-10-13 06:56:11 TP0] Decode batch. #running-req: 1, #token: 6361, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.83, #queue-req: 0, 
[2025-10-13 06:56:12 TP0] Decode batch. #running-req: 1, #token: 6401, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.74, #queue-req: 0, 
[2025-10-13 06:56:12 TP0] Decode batch. #running-req: 1, #token: 6441, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.81, #queue-req: 0, 
[2025-10-13 06:56:13 TP0] Decode batch. #running-req: 1, #token: 6481, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.76, #queue-req: 0, 
[2025-10-13 06:56:14 TP0] Decode batch. #running-req: 1, #token: 6521, token usage: 0.05, cuda graph: True, gen throughput (token/s): 51.75, #queue-req: 0, 
[2025-10-13 06:56:15 TP0] Decode batch. #running-req: 1, #token: 6561, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.77, #queue-req: 0, 
[2025-10-13 06:56:15 TP0] Decode batch. #running-req: 1, #token: 6601, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.78, #queue-req: 0, 
[2025-10-13 06:56:16 TP0] Decode batch. #running-req: 1, #token: 6641, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.72, #queue-req: 0, 
[2025-10-13 06:56:17 TP0] Decode batch. #running-req: 1, #token: 6681, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.74, #queue-req: 0, 
[2025-10-13 06:56:18 TP0] Decode batch. #running-req: 1, #token: 6721, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.74, #queue-req: 0, 
[2025-10-13 06:56:18 TP0] Decode batch. #running-req: 1, #token: 6761, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.77, #queue-req: 0, 
[2025-10-13 06:56:19 TP0] Decode batch. #running-req: 1, #token: 6801, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.68, #queue-req: 0, 
[2025-10-13 06:56:20 TP0] Decode batch. #running-req: 1, #token: 6841, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.67, #queue-req: 0, 
[2025-10-13 06:56:21 TP0] Decode batch. #running-req: 1, #token: 6881, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.72, #queue-req: 0, 
[2025-10-13 06:56:22 TP0] Decode batch. #running-req: 1, #token: 6921, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.68, #queue-req: 0, 
[2025-10-13 06:56:22 TP0] Decode batch. #running-req: 1, #token: 6961, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.63, #queue-req: 0, 
[2025-10-13 06:56:23 TP0] Decode batch. #running-req: 1, #token: 7001, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.61, #queue-req: 0, 
[2025-10-13 06:56:24 TP0] Decode batch. #running-req: 1, #token: 7041, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.68, #queue-req: 0, 
[2025-10-13 06:56:25 TP0] Decode batch. #running-req: 1, #token: 7081, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.57, #queue-req: 0, 
[2025-10-13 06:56:25 TP0] Decode batch. #running-req: 1, #token: 7121, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.60, #queue-req: 0, 
[2025-10-13 06:56:26 TP0] Decode batch. #running-req: 1, #token: 7161, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.64, #queue-req: 0, 
[2025-10-13 06:56:26] Finish: obj=GenerateReqInput(rid='72e9889dff97465db596bb2f303eb636', text=None, input_ids=[128000, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 128012, 198, 14711, 14496, 3962, 41994, 11, 56795, 11, 323, 13791, 66000, 271, 29815, 389, 279, 2262, 323, 5357, 315, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 6848, 41891, 369, 26029, 1888, 12659, 323, 6319, 311, 9504, 28849, 323, 20392, 1473, 45464, 14711, 220, 16, 13, 3146, 27504, 25, 330, 605, 48833, 26029, 56619, 311, 55664, 349, 4718, 8184, 304, 220, 2366, 19, 1, 1035, 256, 482, 3146, 44942, 96618, 26029, 15174, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 10631, 11, 220, 2366, 19, 18845, 627, 256, 482, 3146, 54049, 1873, 96618, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 323, 384, 42685, 35490, 3411, 311, 7417, 872, 2930, 9546, 382, 45464, 14711, 220, 17, 13, 3146, 27504, 25, 330, 57815, 655, 17476, 220, 4645, 25, 2650, 311, 34763, 4718, 8184, 505, 14434, 53435, 82, 1, 1035, 256, 482, 3146, 44942, 96618, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 627, 256, 482, 3146, 54049, 1873, 96618, 8184, 7980, 11, 8871, 20258, 11, 323, 44483, 11920, 922, 41577, 872, 7528, 12032, 382, 45464, 14711, 220, 18, 13, 3146, 27504, 25, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 19, 1, 1035, 256, 482, 3146, 44942, 96618, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 19, 8661, 11, 15592, 18052, 11, 2626, 6650, 627, 256, 482, 3146, 54049, 1873, 96618, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 323, 9721, 3046, 304, 279, 7528, 3634, 382, 45464, 14711, 220, 19, 13, 3146, 27504, 25, 330, 4438, 311, 8012, 264, 27191, 16835, 15457, 17331, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 96618, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 627, 256, 482, 3146, 54049, 1873, 96618, 42759, 16941, 1759, 11, 8661, 7411, 11, 323, 6883, 20258, 10968, 389, 4857, 264, 29722, 6883, 9764, 382, 45464, 14711, 220, 20, 13, 3146, 27504, 25, 330, 5479, 220, 20, 5000, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 1, 1035, 256, 482, 3146, 44942, 96618, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 627, 256, 482, 3146, 54049, 1873, 96618, 16406, 7980, 11, 384, 42685, 9873, 11, 323, 3566, 26897, 3411, 311, 7417, 1217, 3217, 382, 45464, 14711, 220, 21, 13, 3146, 27504, 25, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 96618, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 627, 256, 482, 3146, 54049, 1873, 96618, 8184, 7980, 11, 8661, 20258, 11, 323, 5597, 76170, 44929, 922, 26029, 596, 51752, 382, 45464, 14711, 220, 22, 13, 3146, 27504, 25, 330, 20, 40961, 311, 4621, 1711, 4718, 16406, 30241, 34711, 53435, 82, 1, 1035, 256, 482, 3146, 44942, 96618, 16406, 4868, 11, 21516, 18208, 11, 62542, 7504, 11, 2930, 9313, 11, 2626, 7296, 627, 256, 482, 3146, 54049, 1873, 96618, 15344, 2626, 7980, 11, 3997, 38212, 11, 323, 8871, 15749, 382, 45464, 14711, 220, 23, 13, 3146, 27504, 25, 330, 4438, 311, 22991, 279, 10291, 14434, 18729, 16784, 369, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 96618, 14434, 8661, 9266, 11, 2626, 6650, 11, 8661, 8427, 11, 26029, 3600, 11, 8871, 10105, 627, 256, 482, 3146, 54049, 1873, 96618, 42759, 16941, 1759, 11, 2626, 7980, 11, 323, 55572, 3411, 311, 704, 2484, 872, 7528, 15174, 382, 45464, 14711, 220, 24, 13, 3146, 27504, 25, 330, 791, 94100, 315, 2956, 12, 99584, 41525, 25274, 304, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 96618, 2956, 32505, 8661, 11, 5597, 3339, 11, 7528, 15174, 11, 2626, 6650, 11, 28975, 627, 256, 482, 3146, 54049, 1873, 96618, 18729, 15749, 11, 2626, 31499, 11, 323, 828, 1355, 402, 14029, 35490, 382, 45464, 14711, 220, 605, 13, 3146, 27504, 25, 330, 75392, 26788, 369, 29757, 9151, 4718, 9983, 7972, 18729, 28845, 1, 1035, 256, 482, 3146, 44942, 96618, 9983, 3772, 8661, 11, 8446, 10631, 11, 2626, 6650, 11, 20392, 11, 7528, 8661, 627, 256, 482, 3146, 54049, 1873, 96618, 9983, 3772, 20258, 11, 2678, 2626, 7980, 11, 323, 55572, 3411, 311, 7417, 872, 3674, 3772, 9546, 382, 45464, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 6319, 311, 5398, 449, 279, 3600, 323, 19248, 315, 97693, 5364, 847, 11880, 4340, 1418, 25103, 3230, 30994, 449, 9959, 21513, 13, 3296, 21760, 389, 26029, 11, 62542, 11, 323, 7528, 8661, 11, 279, 2262, 690, 89986, 449, 2626, 7980, 323, 55572, 11125, 92178, 26793, 369, 6650, 13, 128001, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 3747, 128012, 198, 2, 14496, 3962, 41994, 369, 97693, 5364, 847, 11880, 4340, 271, 16, 13, 3146, 3936, 68063, 7054, 220, 605, 26029, 26788, 369, 15344, 71190, 304, 220, 2366, 18, 2355, 256, 482, 3146, 44942, 68063, 26029, 10631, 11, 2678, 9873, 11, 220, 2366, 18, 18845, 2355, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 64758, 7980, 19124, 17, 13, 3146, 3936, 68063, 2650, 311, 34763, 4718, 8184, 505, 34711, 53435, 82, 304, 220, 2366, 18, 2355, 256, 482, 3146, 44942, 68063, 8184, 9313, 11, 21516, 18208, 11, 2930, 4868, 2355, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 15749, 19124, 18, 13, 3146, 3936, 68063, 578, 15766, 315, 8871, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220] ... [3532, 349, 701, 7528, 9546, 2930, 271, 8906, 603, 3432, 369, 264, 12929, 198, 28914, 271, 36, 3532, 349, 271, 456, 311, 1440, 603, 271, 36, 3532, 660, 5364, 847, 11880, 25, 4718, 32413, 304, 14434, 58240, 627, 9411, 72637, 50779, 4718, 16835, 596, 8797, 55057, 449, 97693, 5364, 847, 11880, 271, 1688, 97693, 5364, 847, 11880, 11, 584, 4510, 430, 1475, 6883, 706, 264, 5016, 3446, 8748, 311, 387, 3309, 13, 5751, 11782, 15174, 323, 86282, 54077, 690, 1520, 499, 4667, 449, 701, 10877, 1093, 2646, 1603, 382, 827, 14434, 8661, 271, 1041, 15804, 48341, 46899, 198, 70143, 27414, 7972, 18729, 612, 47423, 271, 47743, 7224, 271, 1816, 35883, 198, 5272, 10669, 9, 256, 2052, 7, 22, 340, 9, 256, 16835, 287, 7, 20, 340, 9, 256, 34711, 8398, 7, 18, 340, 9, 256, 41147, 7, 18, 340, 9, 256, 7694, 8364, 58839, 7, 20, 340, 9, 256, 5000, 7127, 7, 21, 696, 21362, 1945, 220, 16, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 4119, 14148, 2595, 21362, 1945, 220, 17, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 6726, 15189, 2595, 21362, 1945, 220, 18, 25, 86811, 2712, 10766, 47049, 5487, 73, 88, 27515, 22, 39911, 88, 27515, 22, 39911, 85514, 2595, 21362, 1945, 220, 19, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 931, 23929, 2595, 21362, 1945, 220, 20, 25, 220, 21251, 21033, 16, 65, 1481, 64, 6365, 12, 19, 544, 19, 12, 24, 66, 24, 68, 7561, 291, 15, 77576, 19711, 15, 68, 2595, 21362, 1945, 220, 21, 25, 53873, 320, 16, 28871, 21362, 1945, 220, 22, 25, 328, 32209, 220, 2366, 20, 12, 2705, 12, 2705, 220, 931, 9756, 2595, 11296, 11302, 271, 33947, 97693, 5364, 847, 11880, 6444, 512, 1434, 45464, 1, 35078, 449, 20851, 445, 3550, 574, 459, 10973, 1847, 11843, 4091, 369, 1057, 85243, 2626, 13, 5414, 26029, 15174, 3952, 603, 505, 7600, 3750, 311, 279, 1948, 315, 5195, 2778, 3135, 304, 271, 827, 21077, 26224, 271, 13084, 271, 48240, 279, 6506, 315, 264, 13312, 21210, 11, 62542, 374, 2744, 520, 279, 52301, 315, 856, 4059, 13, 42608, 539, 1193, 27528, 1057, 3997, 2403, 4754, 18208, 719, 1101, 271, 827, 8096, 11605, 271, 13084, 271, 1, 48341, 46899, 706, 1027, 459, 10973, 9513, 311, 1057, 8661, 9045, 13, 5414, 11782, 15174, 369, 3674, 3772, 20392, 617, 9087, 603, 5662, 264, 1790, 22622, 10877, 11, 323, 271, 827, 35266, 17200, 271, 13084, 271, 1, 35078, 449, 20851, 445, 3550, 574, 459, 10973, 1847, 11843, 4091, 369, 1057, 85243, 2626, 13, 5414, 26029, 15174, 3952, 603, 505, 7600, 3750, 311, 279, 1948, 315, 5195, 2778, 3135, 304, 271, 827, 21077, 26224, 271, 13084, 271, 48240, 279, 6506, 315, 264, 13312, 21210, 11, 62542, 374, 2744, 520, 279, 52301, 315, 856, 4059, 13, 42608, 539, 1193, 27528, 1057, 3997, 2403, 4754, 18208, 719, 1101, 271, 827, 8096, 11605, 271, 13084, 271, 1, 48341, 46899, 706, 1027, 459, 10973, 9513, 311, 1057, 8661, 9045, 13, 5414, 11782, 15174, 369, 3674, 3772, 20392, 617, 9087, 603, 5662, 264, 1790, 22622, 10877, 11, 323, 271, 827, 35266, 17200, 271, 13084, 271, 1, 35078, 449, 20851, 445, 3550, 574, 459, 10973, 1847, 11843, 4091, 369, 1057, 85243, 2626, 13, 5414, 26029, 15174, 3952, 603, 505, 7600, 3750, 311, 279, 1948, 315, 5195, 2778, 3135, 304, 271, 827, 21077, 26224, 271, 13084, 271, 10348, 7720, 271, 10445, 1472, 1288, 5268, 97693, 5364, 847, 11880, 5380, 1434, 70143, 8140, 19248, 45395, 5370, 5151, 2737, 26029, 3600, 11, 3566, 4500, 11, 62542, 11, 8871, 14054, 11, 828, 3133, 6373, 11, 8661, 15174, 11, 323, 15592, 18052, 13, 1226, 527, 11411, 311, 10043, 6650, 323, 47594, 20392, 1555, 18699, 7528, 10105, 382, 21362, 1945, 220, 23, 25, 4758, 315, 97693, 18729, 596, 8871, 323, 26029, 10105, 430, 18885, 323, 69730, 16097, 36284, 9, 256, 19758, 11278, 271, 9, 256, 96658, 9365, 271, 9, 256, 3799, 3536, 315, 21460, 271, 9, 256, 18591, 2999, 2116, 26891, 271, 1687, 527, 5644, 311, 55664, 349, 271, 701, 2626, 198, 1434, 72637, 41651, 6335, 1274, 271, 414, 2128, 3697, 198, 776, 271, 9, 256, 26029, 5664, 198, 9, 256, 18729, 56619, 198, 9, 256, 34711, 17476, 271, 9, 256, 27370, 33257, 26029, 23508, 271, 85131, 927, 1403, 11026, 315, 3217, 304, 26029, 11, 97693, 5364, 847, 11880, 28421, 17033, 15174, 430, 6678, 17808, 9629, 323, 7916, 701, 2930, 9546, 1210, 720, 9, 256, 27370, 2956, 12, 99584, 18591, 271, 52825, 828, 32505, 5603, 26420, 5190, 2778, 4817, 33407, 11, 51647, 810, 15337, 11767, 323, 7859, 701, 6130, 2385, 1210, 4815, 14711, 1226, 617, 927, 220, 508, 1667, 315, 3217, 271, 58, 4518, 4497, 2595, 21362, 1945, 220, 24, 25, 97693, 18729, 12708, 67908, 6883, 27255, 15174, 323, 6335, 8871, 612, 26029, 3600, 369, 2626, 6650, 36284, 9, 256, 27370, 16542, 27697, 60463, 271, 78299, 3532, 349, 701, 6883, 449, 97693, 5364, 847, 11880, 596, 18699, 8661, 15174, 11, 6319, 311, 35608, 20392, 323, 6678, 6650, 4028, 5361, 19647, 1210, 720, 9, 256, 27370, 13620, 6427, 16835, 15457, 83, 6427, 271, 1, 3915, 86282, 9302, 2262, 311, 29722, 48929, 11, 1057, 8661, 2128, 28421, 3135, 430, 1977, 6883, 17985, 323, 31087, 6130, 32883, 1210, 4815, 14711, 1226, 617, 927, 220, 508, 1667, 315, 3217, 271, 58, 4518, 4497, 2595, 21362, 1945, 220, 605, 25, 68535, 24206, 315, 97693, 18729, 67908, 6883, 27255, 323, 6335, 8871, 612, 26029, 3600, 311, 18885, 2626, 24035, 36284, 9, 256, 27370, 1252, 37834, 34711, 19721, 271, 1, 62647, 701, 2930, 12032, 449, 97693, 5364, 847, 11880, 596, 22514, 62542, 10105, 11, 22542, 555, 12387, 46331, 220, 1041, 5220, 1210, 720, 9, 256, 27370, 1322, 3104, 8398, 73656, 271, 67410, 2642, 82971, 701, 3997, 2403, 4754, 18208, 323, 4774, 9096, 315, 4059, 449, 1057, 16195, 4868, 11193, 41891, 311, 701, 2626, 3966, 1210, 4815, 14711, 1226, 617, 927, 220, 972, 1667, 315, 3217, 271, 58, 4518, 4497, 2595, 21362, 1945, 220, 806, 25, 2418, 42118, 7528, 828, 389, 4264, 3345, 5352, 2595, 36, 3532, 349, 18729, 5918, 11951, 369, 701, 2626, 13, 510, 10086, 4718, 12761, 2595, 414, 5117, 8158, 271, 14710, 612, 29461, 198, 63832, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "<think>\nAlright, let's try to figure out what the user is asking for here. They provided a history of interactions where they requested blog post topics based on a website scrape. The user seems to be involved in digital marketing, possibly working for Elevated-Marketing.io, and they want to generate content that's optimized for SEO and will perform well in search results.\n\nIn the current query, they mention scraping the website and generating 10 blog post topics, keywords, and target audiences, keeping SEO and Google algorithms in mind. The website is Elevated-Marketing.io, which offers services like SEO, digital marketing, IT solutions, and cybersecurity.\n\nLooking at the provided context, it includes a lot of information about the website, including service offerings, customer testimonials, and team expertise. My task is to generate relevant blog topics that align with their services and cater to their target audience.\n\nFirst, I'll identify the main areas the website focuses on: SEO, cybersecurity, digital " ... 'ed IT services, business transformation, IT solutions, tech support, productivity\n   - **Target Audience:** Business owners, IT managers, medium-sized businesses\n\n---\n\n### 9. **Title:** "The Importance of Brand Storytelling in Digital Marketing"\n   - **Keywords:** Brand storytelling, digital marketing, brand awareness, business growth, marketing strategies\n   - **Target Audience:** Entrepreneurs, marketing teams, brand managers\n\n---\n\n### 10. **Title:** "The Impact of UX Design on Your SEO Rankings"\n    - **Keywords:** UX design, SEO rankings, impact on SEO, user experience, website optimization\n    - **Target Audience:** Web designers, SEO specialists, digital marketers, business owners\n\n---\n\n### Summary:\nThese blog post topics are crafted to align with the services and expertise of Elevated-Marketing.io, incorporating relevant keywords to improve SEO performance and attract the right audience. Each topic targets specific pain points or interests of potential readers, ensuring engaging and actionable content.', 'output_ids': [128001], 'meta_info': {'id': '72e9889dff97465db596bb2f303eb636', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 6023, 'weight_version': 'default', 'completion_tokens': 1142, 'cached_tokens': 1, 'e2e_latency': 30.144451379776}}
[2025-10-13 06:56:26] Receive: obj=GenerateReqInput(rid='3f5e99ad72d6428fb6dcc35857887864', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 1784, 5045, 3931, 2891, 25, 20817, 389, 279, 2262, 88937, 505, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 13650, 11, 21513, 11, 323, 2218, 30994, 29068, 291, 311, 26029, 1888, 12659, 11, 5195, 596, 12384, 11, 323, 88278, 2778, 28849, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 25, 330, 5479, 220, 605, 26029, 56619, 311, 34507, 4718, 8184, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 10631, 11, 220, 2366, 18, 18845, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 384, 42685, 35490, 271, 45464, 14711, 220, 17, 13, 3146, 3936, 25, 330, 4438, 311, 35602, 4718, 8184, 30241, 34711, 53435, 82, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 44483, 11920, 922, 7528, 4868, 271, 45464, 14711, 220, 18, 13, 3146, 3936, 25, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 18, 8661, 11, 15592, 18052, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 9721, 3046, 304, 279, 7528, 3634, 271, 45464, 14711, 220, 19, 13, 3146, 3936, 25, 330, 6109, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 1, 1035, 256, 482, 3146, 44942, 68063, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 16406, 7980, 11, 384, 42685, 9873, 11, 3566, 26897, 10968, 389, 18899, 1217, 3217, 271, 45464, 14711, 220, 20, 13, 3146, 3936, 25, 330, 4301, 19723, 25, 2650, 97693, 5364, 847, 11880, 34507, 291, 264, 17829, 39514, 596, 34711, 17476, 1, 1035, 256, 482, 3146, 44942, 68063, 11799, 4007, 11, 62542, 2450, 11, 21210, 6650, 11, 97693, 5364, 847, 11880, 11, 2626, 1162, 198, 256, 482, 3146, 6531, 66000, 68063, 17829, 44483, 11, 5597, 76170, 11, 9873, 3411, 369, 62542, 10105, 271, 45464, 14711, 220, 21, 13, 3146, 3936, 25, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 8184, 13346, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 5597, 76170, 44929, 922, 26029, 753, 51752, 271, 45464, 14711, 220, 22, 13, 3146, 3936, 25, 330, 1061, 12, 99584, 18729, 25, 2650, 33527, 3053, 15583, 4718, 28845, 1, 1035, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 28975, 11, 8446, 18475, 11, 2626, 6650, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 3411, 311, 33164, 828, 271, 45464, 14711, 220, 23, 13, 3146, 3936, 25, 330, 791, 7572, 315, 16835, 15457, 83, 6427, 304, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 68063, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 8661, 7411, 11, 6883, 20258, 10968, 389, 4857, 264, 29722, 198, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 1784, 5045, 3931, 2891, 25, 23371, 0, 2057, 7068, 7701, 9959, 323, 7524, 5117, 1772, 13650, 369, 97693, 5364, 847, 11880, 4340, 11, 584, 690, 5357, 389, 279, 6332, 3600, 323, 94436, 27463, 389, 872, 3997, 25, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 13, 5810, 527, 220, 605, 29722, 5117, 1772, 13650, 11, 3235, 449, 872, 20081, 21513, 323, 2218, 30994, 1473, 14711, 220, 16, 13, 3146, 3936, 25, 330, 11060, 287, 26029, 304, 220, 2366, 18, 25, 7054, 56619, 369, 15344, 71190, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2678, 9873, 11, 220, 2366, 18, 26029, 11, 2778, 4817, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 2626, 7980, 11, 7528, 55572, 11, 35490, 271, 14711] ... [271, 14711, 220, 23, 13, 3146, 3936, 25, 330, 4438, 61844, 8871, 8471, 3053, 15583, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 68063, 61844, 8871, 3600, 11, 2626, 18475, 11, 8871, 10105, 11, 13312, 1862, 11, 26206, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 6278, 3157, 40919, 271, 14711, 220, 24, 13, 3146, 3936, 25, 330, 53463, 287, 4718, 9983, 7972, 79723, 25, 33257, 26788, 369, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 9983, 3772, 9546, 11, 6335, 10631, 11, 220, 2366, 18, 3674, 3772, 11, 2626, 20392, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 9983, 3772, 20258, 11, 2678, 2626, 7980, 11, 8661, 50184, 271, 14711, 220, 605, 13, 3146, 3936, 25, 330, 791, 29680, 315, 62593, 7127, 389, 4718, 26029, 83899, 1, 1035, 262, 482, 3146, 44942, 68063, 62593, 2955, 11, 26029, 33407, 11, 5536, 389, 26029, 11, 1217, 3217, 11, 3997, 26329, 198, 262, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 26029, 35416, 11, 7528, 55572, 11, 2626, 7980, 271, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 6319, 311, 5398, 449, 279, 6332, 4634, 6072, 323, 3600, 315, 97693, 5364, 847, 11880, 4340, 13, 3296, 52913, 9959, 21513, 11, 1521, 8158, 690, 9504, 279, 1314, 10877, 323, 6678, 23222, 20392, 13, 9062, 8712, 374, 17550, 7119, 264, 3230, 38462, 11125, 26793, 1139, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 11, 47594, 279, 2816, 596, 26029, 5178, 323, 1217, 20392, 627, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 1784, 5045, 3931, 2891, 25, 20817, 389, 279, 2262, 505, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 13650, 11, 21513, 11, 323, 2218, 30994, 6319, 311, 5398, 449, 26029, 1888, 12659, 323, 9504, 9959, 13016, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 605, 48833, 26029, 26788, 311, 55664, 349, 4718, 8184, 304, 220, 2366, 18, 702, 256, 482, 3146, 44942, 68063, 26029, 10631, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 15174, 11, 220, 2366, 18, 18845, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 384, 42685, 35490, 271, 45464, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 4438, 311, 34763, 4718, 8184, 505, 34711, 53435, 82, 304, 220, 2366, 18, 702, 256, 482, 3146, 44942, 68063, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 44483, 11920, 922, 7528, 4868, 271, 45464, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 18, 702, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 18, 8661, 11, 15592, 18052, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 9721, 3046, 304, 279, 7528, 3634, 271, 45464, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 5479, 5000, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 702, 256, 482, 3146, 44942, 68063, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 16406, 7980, 11, 384, 42685, 9873, 11, 3566, 26897, 271, 45464, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 4438, 311, 22991, 279, 10291, 14434, 18729, 16784, 369, 4718, 8184, 702, 256, 482, 3146, 44942, 68063, 14434, 8661, 9266, 11, 2626, 6650, 11, 8661, 8427, 11, 26029, 3600, 11, 8871, 10105, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 2626, 7980, 11, 323, 55572, 3411, 311, 704, 2484, 872, 7528, 15174, 271, 45464, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 4718, 8184, 702, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 5597, 76170, 44929, 922, 26029, 753, 51752, 271, 45464, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 6102, 318, 4954, 51752, 449, 2956, 12, 99584, 18729, 56619, 702, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 51752, 11, 8661, 15174, 11, 28975, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 271, 45464, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 4438, 61844, 8871, 8471, 3053, 15583, 4718, 8184, 702, 256, 482, 3146, 44942, 68063, 61844, 8871, 3600, 11, 2626, 18475, 11, 8871, 10105, 11, 13312, 1862, 11, 26206, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 11298, 28935, 9873, 271, 45464, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 791, 94100, 315, 16835, 15457, 83, 6427, 304, 14434, 18729, 702, 256, 482, 3146, 44942, 68063, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 8661, 7411, 11, 6883, 20258, 271, 45464, 14711, 220, 605, 13, 3146, 3936, 68063, 330, 791, 29680, 315, 62593, 7127, 389, 4718, 26029, 83899, 702, 262, 482, 3146, 44942, 68063, 62593, 2955, 11, 26029, 33407, 11, 5536, 389, 26029, 11, 1217, 3217, 11, 3997, 26329, 198, 262, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 26029, 35416, 11, 7528, 55572, 11, 2626, 7980, 271, 45464, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 34558, 311, 5398, 449, 279, 3600, 323, 19248, 315, 97693, 5364, 847, 11880, 4340, 11, 52913, 9959, 21513, 311, 7417, 26029, 5178, 323, 9504, 279, 1314, 10877, 13, 9062, 8712, 11811, 3230, 6784, 3585, 477, 12034, 315, 4754, 13016, 11, 23391, 23387, 323, 92178, 2262, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 06:56:26 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 06:56:27 TP0] Prefill batch. #new-seq: 1, #new-token: 342, #cached-token: 0, token usage: 0.02, #running-req: 0, #queue-req: 0, 
[2025-10-13 06:56:30 TP0] Decode batch. #running-req: 1, #token: 2428, token usage: 0.02, cuda graph: True, gen throughput (token/s): 10.04, #queue-req: 0, 
[2025-10-13 06:56:31 TP0] Decode batch. #running-req: 1, #token: 2468, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.97, #queue-req: 0, 
[2025-10-13 06:56:32 TP0] Decode batch. #running-req: 1, #token: 2508, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.97, #queue-req: 0, 
[2025-10-13 06:56:32 TP0] Decode batch. #running-req: 1, #token: 2548, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.76, #queue-req: 0, 
[2025-10-13 06:56:33 TP0] Decode batch. #running-req: 1, #token: 2588, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.90, #queue-req: 0, 
[2025-10-13 06:56:34 TP0] Decode batch. #running-req: 1, #token: 2628, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.04, #queue-req: 0, 
[2025-10-13 06:56:35 TP0] Decode batch. #running-req: 1, #token: 2668, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.01, #queue-req: 0, 
[2025-10-13 06:56:36 TP0] Decode batch. #running-req: 1, #token: 2708, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.85, #queue-req: 0, 
[2025-10-13 06:56:36 TP0] Decode batch. #running-req: 1, #token: 2748, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.95, #queue-req: 0, 
[2025-10-13 06:56:37 TP0] Decode batch. #running-req: 1, #token: 2788, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.96, #queue-req: 0, 
[2025-10-13 06:56:38 TP0] Decode batch. #running-req: 1, #token: 2828, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.95, #queue-req: 0, 
[2025-10-13 06:56:39 TP0] Decode batch. #running-req: 1, #token: 2868, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.95, #queue-req: 0, 
[2025-10-13 06:56:39 TP0] Decode batch. #running-req: 1, #token: 2908, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.89, #queue-req: 0, 
[2025-10-13 06:56:40] Finish: obj=GenerateReqInput(rid='3f5e99ad72d6428fb6dcc35857887864', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 1784, 5045, 3931, 2891, 25, 20817, 389, 279, 2262, 88937, 505, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 13650, 11, 21513, 11, 323, 2218, 30994, 29068, 291, 311, 26029, 1888, 12659, 11, 5195, 596, 12384, 11, 323, 88278, 2778, 28849, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 25, 330, 5479, 220, 605, 26029, 56619, 311, 34507, 4718, 8184, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 10631, 11, 220, 2366, 18, 18845, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 384, 42685, 35490, 271, 45464, 14711, 220, 17, 13, 3146, 3936, 25, 330, 4438, 311, 35602, 4718, 8184, 30241, 34711, 53435, 82, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 44483, 11920, 922, 7528, 4868, 271, 45464, 14711, 220, 18, 13, 3146, 3936, 25, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 18, 8661, 11, 15592, 18052, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 9721, 3046, 304, 279, 7528, 3634, 271, 45464, 14711, 220, 19, 13, 3146, 3936, 25, 330, 6109, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 1, 1035, 256, 482, 3146, 44942, 68063, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 16406, 7980, 11, 384, 42685, 9873, 11, 3566, 26897, 10968, 389, 18899, 1217, 3217, 271, 45464, 14711, 220, 20, 13, 3146, 3936, 25, 330, 4301, 19723, 25, 2650, 97693, 5364, 847, 11880, 34507, 291, 264, 17829, 39514, 596, 34711, 17476, 1, 1035, 256, 482, 3146, 44942, 68063, 11799, 4007, 11, 62542, 2450, 11, 21210, 6650, 11, 97693, 5364, 847, 11880, 11, 2626, 1162, 198, 256, 482, 3146, 6531, 66000, 68063, 17829, 44483, 11, 5597, 76170, 11, 9873, 3411, 369, 62542, 10105, 271, 45464, 14711, 220, 21, 13, 3146, 3936, 25, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 8184, 13346, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 5597, 76170, 44929, 922, 26029, 753, 51752, 271, 45464, 14711, 220, 22, 13, 3146, 3936, 25, 330, 1061, 12, 99584, 18729, 25, 2650, 33527, 3053, 15583, 4718, 28845, 1, 1035, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 28975, 11, 8446, 18475, 11, 2626, 6650, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 3411, 311, 33164, 828, 271, 45464, 14711, 220, 23, 13, 3146, 3936, 25, 330, 791, 7572, 315, 16835, 15457, 83, 6427, 304, 14434, 18729, 1, 1035, 256, 482, 3146, 44942, 68063, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 8661, 7411, 11, 6883, 20258, 10968, 389, 4857, 264, 29722, 198, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 1784, 5045, 3931, 2891, 25, 23371, 0, 2057, 7068, 7701, 9959, 323, 7524, 5117, 1772, 13650, 369, 97693, 5364, 847, 11880, 4340, 11, 584, 690, 5357, 389, 279, 6332, 3600, 323, 94436, 27463, 389, 872, 3997, 25, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 13, 5810, 527, 220, 605, 29722, 5117, 1772, 13650, 11, 3235, 449, 872, 20081, 21513, 323, 2218, 30994, 1473, 14711, 220, 16, 13, 3146, 3936, 25, 330, 11060, 287, 26029, 304, 220, 2366, 18, 25, 7054, 56619, 369, 15344, 71190, 1, 1035, 256, 482, 3146, 44942, 68063, 26029, 15174, 11, 2678, 9873, 11, 220, 2366, 18, 26029, 11, 2778, 4817, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 2626, 7980, 11, 7528, 55572, 11, 35490, 271, 14711] ... [271, 14711, 220, 23, 13, 3146, 3936, 25, 330, 4438, 61844, 8871, 8471, 3053, 15583, 4718, 8184, 1, 1035, 256, 482, 3146, 44942, 68063, 61844, 8871, 3600, 11, 2626, 18475, 11, 8871, 10105, 11, 13312, 1862, 11, 26206, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 6278, 3157, 40919, 271, 14711, 220, 24, 13, 3146, 3936, 25, 330, 53463, 287, 4718, 9983, 7972, 79723, 25, 33257, 26788, 369, 220, 2366, 18, 1, 1035, 256, 482, 3146, 44942, 68063, 9983, 3772, 9546, 11, 6335, 10631, 11, 220, 2366, 18, 3674, 3772, 11, 2626, 20392, 11, 7528, 8661, 198, 256, 482, 3146, 6531, 66000, 68063, 9983, 3772, 20258, 11, 2678, 2626, 7980, 11, 8661, 50184, 271, 14711, 220, 605, 13, 3146, 3936, 25, 330, 791, 29680, 315, 62593, 7127, 389, 4718, 26029, 83899, 1, 1035, 262, 482, 3146, 44942, 68063, 62593, 2955, 11, 26029, 33407, 11, 5536, 389, 26029, 11, 1217, 3217, 11, 3997, 26329, 198, 262, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 26029, 35416, 11, 7528, 55572, 11, 2626, 7980, 271, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 6319, 311, 5398, 449, 279, 6332, 4634, 6072, 323, 3600, 315, 97693, 5364, 847, 11880, 4340, 13, 3296, 52913, 9959, 21513, 11, 1521, 8158, 690, 9504, 279, 1314, 10877, 323, 6678, 23222, 20392, 13, 9062, 8712, 374, 17550, 7119, 264, 3230, 38462, 11125, 26793, 1139, 26029, 11, 62542, 11, 8871, 10105, 11, 323, 7528, 8661, 11, 47594, 279, 2816, 596, 26029, 5178, 323, 1217, 20392, 627, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 68, 3532, 660, 30004, 11880, 4340, 1784, 5045, 3931, 2891, 25, 20817, 389, 279, 2262, 505, 97693, 5364, 847, 11880, 4340, 11, 1618, 527, 220, 605, 5117, 1772, 13650, 11, 21513, 11, 323, 2218, 30994, 6319, 311, 5398, 449, 26029, 1888, 12659, 323, 9504, 9959, 13016, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 605, 48833, 26029, 26788, 311, 55664, 349, 4718, 8184, 304, 220, 2366, 18, 702, 256, 482, 3146, 44942, 68063, 26029, 10631, 11, 2626, 6650, 11, 7528, 8661, 11, 26029, 15174, 11, 220, 2366, 18, 18845, 198, 256, 482, 3146, 6531, 66000, 68063, 15344, 311, 11298, 28935, 2626, 7980, 11, 7528, 55572, 11, 384, 42685, 35490, 271, 45464, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 4438, 311, 34763, 4718, 8184, 505, 34711, 53435, 82, 304, 220, 2366, 18, 702, 256, 482, 3146, 44942, 68063, 34711, 17476, 11, 2626, 9313, 11, 7528, 18208, 11, 2930, 4868, 11, 828, 9313, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 44483, 11920, 922, 7528, 4868, 271, 45464, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 791, 15766, 315, 15592, 304, 14434, 18729, 25, 50730, 311, 10573, 304, 220, 2366, 18, 702, 256, 482, 3146, 44942, 68063, 15592, 304, 8661, 11, 7528, 18845, 11, 220, 2366, 18, 8661, 11, 15592, 18052, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 13312, 1355, 402, 14029, 2626, 7980, 11, 9721, 3046, 304, 279, 7528, 3634, 271, 45464, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 5479, 5000, 7127, 35198, 2094, 3011, 8886, 70677, 4718, 43609, 48076, 702, 256, 482, 3146, 44942, 68063, 5000, 2955, 21294, 11, 14747, 7969, 11, 62593, 75983, 11, 3997, 26329, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 16406, 7980, 11, 384, 42685, 9873, 11, 3566, 26897, 271, 45464, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 4438, 311, 22991, 279, 10291, 14434, 18729, 16784, 369, 4718, 8184, 702, 256, 482, 3146, 44942, 68063, 14434, 8661, 9266, 11, 2626, 6650, 11, 8661, 8427, 11, 26029, 3600, 11, 8871, 10105, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 2626, 7980, 11, 323, 55572, 3411, 311, 704, 2484, 872, 7528, 15174, 271, 45464, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 10445, 26029, 374, 264, 5843, 9469, 4289, 33350, 369, 4718, 8184, 702, 256, 482, 3146, 44942, 68063, 26029, 9341, 11, 1317, 9860, 6650, 11, 7528, 8661, 11, 2626, 2450, 11, 17808, 9629, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8661, 20258, 11, 5597, 76170, 44929, 922, 26029, 753, 51752, 271, 45464, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 6102, 318, 4954, 51752, 449, 2956, 12, 99584, 18729, 56619, 702, 256, 482, 3146, 44942, 68063, 2956, 32505, 8661, 11, 51752, 11, 8661, 15174, 11, 28975, 11, 2626, 6650, 198, 256, 482, 3146, 6531, 66000, 68063, 18729, 15749, 11, 828, 31499, 11, 2626, 7980, 271, 45464, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 4438, 61844, 8871, 8471, 3053, 15583, 4718, 8184, 702, 256, 482, 3146, 44942, 68063, 61844, 8871, 3600, 11, 2626, 18475, 11, 8871, 10105, 11, 13312, 1862, 11, 26206, 198, 256, 482, 3146, 6531, 66000, 68063, 8184, 7980, 11, 8871, 20258, 11, 11298, 28935, 9873, 271, 45464, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 791, 94100, 315, 16835, 15457, 83, 6427, 304, 14434, 18729, 702, 256, 482, 3146, 44942, 68063, 16835, 48929, 11, 7528, 8661, 11, 6883, 17985, 11, 2626, 6650, 11, 8661, 15174, 198, 256, 482, 3146, 6531, 66000, 68063, 42759, 16941, 1759, 11, 8661, 7411, 11, 6883, 20258, 271, 45464, 14711, 220, 605, 13, 3146, 3936, 68063, 330, 791, 29680, 315, 62593, 7127, 389, 4718, 26029, 83899, 702, 262, 482, 3146, 44942, 68063, 62593, 2955, 11, 26029, 33407, 11, 5536, 389, 26029, 11, 1217, 3217, 11, 3997, 26329, 198, 262, 482, 3146, 6531, 66000, 68063, 5000, 26897, 11, 26029, 35416, 11, 7528, 55572, 11, 2626, 7980, 271, 45464, 14711, 22241, 512, 9673, 5117, 1772, 13650, 527, 34558, 311, 5398, 449, 279, 3600, 323, 19248, 315, 97693, 5364, 847, 11880, 4340, 11, 52913, 9959, 21513, 311, 7417, 26029, 5178, 323, 9504, 279, 1314, 10877, 13, 9062, 8712, 11811, 3230, 6784, 3585, 477, 12034, 315, 4754, 13016, 11, 23391, 23387, 323, 92178, 2262, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "<think>\nOkay, so the user is looking for follow-up questions based on the chat history provided. Let me go through the chat to understand what's been discussed so far.\n\nLooking back, the user has repeatedly asked the assistant to scrape a specific website, Elevated-Marketing.io, to generate 10 different blog post topics, each with keywords and target audiences. The assistant has provided these lists in three separate instances, each time with slightly different topics but all focusing on SEO, digital marketing, cybersecurity, etc.\n\nNow, I need to come up with 3-5 relevant follow-up questions that the user might naturally ask next. The guidelines say to make the questions concise, clear, and directly related to the discussed topics. They should not repeat what's already been covered.\n\nLet me think about what the user might want to explore further. Since the topics are about SEO, content, and audience targeting, the user might want more details on how to structure these blog posts, how to maximize their SEO eff" ... 'ould track to measure the success of these blog posts?\n\nThese questions delve deeper into each aspect of creating and measuring the blog posts, without repeating the initial request. They also show a progression from idea generation to execution and analysis, which is a natural flow for the user.\n\nEach of these questions relates directly to the provided topics, either by seeking more detailed structures, optimization strategies, or methods of expanding the content\'s reach and measuring its impact. They are specific and can lead to actionable next steps for the user.\n</think>\n\n```json\n{\n  "follow_ups": [\n    "Could you provide a detailed outline for one of these blog posts?",\n    "What subheadings or sections should each blog post include to maximize SEO?",\n    "Can you suggest some optimizing tips to make these blog posts more engaging?",\n    "How can we repurpose these blog topics into social media content?",\n    "What are the key metrics we should track to measure the success of these blog posts?"\n  ]\n}\n```', 'output_ids': [524, 9884, 20389, 29, 128012, 128013, 198, 33413, 11, 779, 279, 1217, 374, 3411, 369, 1833, 5352, 4860, 3196, 389, 279, 6369, 3925, 3984, 13, 6914, 757, 733, 1555, 279, 6369, 311, 3619, 1148, 596, 1027, 14407, 779, 3117, 382, 23274, 1203, 11, 279, 1217, 706, 19352, 4691, 279, 18328, 311, 58228, 264, 3230, 3997, 11, 97693, 5364, 847, 11880, 4340, 11, 311, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 1855, 449, 21513, 323, 2218, 30994, 13, 578, 18328, 706, 3984, 1521, 11725, 304, 2380, 8821, 13422, 11, 1855, 892, 449, 10284, 2204, 13650, 719, 682, 21760, 389, 26029, 11, 7528, 8661, 11, 62542, 11, 5099, 382, 7184, 11, 358, 1205, 311, 2586, 709, 449, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 430, 279, 1217, 2643, 18182, 2610, 1828, 13, 578, 17959, 2019, 311, 1304, 279, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 13650, 13, 2435, 1288, 539, 13454, 1148, 596, 2736, 1027, 9960, 382, 10267, 757, 1781, 922, 1148, 279, 1217, 2643, 1390, 311, 13488, 4726, 13, 8876, 279, 13650, 527, 922, 26029, 11, 2262, 11, 323, 10877, 25103, 11, 279, 1217, 2643, 1390, 810, 3649, 389, 1268, 311, 6070, 1521, 5117, 8158, 11, 1268, 311, 35608, 872, 26029, 27375, 11, 477, 10631, 389, 3339, 1124, 23387, 382, 66322, 4860, 1436, 2997, 10371, 369, 50729, 11, 26329, 10631, 11, 477, 2262, 8149, 13, 13596, 9392, 374, 24050, 7953, 26743, 11, 1093, 3674, 3772, 477, 6946, 11, 311, 2109, 30349, 2262, 13, 2435, 2643, 1101, 387, 8173, 304, 1268, 311, 6767, 279, 2450, 315, 1521, 8158, 382, 4516, 11, 13126, 682, 420, 11, 1618, 527, 1063, 3284, 1833, 5352, 4860, 1473, 16, 13, 16910, 499, 3493, 264, 11944, 21782, 369, 832, 315, 1521, 5117, 8158, 5380, 17, 13, 3639, 1207, 2025, 826, 477, 14491, 1288, 1855, 5117, 1772, 2997, 311, 35608, 26029, 5380, 18, 13, 3053, 499, 4284, 1063, 74142, 10631, 311, 1304, 1521, 5117, 8158, 810, 23387, 5380, 19, 13, 2650, 649, 584, 2109, 30349, 1521, 5117, 13650, 1139, 3674, 3772, 2262, 5380, 20, 13, 3639, 527, 279, 1401, 17150, 584, 1288, 3839, 311, 6767, 279, 2450, 315, 1521, 5117, 8158, 1980, 9673, 4860, 82845, 19662, 1139, 1855, 13189, 315, 6968, 323, 30090, 279, 5117, 8158, 11, 2085, 40916, 279, 2926, 1715, 13, 2435, 1101, 1501, 264, 33824, 505, 4623, 9659, 311, 11572, 323, 6492, 11, 902, 374, 264, 5933, 6530, 369, 279, 1217, 382, 4959, 315, 1521, 4860, 36716, 6089, 311, 279, 3984, 13650, 11, 3060, 555, 11125, 810, 11944, 14726, 11, 26329, 15174, 11, 477, 5528, 315, 24050, 279, 2262, 596, 5662, 323, 30090, 1202, 5536, 13, 2435, 527, 3230, 323, 649, 3063, 311, 92178, 1828, 7504, 369, 279, 1217, 627, 128014, 271, 74694, 2285, 198, 517, 220, 330, 19070, 8401, 82, 794, 2330, 262, 330, 13191, 499, 3493, 264, 11944, 21782, 369, 832, 315, 1521, 5117, 8158, 36818, 262, 330, 3923, 1207, 2025, 826, 477, 14491, 1288, 1855, 5117, 1772, 2997, 311, 35608, 26029, 36818, 262, 330, 6854, 499, 4284, 1063, 74142, 10631, 311, 1304, 1521, 5117, 8158, 810, 23387, 36818, 262, 330, 4438, 649, 584, 2109, 30349, 1521, 5117, 13650, 1139, 3674, 3772, 2262, 36818, 262, 330, 3923, 527, 279, 1401, 17150, 584, 1288, 3839, 311, 6767, 279, 2450, 315, 1521, 5117, 8158, 48469, 220, 5243, 534, 74694, 128001], 'meta_info': {'id': '3f5e99ad72d6428fb6dcc35857887864', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 2392, 'weight_version': 'default', 'completion_tokens': 545, 'cached_tokens': 2, 'e2e_latency': 13.42341423034668}}
[2025-10-13 06:56:40] INFO:     127.0.0.1:57072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 06:59:42] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 06:59:42] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 06:59:45] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/launch_server.py", line 11, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 3245, in prepare_server_args
    server_args = ServerArgs.from_cli_args(raw_args)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2939, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 244, in __init__
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 507, in __post_init__
    self._handle_model_specific_adjustments()
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 767, in _handle_model_specific_adjustments
    hf_config = self.get_hf_config()
                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/server_args.py", line 2949, in get_hf_config
    hf_config = get_config(
                ^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/common.py", line 2923, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/utils/hf_transformers_utils.py", line 182, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1333, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
    raise OSError(
OSError: Can't load the configuration of '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/home/ivan/models/GLM-4.5-Air-AWQ-FP16Mix' is the correct path to a directory containing a config.json file
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 07:05:06] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=949879995, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:05:06] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:05:07] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:05:14 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:05:14 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:05:14 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:05:14 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:05:15 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:05:15 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:05:15 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:05:15 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 07:05:15 TP0] Init torch distributed begin.
[2025-10-13 07:05:15 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 07:05:15 TP0] sglang is using nccl==2.27.3
[2025-10-13 07:05:16 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 07:05:16 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 07:05:16 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 07:05:16 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 07:05:16 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 07:05:16 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:05:16 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:05:16 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:05:16 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:05:17 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:03,  2.01it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:02,  2.40it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.59it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:01<00:02,  2.45it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:01<00:01,  3.27it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.74it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:02<00:00,  2.49it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:03<00:00,  2.35it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.39it/s]

[2025-10-13 07:05:21 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 07:05:21 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 07:05:21 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:05:21 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:05:21 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 07:05:21 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:05:21 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:05:22 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 07:05:22 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:15,  1.16s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:15,  1.16s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.23it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.23it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:01<00:06,  1.82it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:01<00:06,  1.82it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.38it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.38it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.94it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.94it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.46it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.46it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:52<01:54, 16.38s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:52<01:54, 16.38s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:09<01:40, 16.73s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:09<01:40, 16.73s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:27<01:26, 17.23s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:27<01:26, 17.23s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [01:48<01:12, 18.19s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [01:48<01:12, 18.19s/it] /usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 07:15:07] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=667318658, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:15:07] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:15:07] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:15:14 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:15:15 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:15:15 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:15:15 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 07:15:15 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:15:15 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:15:15 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 07:15:15 TP0] Init torch distributed begin.
[2025-10-13 07:15:15 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 07:15:15 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 07:15:16 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 07:15:16 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 07:15:16 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 07:15:16 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 07:15:16 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 07:15:16 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 07:15:16 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:15:16 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:15:16 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:15:16 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 07:15:17 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.88it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:03,  2.32it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.54it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:01<00:02,  2.40it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:01<00:01,  3.21it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.91it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:02<00:00,  2.37it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:03<00:00,  2.16it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.13it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:03<00:00,  2.34it/s]

[2025-10-13 07:15:22 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 07:15:22 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 07:15:22 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:15:22 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:15:22 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:15:22 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 07:15:22 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 07:15:22 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 07:15:22 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:15,  1.18s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:15,  1.18s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.27it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:09,  1.27it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:01<00:05,  1.87it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:01<00:05,  1.87it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.43it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.43it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.99it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.99it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.51it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.51it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:52<01:55, 16.57s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:52<01:55, 16.57s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:09<01:40, 16.73s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:09<01:40, 16.73s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:27<01:25, 17.13s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:27<01:25, 17.13s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [01:46<01:10, 17.50s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [01:46<01:10, 17.50s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [02:05<00:54, 18.21s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [02:05<00:54, 18.21s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [02:27<00:38, 19.36s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [02:27<00:38, 19.36s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [02:50<00:20, 20.22s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [02:50<00:20, 20.22s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:12<00:00, 20.90s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:12<00:00, 13.75s/it]
[2025-10-13 07:18:36 TP0] Capture cuda graph end. Time elapsed: 193.73 s. mem usage=0.41 GB. avail mem=3.23 GB.
[2025-10-13 07:18:36 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.23 GB
[2025-10-13 07:18:37] INFO:     Started server process [2237642]
[2025-10-13 07:18:37] INFO:     Waiting for application startup.
[2025-10-13 07:18:37] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 07:18:37] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 07:18:37] INFO:     Application startup complete.
[2025-10-13 07:18:37] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 07:18:38] INFO:     127.0.0.1:51344 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 07:18:38] Receive: obj=GenerateReqInput(rid='531e7ab4e5c240da8f8e7f5bc3ea4761', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 07:18:38 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 07:18:38] Finish: obj=GenerateReqInput(rid='531e7ab4e5c240da8f8e7f5bc3ea4761', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is known for its iconic', 'output_ids': [6864, 3363, 315, 9822, 374, 12366, 13, 1102, 374, 3967, 369, 1202, 27373], 'meta_info': {'id': '531e7ab4e5c240da8f8e7f5bc3ea4761', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 0.6904251575469971}}
[2025-10-13 07:18:38] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 07:18:38] The server is fired up and ready to roll!
[2025-10-13 08:02:52] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 08:02:52] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-13 08:02:53] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:03:01] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=652306085, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:03:01] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:03:02] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:03:09 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:03:09 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:03:09 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:03:10 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:03:10 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 08:03:10 TP0] Init torch distributed begin.
[2025-10-13 08:03:10 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:03:10 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:03:10 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:03:10 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:03:11 TP0] sglang is using nccl==2.27.3
[2025-10-13 08:03:11 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:03:11 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:03:11 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:03:11 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:03:11 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 08:03:11 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:03:11 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:03:11 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:03:11 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:03:12 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.94it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:02,  2.36it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.43it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:01<00:02,  2.19it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:03<00:03,  1.28it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:03<00:01,  1.51it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:04<00:01,  1.63it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  1.75it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.57it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.68it/s]

[2025-10-13 08:03:18 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 08:03:18 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 08:03:18 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:03:18 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:03:18 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:03:18 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 08:03:18 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:03:18 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 08:03:18 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:15,  1.17s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:15,  1.17s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.39it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:08,  1.39it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:01<00:05,  2.01it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:01<00:05,  2.01it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:02<00:03,  2.58it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:02<00:03,  2.58it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:02,  3.14it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:02,  3.14it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.66it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.66it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:52<01:55, 16.46s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:52<01:55, 16.46s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:09<01:40, 16.82s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:09<01:40, 16.82s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:27<01:26, 17.25s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:27<01:26, 17.25s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [01:46<01:10, 17.69s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [01:46<01:10, 17.69s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [02:09<00:57, 19.14s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [02:09<00:57, 19.14s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [02:37<00:43, 21.83s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [02:37<00:43, 21.83s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [03:08<00:24, 24.61s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [03:08<00:24, 24.61s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:41<00:00, 27.31s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:41<00:00, 15.83s/it]
[2025-10-13 08:07:01 TP0] Capture cuda graph end. Time elapsed: 222.88 s. mem usage=0.41 GB. avail mem=3.23 GB.
[2025-10-13 08:07:02 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.23 GB
[2025-10-13 08:07:03] INFO:     Started server process [2281363]
[2025-10-13 08:07:03] INFO:     Waiting for application startup.
[2025-10-13 08:07:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 08:07:03] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 08:07:03] INFO:     Application startup complete.
[2025-10-13 08:07:03] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 08:07:04] INFO:     127.0.0.1:50908 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 08:07:04] Receive: obj=GenerateReqInput(rid='69c05c8c458d442e88e12b956f4f14e1', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:07:04 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:07:05] Finish: obj=GenerateReqInput(rid='69c05c8c458d442e88e12b956f4f14e1', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is known for its iconic', 'output_ids': [6864, 3363, 315, 9822, 374, 12366, 13, 1102, 374, 3967, 369, 1202, 27373], 'meta_info': {'id': '69c05c8c458d442e88e12b956f4f14e1', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 0.969977617263794}}
[2025-10-13 08:07:05] INFO:     127.0.0.1:50918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 08:07:05] The server is fired up and ready to roll!
[2025-10-13 08:07:06] INFO:     127.0.0.1:50920 - "GET /v1/models HTTP/1.1" 200 OK
[2025-10-13 08:07:06] Receive: obj=GenerateReqInput(rid='b99ff3465ca145fcbed82751c4bf37f9', text=None, input_ids=[128000, 16892, 14173, 25, 62853, 609, 794, 330, 2984, 13473, 20432, 498, 330, 4789, 794, 2990, 77, 286, 2522, 20432, 323, 1920, 264, 3566, 2199, 1701, 436, 1190, 2259, 41483, 1734, 1734, 286, 3755, 330, 14105, 794, 5324, 13495, 794, 5324, 1103, 794, 5324, 4789, 794, 330, 791, 5665, 315, 279, 3566, 2199, 311, 58228, 10684, 330, 1337, 794, 330, 928, 9388, 2186, 330, 6413, 794, 4482, 1103, 8073, 330, 1337, 794, 330, 1735, 32075, 2595, 7927, 3465, 374, 311, 5268, 323, 471, 279, 4495, 5507, 1161, 8, 505, 279, 1160, 315, 2561, 7526, 3196, 389, 279, 3319, 13, 11359, 1521, 17959, 1473, 12, 3494, 1193, 279, 4823, 1665, 11, 2085, 904, 5217, 1495, 477, 16540, 382, 12, 1442, 912, 7526, 2489, 279, 3319, 11, 471, 459, 4384, 1358, 25, 720, 256, 341, 257, 330, 14506, 46736, 794, 4260, 256, 557, 12, 1442, 832, 477, 810, 7526, 2489, 279, 3319, 11, 9429, 264, 4823, 2077, 8649, 264, 330, 14506, 46736, 1, 1358, 449, 6302, 430, 2997, 512, 256, 482, 330, 609, 794, 578, 5507, 596, 836, 627, 256, 482, 330, 14105, 794, 362, 11240, 315, 2631, 5137, 323, 872, 12435, 2819, 382, 791, 3645, 369, 279, 4823, 2077, 374, 26549, 512, 517, 220, 330, 14506, 46736, 794, 2330, 262, 5324, 609, 794, 330, 14506, 678, 16, 498, 330, 14105, 794, 5324, 798, 16, 794, 330, 970, 16, 49185, 262, 5324, 609, 794, 330, 14506, 678, 17, 498, 330, 14105, 794, 5324, 798, 17, 794, 330, 970, 17, 96742, 220, 5243, 92, 128011, 2929, 25, 11346, 512, 5045, 3931, 2891, 25, 3270, 22111, 0, 20817, 389, 279, 2262, 505, 3146, 791, 9332, 14285, 4821, 98319, 358, 4805, 7068, 220, 605, 5117, 1772, 6848, 41891, 311, 872, 3600, 11, 10877, 11, 323, 26029, 1888, 12659, 382, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 21785, 369, 16923, 25733, 25, 362, 19121, 13002, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 1268, 433, 4375, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 8173, 304, 2536, 3502, 78134, 4785, 4814, 11, 39890, 43448, 11, 1274, 45243, 2579, 3177, 15419, 271, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 39195, 315, 3816, 8828, 40143, 369, 28049, 6401, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 6930, 2890, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 13444, 50343, 271, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 43, 12351, 445, 7088, 6296, 13, 3816, 8828, 40143, 25, 16299, 374, 7252, 369, 14285, 2140, 414, 287, 7673, 2355, 256, 482, 3146, 44942, 68063, 40708, 326, 7088, 11, 2579, 3177, 15419, 11, 2547, 42533, 287, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 13126, 2536, 3502, 78134, 8834, 14278, 5528, 11, 17479, 43448, 11, 7931, 3411, 369, 2547, 42533, 287, 2671, 271, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 3816, 8828, 40143, 369, 28993, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 39890, 11, 37140, 14278, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 50433, 11, 5933, 15419, 28424, 271, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 13346, 30129, 25, 8976, 18591, 505, 578, 9332, 14285, 4821, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2450, 7493, 11, 4785, 4814, 11, 84610, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 44929, 922, 2579, 3177, 15419, 11, 7931, 3411, 369, 17033, 3135, 11, 39890, 43448, 271, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 4438, 311, 47912, 3816, 8828, 40143, 449, 4718, 70242, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 26308, 14348, 11, 16124, 13654, 11, 17479, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 19343, 20521, 388, 11, 23579, 8173, 304, 13654, 271, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 16233, 8733, 43541, 295, 40143, 369, 16828, 5241, 323, 68685, 367, 1, 2355, 256, 482, 3146, 44942, 68063, 16233, 8733, 39139, 11, 61987, 11, 43685, 11, 4785, 4814, 2355, 256, 482, 3146, 54049, 1873, 68063, 35914, 20521, 388, 11, 61987, 43448, 11, 7931, 3411, 369, 43685, 12823, 271, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 10480, 17375, 10522, 520, 578, 9332, 14285, 4821, 25, 2650, 2435, 34507, 28049, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 8572, 3547, 10522, 11, 6930, 39890, 11, 49592, 6930, 11, 7294, 12, 4210, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 13444, 50343, 11, 7931, 449, 3230, 6930, 10742, 271, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 763, 45864, 7275, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 334, 7275, 5045, 3931, 2891, 25, 3270, 31140, 11, 358, 4250, 58228, 13335, 304, 1972, 7394, 11, 719, 358, 649, 1520, 499, 1893, 5117, 1772, 6848, 11, 21513, 11, 323, 10877, 25103, 3146, 31039, 389, 279, 14595, 2262, 315, 264, 3997, 1093, 578, 9332, 14285, 4821, 98319, 902, 8111, 311, 5357, 389, 39890, 11, 2547, 22972, 11, 323, 52312, 1093, 2579, 3177, 15419, 13, 5810, 527, 220, 605, 41891, 5117, 1772, 13650, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 15583, 4718, 4923, 67003, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 80705, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 4923, 67003, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 39890, 50433, 19124, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 3816, 8828, 40143, 369, 16923, 25733, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 8834, 14278, 11, 2547, 42533, 287, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 7931, 3411, 369, 2536, 3502, 78134, 4785, 4814, 11, 2890, 66666, 7931, 19124, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 6296, 13, 350, 6073, 67354, 25, 16299, 374, 24327, 369, 4718, 28049, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 259, 6073, 28036, 11, 6930, 2890, 11, 30136, 5674, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 13126, 259, 6073, 477, 6930, 22972, 11, 39890, 43448, 19124, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 65184, 68427, 35011, 4740, 5664, 11934, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 16124, 13654, 11, 17479, 11, 1772, 29721, 412, 2512, 2355, 256, 482, 3146, 54049, 1873, 68063, 20277, 14203, 11, 19343, 20521, 388, 11, 17479, 43448, 19124, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 791, 10170, 315, 3816, 8828, 40143, 25, 2650, 1102, 34507, 82, 13201, 8703, 25003, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 71313, 11, 7294, 12, 4210, 11, 6930, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 8173, 304, 5933, 7294, 12, 4210, 10105, 19124, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 520, 5492, 25, 12838, 1102, 5664, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2162, 15419, 11, 32558, 39890, 11, 2853, 53421, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 32558, 43448, 11, 7931, 8173, 304, 520, 25389, 39890, 10105, 19124, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 6854, 3816, 8828, 40143, 53253, 763, 45864, 367, 30, 5810, 596, 3639, 279, 8483, 47559, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 37140, 11, 21249, 6784, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 43448, 11, 1274, 449, 21249, 6784, 19124, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 26781, 34871, 25, 37812, 477, 43754, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 7013, 6650, 11, 7013, 4814, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 25051, 7013, 4814, 11, 13444, 43448, 11, 39890, 50433, 19124, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 1951, 65, 3200, 287, 7874, 3092, 17323, 10180, 3816, 8828, 40143, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 51286, 11, 13363, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 502, 311, 2579, 3177, 15419, 11, 39890, 43448, 11, 37679, 1233, 19124, 14711, 220, 605, 13, 3146, 3936, 68063, 330, 6102, 318, 4954, 4718, 3816, 8828, 40143, 31900, 25, 26788, 369, 279, 7252, 18591, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 10631, 11, 1888, 3135, 11, 39890, 26329, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 2736, 1701, 477, 13126, 2579, 3177, 15419, 11, 39890, 43448, 19124, 45464, 14711, 26029, 7252, 64813, 311, 11359, 512, 16, 13, 3146, 35581, 8483, 68063, 5560, 7526, 1093, 5195, 50070, 75865, 11, 46544, 78277, 11, 477, 362, 13638, 82, 311, 10765, 1579, 67868, 11, 3428, 12, 79123, 21513, 13, 2355, 17, 13, 3146, 6720, 9469, 607, 56795, 68063, 26891, 389, 1317, 2442, 607, 21513, 1093, 330, 68244, 1220, 315, 2579, 3177, 15419, 369, 4785, 4814, 1, 311, 2218, 3230, 10877, 3966, 13, 2355, 18, 13, 3146, 2831, 18410, 68063, 9842, 39319, 11, 1664, 5621, 1874, 291, 11, 323, 23387, 2262, 430, 11503, 1217, 4860, 13, 2355, 19, 13, 3146, 11830, 6074, 287, 68063, 6074, 311, 1023, 5117, 8158, 477, 3600, 389, 701, 3997, 311, 7417, 26029, 323, 1217, 20392, 13, 2355, 20, 13, 3146, 12452, 3959, 25712, 68063, 24969, 29722, 8999, 28887, 311, 5376, 4299, 43847, 7969, 13, 2355, 21, 13, 3146, 1945, 58839, 68063, 5560, 1579, 22867, 5448, 449, 4902, 1495, 430, 5764, 701, 2218, 21513, 13, 2355, 22, 13, 3146, 18876, 58839, 68063, 30379, 701, 5117, 374, 6505, 22658, 369, 2731, 33407, 323, 1217, 3217, 13, 2355, 23, 13, 3146, 27414, 57135, 68063, 12037, 701, 5117, 8158, 389, 3674, 3772, 15771, 311, 6678, 9629, 323, 5376, 24035, 382, 1383, 2768, 1521, 1888, 12659, 323, 6968, 2262, 430, 5398, 82, 449, 701, 10877, 596, 12034, 11, 499, 3358, 7417, 701, 5117, 596, 26029, 5178, 323, 9504, 810, 17808, 9629, 13, 6914, 757, 1440, 422, 499, 1205, 1520, 449, 4477, 904, 315, 1521, 8158, 0, 7275, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 334, 7275, 2929, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 334, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:07:06 TP0] Prefill batch. #new-seq: 1, #new-token: 1910, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:07:10 TP0] Decode batch. #running-req: 1, #token: 1944, token usage: 0.02, cuda graph: True, gen throughput (token/s): 5.27, #queue-req: 0, 
[2025-10-13 08:07:10 TP0] Decode batch. #running-req: 1, #token: 1984, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.32, #queue-req: 0, 
[2025-10-13 08:07:11 TP0] Decode batch. #running-req: 1, #token: 2024, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.08, #queue-req: 0, 
[2025-10-13 08:07:12 TP0] Decode batch. #running-req: 1, #token: 2064, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.29, #queue-req: 0, 
[2025-10-13 08:07:13 TP0] Decode batch. #running-req: 1, #token: 2104, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.47, #queue-req: 0, 
[2025-10-13 08:07:13 TP0] Decode batch. #running-req: 1, #token: 2144, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.27, #queue-req: 0, 
[2025-10-13 08:07:14 TP0] Decode batch. #running-req: 1, #token: 2184, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.13, #queue-req: 0, 
[2025-10-13 08:07:15] Finish: obj=GenerateReqInput(rid='b99ff3465ca145fcbed82751c4bf37f9', text=None, input_ids=[128000, 16892, 14173, 25, 62853, 609, 794, 330, 2984, 13473, 20432, 498, 330, 4789, 794, 2990, 77, 286, 2522, 20432, 323, 1920, 264, 3566, 2199, 1701, 436, 1190, 2259, 41483, 1734, 1734, 286, 3755, 330, 14105, 794, 5324, 13495, 794, 5324, 1103, 794, 5324, 4789, 794, 330, 791, 5665, 315, 279, 3566, 2199, 311, 58228, 10684, 330, 1337, 794, 330, 928, 9388, 2186, 330, 6413, 794, 4482, 1103, 8073, 330, 1337, 794, 330, 1735, 32075, 2595, 7927, 3465, 374, 311, 5268, 323, 471, 279, 4495, 5507, 1161, 8, 505, 279, 1160, 315, 2561, 7526, 3196, 389, 279, 3319, 13, 11359, 1521, 17959, 1473, 12, 3494, 1193, 279, 4823, 1665, 11, 2085, 904, 5217, 1495, 477, 16540, 382, 12, 1442, 912, 7526, 2489, 279, 3319, 11, 471, 459, 4384, 1358, 25, 720, 256, 341, 257, 330, 14506, 46736, 794, 4260, 256, 557, 12, 1442, 832, 477, 810, 7526, 2489, 279, 3319, 11, 9429, 264, 4823, 2077, 8649, 264, 330, 14506, 46736, 1, 1358, 449, 6302, 430, 2997, 512, 256, 482, 330, 609, 794, 578, 5507, 596, 836, 627, 256, 482, 330, 14105, 794, 362, 11240, 315, 2631, 5137, 323, 872, 12435, 2819, 382, 791, 3645, 369, 279, 4823, 2077, 374, 26549, 512, 517, 220, 330, 14506, 46736, 794, 2330, 262, 5324, 609, 794, 330, 14506, 678, 16, 498, 330, 14105, 794, 5324, 798, 16, 794, 330, 970, 16, 49185, 262, 5324, 609, 794, 330, 14506, 678, 17, 498, 330, 14105, 794, 5324, 798, 17, 794, 330, 970, 17, 96742, 220, 5243, 92, 128011, 2929, 25, 11346, 512, 5045, 3931, 2891, 25, 3270, 22111, 0, 20817, 389, 279, 2262, 505, 3146, 791, 9332, 14285, 4821, 98319, 358, 4805, 7068, 220, 605, 5117, 1772, 6848, 41891, 311, 872, 3600, 11, 10877, 11, 323, 26029, 1888, 12659, 382, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 21785, 369, 16923, 25733, 25, 362, 19121, 13002, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 1268, 433, 4375, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 8173, 304, 2536, 3502, 78134, 4785, 4814, 11, 39890, 43448, 11, 1274, 45243, 2579, 3177, 15419, 271, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 39195, 315, 3816, 8828, 40143, 369, 28049, 6401, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 6930, 2890, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 13444, 50343, 271, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 43, 12351, 445, 7088, 6296, 13, 3816, 8828, 40143, 25, 16299, 374, 7252, 369, 14285, 2140, 414, 287, 7673, 2355, 256, 482, 3146, 44942, 68063, 40708, 326, 7088, 11, 2579, 3177, 15419, 11, 2547, 42533, 287, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 13126, 2536, 3502, 78134, 8834, 14278, 5528, 11, 17479, 43448, 11, 7931, 3411, 369, 2547, 42533, 287, 2671, 271, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 3816, 8828, 40143, 369, 28993, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 39890, 11, 37140, 14278, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 50433, 11, 5933, 15419, 28424, 271, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 13346, 30129, 25, 8976, 18591, 505, 578, 9332, 14285, 4821, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2450, 7493, 11, 4785, 4814, 11, 84610, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 44929, 922, 2579, 3177, 15419, 11, 7931, 3411, 369, 17033, 3135, 11, 39890, 43448, 271, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 4438, 311, 47912, 3816, 8828, 40143, 449, 4718, 70242, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 26308, 14348, 11, 16124, 13654, 11, 17479, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 19343, 20521, 388, 11, 23579, 8173, 304, 13654, 271, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 16233, 8733, 43541, 295, 40143, 369, 16828, 5241, 323, 68685, 367, 1, 2355, 256, 482, 3146, 44942, 68063, 16233, 8733, 39139, 11, 61987, 11, 43685, 11, 4785, 4814, 2355, 256, 482, 3146, 54049, 1873, 68063, 35914, 20521, 388, 11, 61987, 43448, 11, 7931, 3411, 369, 43685, 12823, 271, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 10480, 17375, 10522, 520, 578, 9332, 14285, 4821, 25, 2650, 2435, 34507, 28049, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 8572, 3547, 10522, 11, 6930, 39890, 11, 49592, 6930, 11, 7294, 12, 4210, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 13444, 50343, 11, 7931, 449, 3230, 6930, 10742, 271, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 763, 45864, 7275, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 334, 7275, 5045, 3931, 2891, 25, 3270, 31140, 11, 358, 4250, 58228, 13335, 304, 1972, 7394, 11, 719, 358, 649, 1520, 499, 1893, 5117, 1772, 6848, 11, 21513, 11, 323, 10877, 25103, 3146, 31039, 389, 279, 14595, 2262, 315, 264, 3997, 1093, 578, 9332, 14285, 4821, 98319, 902, 8111, 311, 5357, 389, 39890, 11, 2547, 22972, 11, 323, 52312, 1093, 2579, 3177, 15419, 13, 5810, 527, 220, 605, 41891, 5117, 1772, 13650, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 15583, 4718, 4923, 67003, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 80705, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 4923, 67003, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 39890, 50433, 19124, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 3816, 8828, 40143, 369, 16923, 25733, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 8834, 14278, 11, 2547, 42533, 287, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 7931, 3411, 369, 2536, 3502, 78134, 4785, 4814, 11, 2890, 66666, 7931, 19124, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 6296, 13, 350, 6073, 67354, 25, 16299, 374, 24327, 369, 4718, 28049, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 259, 6073, 28036, 11, 6930, 2890, 11, 30136, 5674, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 13126, 259, 6073, 477, 6930, 22972, 11, 39890, 43448, 19124, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 65184, 68427, 35011, 4740, 5664, 11934, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 16124, 13654, 11, 17479, 11, 1772, 29721, 412, 2512, 2355, 256, 482, 3146, 54049, 1873, 68063, 20277, 14203, 11, 19343, 20521, 388, 11, 17479, 43448, 19124, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 791, 10170, 315, 3816, 8828, 40143, 25, 2650, 1102, 34507, 82, 13201, 8703, 25003, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 71313, 11, 7294, 12, 4210, 11, 6930, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 8173, 304, 5933, 7294, 12, 4210, 10105, 19124, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 520, 5492, 25, 12838, 1102, 5664, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2162, 15419, 11, 32558, 39890, 11, 2853, 53421, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 32558, 43448, 11, 7931, 8173, 304, 520, 25389, 39890, 10105, 19124, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 6854, 3816, 8828, 40143, 53253, 763, 45864, 367, 30, 5810, 596, 3639, 279, 8483, 47559, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 37140, 11, 21249, 6784, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 43448, 11, 1274, 449, 21249, 6784, 19124, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 26781, 34871, 25, 37812, 477, 43754, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 7013, 6650, 11, 7013, 4814, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 25051, 7013, 4814, 11, 13444, 43448, 11, 39890, 50433, 19124, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 1951, 65, 3200, 287, 7874, 3092, 17323, 10180, 3816, 8828, 40143, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 51286, 11, 13363, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 502, 311, 2579, 3177, 15419, 11, 39890, 43448, 11, 37679, 1233, 19124, 14711, 220, 605, 13, 3146, 3936, 68063, 330, 6102, 318, 4954, 4718, 3816, 8828, 40143, 31900, 25, 26788, 369, 279, 7252, 18591, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 10631, 11, 1888, 3135, 11, 39890, 26329, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 2736, 1701, 477, 13126, 2579, 3177, 15419, 11, 39890, 43448, 19124, 45464, 14711, 26029, 7252, 64813, 311, 11359, 512, 16, 13, 3146, 35581, 8483, 68063, 5560, 7526, 1093, 5195, 50070, 75865, 11, 46544, 78277, 11, 477, 362, 13638, 82, 311, 10765, 1579, 67868, 11, 3428, 12, 79123, 21513, 13, 2355, 17, 13, 3146, 6720, 9469, 607, 56795, 68063, 26891, 389, 1317, 2442, 607, 21513, 1093, 330, 68244, 1220, 315, 2579, 3177, 15419, 369, 4785, 4814, 1, 311, 2218, 3230, 10877, 3966, 13, 2355, 18, 13, 3146, 2831, 18410, 68063, 9842, 39319, 11, 1664, 5621, 1874, 291, 11, 323, 23387, 2262, 430, 11503, 1217, 4860, 13, 2355, 19, 13, 3146, 11830, 6074, 287, 68063, 6074, 311, 1023, 5117, 8158, 477, 3600, 389, 701, 3997, 311, 7417, 26029, 323, 1217, 20392, 13, 2355, 20, 13, 3146, 12452, 3959, 25712, 68063, 24969, 29722, 8999, 28887, 311, 5376, 4299, 43847, 7969, 13, 2355, 21, 13, 3146, 1945, 58839, 68063, 5560, 1579, 22867, 5448, 449, 4902, 1495, 430, 5764, 701, 2218, 21513, 13, 2355, 22, 13, 3146, 18876, 58839, 68063, 30379, 701, 5117, 374, 6505, 22658, 369, 2731, 33407, 323, 1217, 3217, 13, 2355, 23, 13, 3146, 27414, 57135, 68063, 12037, 701, 5117, 8158, 389, 3674, 3772, 15771, 311, 6678, 9629, 323, 5376, 24035, 382, 1383, 2768, 1521, 1888, 12659, 323, 6968, 2262, 430, 5398, 82, 449, 701, 10877, 596, 12034, 11, 499, 3358, 7417, 701, 5117, 596, 26029, 5178, 323, 9504, 810, 17808, 9629, 13, 6914, 757, 1440, 422, 499, 1205, 1520, 449, 4477, 904, 315, 1521, 8158, 0, 7275, 6584, 25, 4304, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 334, 7275, 2929, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 334, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\nOkay, so I need to figure out which tool to use for this task. The user is asking me to scrape a website and generate blog post topics, keywords, and identify the audience. The available tool is web_scrape, which uses r.jina.ai. \n\nFirst, I should confirm if the tool can handle the website provided. The tool requires a URL, which the user has given as https://Thebaybodybar.com. That seems to fit the requirements. \n\nI need to make sure that the parameters are correctly inputted. The tool expects a URL, so in the parameters, I\'ll set the URL to the specified website. \n\nSince the user specifically wants to generate content based on the scraped data, using web_scrape should be appropriate here. \n\nI don\'t think any other tools are necessary because the task is solely about web scraping and generating content from that data. \n\nI should structure the JSON response correctly, ensuring only the "tool_calls" array is present with the web_scrape tool and the correct URL parameter.\n</think>\n\nThe correct tool to use for this task is **web_scrape**, as it fulfills the requirement of scraping the website to generate blog post topics, keywords, and audience. The URL provided is valid and necessary for the tool to function properly.\n\n```json\n{\n  "tool_calls": [\n    {\n      "name": "web_scrape",\n      "parameters": {\n        "url": "https://Thebaybodybar.com"\n      }\n    }\n  ]\n}\n```', 'output_ids': [2664, 2308, 916, 334, 128012, 128013, 198, 33413, 11, 779, 358, 1205, 311, 7216, 704, 902, 5507, 311, 1005, 369, 420, 3465, 13, 578, 1217, 374, 10371, 757, 311, 58228, 264, 3997, 323, 7068, 5117, 1772, 13650, 11, 21513, 11, 323, 10765, 279, 10877, 13, 578, 2561, 5507, 374, 3566, 13473, 20432, 11, 902, 5829, 436, 1190, 2259, 41483, 13, 4815, 5451, 11, 358, 1288, 7838, 422, 279, 5507, 649, 3790, 279, 3997, 3984, 13, 578, 5507, 7612, 264, 5665, 11, 902, 279, 1217, 706, 2728, 439, 3788, 1129, 791, 47364, 2664, 2308, 916, 13, 3011, 5084, 311, 5052, 279, 8670, 13, 4815, 40, 1205, 311, 1304, 2771, 430, 279, 5137, 527, 12722, 1988, 6702, 13, 578, 5507, 25283, 264, 5665, 11, 779, 304, 279, 5137, 11, 358, 3358, 743, 279, 5665, 311, 279, 5300, 3997, 13, 4815, 12834, 279, 1217, 11951, 6944, 311, 7068, 2262, 3196, 389, 279, 88937, 828, 11, 1701, 3566, 13473, 20432, 1288, 387, 8475, 1618, 13, 4815, 40, 1541, 956, 1781, 904, 1023, 7526, 527, 5995, 1606, 279, 3465, 374, 21742, 922, 3566, 82538, 323, 24038, 2262, 505, 430, 828, 13, 4815, 40, 1288, 6070, 279, 4823, 2077, 12722, 11, 23391, 1193, 279, 330, 14506, 46736, 1, 1358, 374, 3118, 449, 279, 3566, 13473, 20432, 5507, 323, 279, 4495, 5665, 5852, 627, 128014, 271, 791, 4495, 5507, 311, 1005, 369, 420, 3465, 374, 3146, 2984, 13473, 20432, 98319, 439, 433, 5716, 67267, 279, 16686, 315, 82538, 279, 3997, 311, 7068, 5117, 1772, 13650, 11, 21513, 11, 323, 10877, 13, 578, 5665, 3984, 374, 2764, 323, 5995, 369, 279, 5507, 311, 734, 10489, 382, 74694, 2285, 198, 517, 220, 330, 14506, 46736, 794, 2330, 262, 341, 415, 330, 609, 794, 330, 2984, 13473, 20432, 761, 415, 330, 14105, 794, 341, 286, 330, 1103, 794, 330, 2485, 1129, 791, 47364, 2664, 2308, 916, 702, 415, 457, 262, 457, 220, 5243, 534, 74694, 128001], 'meta_info': {'id': 'b99ff3465ca145fcbed82751c4bf37f9', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 1911, 'weight_version': 'default', 'completion_tokens': 311, 'cached_tokens': 1, 'e2e_latency': 8.451682806015015}}
[2025-10-13 08:07:15] INFO:     127.0.0.1:50922 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 08:07:18] INFO:     127.0.0.1:49080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 08:07:18] Receive: obj=GenerateReqInput(rid='d41abf2378fa442eb6f29bd801a3024a', text=None, input_ids=[128000, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3788, 1129, 791, 47364, 2664, 2308, 916, 128012, 40, 2846, 12153, 311, 27100, 279, 3566, 477, 58228, 13335, 304, 1972, 7394, 13, 4452, 11, 358, 649, 1520, 499, 7068, 5117, 1772, 6848, 11, 21513, 11, 323, 10765, 4754, 30994, 3196, 389, 279, 14595, 8970, 315, 264, 3997, 7086, 330, 791, 9332, 14285, 4821, 1210, 1102, 10578, 1093, 264, 2626, 4461, 5552, 311, 2890, 11, 17479, 11, 477, 39890, 13, 5810, 527, 220, 605, 5117, 1772, 6848, 41891, 369, 26029, 1888, 12659, 1473, 14711, 14496, 3962, 23748, 1473, 16, 13, 3146, 3936, 68063, 330, 5479, 220, 20, 39195, 315, 27483, 40143, 369, 20277, 14203, 702, 256, 482, 3146, 44942, 68063, 27483, 15419, 11, 7720, 11, 23579, 11, 10034, 13654, 198, 256, 482, 3146, 54049, 1873, 68063, 20277, 14203, 11, 10034, 43448, 11, 17479, 51191, 271, 17, 13, 3146, 3936, 68063, 330, 791, 29950, 13002, 311, 68685, 367, 25, 35914, 19409, 1392, 1472, 14998, 311, 9934, 702, 256, 482, 3146, 44942, 68063, 35914, 22972, 11, 43685, 11, 17139, 8641, 11, 39890, 198, 256, 482, 3146, 54049, 1873, 68063, 3331, 39890, 50433, 11, 31493, 43448, 11, 8631, 59644, 3078, 271, 18, 13, 3146, 3936, 68063, 330, 10445, 29900, 14285, 39306, 374, 76104, 532, 369, 38895, 6401, 702, 256, 482, 3146, 44942, 68063, 14285, 13709, 11, 10723, 2890, 11, 39890, 11, 5912, 2512, 198, 256, 482, 3146, 54049, 1873, 68063, 38895, 2890, 28424, 11, 39890, 43448, 11, 8631, 29972, 271, 19, 13, 3146, 3936, 68063, 330, 14809, 3962, 12, 6919, 412, 27483, 66044, 369, 68427, 35011, 702, 256, 482, 3146, 44942, 68063, 3962, 29721, 412, 6378, 11, 16124, 13654, 11, 12823, 11, 17479, 198, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 19343, 20521, 388, 11, 23579, 271, 20, 13, 3146, 3936, 68063, 330, 791, 39195, 315, 362, 442, 1894, 9864, 25, 48833, 507, 8839, 369, 51568, 53495, 702, 256, 482, 3146, 44942, 68063, 362, 442, 1894, 9864, 11, 7720, 11, 7718, 32462, 11, 8631, 16337, 198, 256, 482, 3146, 54049, 1873, 68063, 362, 442, 1894, 9864, 33218, 11, 8631, 6373, 50433, 11, 7718, 5707, 3932, 271, 21, 13, 3146, 3936, 68063, 330, 4438, 311, 22991, 279, 10291, 27483, 369, 4718, 44240, 702, 256, 482, 3146, 44942, 68063, 22991, 279, 1314, 6378, 11, 6378, 4595, 11, 3966, 11, 39890, 198, 256, 482, 3146, 54049, 1873, 68063, 5629, 7394, 31493, 20521, 388, 11, 39890, 50433, 11, 7931, 449, 3230, 3966, 271, 22, 13, 3146, 3936, 68063, 330, 17513, 5241, 7922, 4718, 14285, 25, 48833, 27483, 323, 35914, 26788, 702, 256, 482, 3146, 44942, 68063, 16828, 5241, 11, 2547, 61987, 11, 6378, 10631, 11, 31493, 10631, 198, 256, 482, 3146, 54049, 1873, 68063, 16828, 5241, 50433, 11, 39890, 43448, 11, 31493, 5912, 82, 271, 23, 13, 3146, 3936, 68063, 330, 53463, 4718, 15695, 2957, 744, 25, 578, 7572, 315, 14285, 10852, 702, 256, 482, 3146, 44942, 68063, 15695, 2957, 1887, 7916, 11, 2547, 2512, 11, 39890, 11, 2890, 198, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 50433, 271, 24, 13, 3146, 3936, 68063, 330, 7778, 35914, 23748, 25, 4324, 4718, 21189, 68685, 367, 88391, 702, 256, 482, 3146, 44942, 68063, 5492, 31493, 11, 43685, 11, 32558, 11, 39890, 198, 256, 482, 3146, 54049, 1873, 68063, 5492, 39890, 50433, 11, 32558, 43448, 11, 43685, 33218, 271, 605, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 14285, 10852, 25, 2650, 27483, 362, 41027, 4718, 6401, 702, 262, 482, 3146, 44942, 68063, 14285, 2512, 11, 8198, 11, 6378, 7720, 11, 2890, 198, 262, 482, 3146, 54049, 1873, 68063, 6401, 323, 39890, 43448, 11, 8198, 45994, 10185, 7931, 271, 14711, 26029, 7252, 64813, 1473, 16, 13, 3146, 35581, 8483, 68063, 5560, 7526, 1093, 5195, 50070, 75865, 11, 46544, 78277, 11, 477, 362, 13638, 82, 311, 1505, 9959, 323, 1579, 19993, 67868, 21513, 627, 17, 13, 3146, 12452, 3959, 25712, 68063, 9842, 29722, 8999, 28887, 311, 5376, 4299, 43847, 7969, 627, 18, 13, 3146, 11830, 6074, 287, 68063, 6074, 311, 1023, 9959, 9908, 389, 701, 3997, 311, 2567, 3932, 17045, 323, 7417, 26029, 627, 19, 13, 3146, 34174, 9059, 68063, 30379, 701, 2262, 374, 39319, 11, 1664, 5621, 1874, 291, 11, 323, 5825, 907, 311, 279, 6742, 627, 20, 13, 3146, 18876, 7424, 18260, 7127, 68063, 7557, 2771, 701, 5117, 374, 6505, 21638, 369, 2731, 1217, 3217, 323, 26029, 7720, 627, 21, 13, 3146, 4198, 4210, 20796, 82, 68063, 5560, 1579, 22867, 5448, 323, 6946, 311, 18885, 279, 5403, 3217, 627, 22, 13, 3146, 27414, 57135, 68063, 10984, 61140, 13016, 311, 4430, 701, 5117, 8158, 389, 3674, 3772, 15771, 627, 23, 13, 3146, 31504, 28600, 68063, 7440, 380, 4501, 2713, 701, 5117, 449, 7878, 2262, 311, 8450, 311, 2778, 21787, 430, 701, 2816, 374, 4642, 382, 1383, 2768, 1521, 1888, 12659, 11, 499, 649, 5376, 279, 29736, 315, 701, 5117, 8158, 23862, 5190, 304, 2778, 3135, 323, 51647, 810, 17808, 9629, 13, 128001, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 2355, 47530, 757, 1063, 13650, 922, 3816, 3177, 15419, 128012, 40914, 11, 1095, 596, 1893, 1063, 5117, 1772, 6848, 389, 2579, 3177, 15419, 41891, 369, 26029, 1888, 12659, 13, 16644, 430, 279, 3997, 374, 4461, 5552, 311, 2890, 323, 39890, 11, 1521, 13650, 1288, 5398, 1664, 449, 279, 7057, 382, 14711, 14496, 3962, 23748, 1473, 16, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 3816, 8828, 40143, 25, 2650, 1102, 21785, 323, 11699, 39195, 702, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 8198, 11, 7720, 11, 1268, 433, 4375] ... [24762, 4497, 2595, 21362, 1945, 220, 22, 25, 4487, 8138, 5333, 449, 11113, 6930, 67908, 2547, 42533, 287, 3135, 2403, 264, 18718, 4092, 11, 14393, 578, 9332, 14285, 4821, 596, 4785, 4814, 323, 13444, 3600, 36284, 414, 3600, 271, 10596, 323, 32833, 24327, 198, 776, 57970, 14711, 40708, 445, 7088, 271, 58, 4518, 4497, 2595, 14711, 16233, 8733, 43287, 271, 58, 4518, 4497, 2595, 2822, 35567, 11, 2360, 92132, 11, 2360, 423, 13197, 547, 11, 2360, 17072, 30246, 627, 9411, 271, 6161, 3177, 15419, 369, 4785, 4814, 4375, 555, 1701, 3230, 93959, 315, 2579, 3177, 311, 64855, 279, 6930, 323, 5662, 8834, 7917, 304, 279, 2547, 13, 1115, 3177, 95455, 279, 55042, 4298, 2949, 8834, 7917, 11, 26921, 1124, 311, 4984, 9967, 8834, 13, 9843, 6004, 11, 279, 2547, 649, 28168, 553, 323, 22472, 279, 8834, 810, 13750, 13, 6104, 3135, 1253, 13592, 505, 1732, 311, 1732, 11, 2579, 3177, 15419, 374, 11846, 311, 18885, 279, 2547, 753, 5933, 8834, 1481, 54444, 11618, 11, 3339, 433, 459, 33352, 3072, 369, 1884, 3411, 311, 22822, 872, 4785, 4814, 9045, 382, 358, 4265, 53485, 47706, 423, 461, 268, 323, 578, 9332, 14285, 4821, 13, 4815, 358, 5675, 264, 2860, 315, 220, 22, 15271, 304, 264, 2478, 22972, 13, 4815, 34, 13, 17882, 271, 21362, 1945, 220, 24, 25, 25525, 449, 69203, 7013, 323, 5933, 27649, 11, 53004, 3463, 3725, 2403, 264, 14071, 4092, 11, 14393, 13444, 323, 39890, 22100, 9959, 311, 578, 9332, 14285, 4821, 596, 2547, 42533, 287, 323, 4785, 4814, 3600, 13, 9725, 36212, 70559, 1129, 8465, 14, 15257, 64, 19, 13536, 24289, 23, 69, 22, 67, 24951, 6365, 4484, 25326, 21717, 67, 12676, 15, 696, 94909, 382, 11649, 2136, 382, 2675, 627, 1547, 895, 271, 1688, 578, 9332, 14285, 4821, 11, 1057, 68009, 374, 4382, 3686, 8147, 1473, 94909, 13, 61283, 13, 1472, 382, 1453, 1195, 325, 6261, 304, 264, 51639, 315, 73484, 367, 11, 1405, 6335, 2512, 20628, 35649, 11704, 13, 6914, 603, 8641, 499, 389, 264, 11879, 311, 77443, 12410, 323, 1664, 33851, 382, 58, 24762, 4497, 2595, 827, 3639, 656, 584, 656, 1980, 1688, 578, 9332, 14285, 4821, 11, 584, 48444, 304, 33508, 287, 279, 73484, 1113, 2410, 315, 2579, 3177, 15419, 13, 5751, 14713, 48448, 22972, 527, 6319, 311, 18885, 6930, 2890, 11, 8108, 37140, 11, 323, 12192, 8244, 1664, 33851, 13, 15166, 1139, 264, 1917, 1405, 8198, 20628, 1446, 56685, 11, 323, 3217, 279, 87435, 6372, 315, 1057, 12223, 2579, 3177, 15419, 16079, 13, 55664, 349, 701, 659, 41206, 14348, 449, 603, 11, 323, 1095, 279, 12164, 685, 2949, 499, 33505, 53657, 1109, 3596, 382, 58, 24762, 4497, 2595, 21362, 1945, 220, 806, 25, 4487, 8138, 5333, 449, 11113, 6930, 67908, 2547, 42533, 287, 3135, 2403, 264, 18718, 4092, 11, 14393, 578, 9332, 14285, 4821, 596, 4785, 4814, 323, 13444, 3600, 13, 9725, 36212, 70559, 1129, 8465, 14, 2839, 4313, 2495, 64, 21969, 966, 66, 9892, 4484, 1897, 15, 68, 2545, 67, 2946, 11032, 69, 696, 414, 3600, 271, 10596, 323, 32833, 24327, 198, 776, 57970, 14711, 40708, 445, 7088, 271, 58, 4518, 4497, 2595, 14711, 16233, 8733, 43287, 271, 58, 4518, 4497, 2595, 2822, 35567, 11, 2360, 92132, 11, 2360, 423, 13197, 547, 11, 2360, 17072, 30246, 627, 9411, 271, 6161, 3177, 15419, 369, 4785, 4814, 4375, 555, 1701, 3230, 93959, 315, 2579, 3177, 311, 64855, 279, 6930, 323, 5662, 8834, 7917, 304, 279, 2547, 13, 1115, 3177, 95455, 279, 55042, 4298, 2949, 8834, 7917, 11, 26921, 1124, 311, 4984, 9967, 8834, 13, 9843, 6004, 11, 279, 2547, 649, 28168, 553, 323, 22472, 279, 8834, 810, 13750, 13, 6104, 3135, 1253, 13592, 505, 1732, 311, 1732, 11, 2579, 3177, 15419, 374, 11846, 311, 18885, 279, 2547, 753, 5933, 8834, 1481, 54444, 11618, 11, 3339, 433, 459, 33352, 3072, 369, 1884, 3411, 311, 22822, 872, 4785, 4814, 9045, 382, 358, 4265, 53485, 47706, 423, 461, 268, 323, 578, 9332, 14285, 4821, 13, 4815, 358, 5675, 264, 2860, 315, 220, 22, 15271, 304, 264, 2478, 22972, 13, 4815, 34, 13, 17882, 271, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 1318, 271, 21362, 1945, 220, 1032, 25, 25525, 449, 69203, 7013, 323, 5933, 27649, 11, 53004, 3463, 3725, 2403, 264, 14071, 4092, 11, 14393, 13444, 323, 39890, 22100, 9959, 311, 578, 9332, 14285, 4821, 596, 2547, 42533, 287, 323, 4785, 4814, 3600, 13, 9725, 36212, 70559, 1129, 8465, 14, 15257, 64, 19, 13536, 24289, 23, 69, 22, 67, 24951, 6365, 4484, 25326, 21717, 67, 12676, 15, 696, 58, 21124, 2269, 1483, 50444, 2595, 78229, 17697, 26496, 271, 9, 256, 510, 23407, 14285, 4821, 933, 9, 256, 510, 10714, 933, 9, 256, 510, 20989, 82, 933, 9, 256, 510, 43, 12351, 445, 7088, 933, 9, 256, 510, 10254, 4779, 933, 9, 256, 510, 7280, 4800, 933, 9, 256, 510, 8906, 933, 9, 256, 510, 48932, 11216, 2595, 9, 256, 510, 23407, 14285, 4821, 933, 9, 256, 510, 10714, 933, 9, 256, 510, 20989, 82, 933, 9, 256, 510, 43, 12351, 445, 7088, 933, 9, 256, 510, 10254, 4779, 933, 9, 256, 510, 7280, 4800, 933, 9, 256, 510, 8906, 933, 9, 256, 510, 48932, 11216, 2595, 78229, 2175, 763, 19898, 271, 9, 256, 510, 16718, 12, 26519, 12, 23324, 24, 9725, 23774, 25, 21290, 3192, 24876, 15, 340, 9, 256, 510, 18248, 17, 12292, 85, 9265, 393, 74, 23361, 11, 21652, 220, 23, 33, 42451, 11, 2006, 220, 20911, 3487, 9725, 23774, 25, 21290, 3192, 24876, 15, 696, 1820, 2547, 3703, 11, 2547, 3703, 11, 15419, 11, 87942, 3703, 11, 5933, 2547, 27883, 11, 16434, 2547, 27883, 11, 6573, 87942, 3703, 11, 2547, 87942, 3703, 11, 2547, 27883, 3703, 11, 2547, 3703, 10368, 11, 279, 2547, 2308, 11, 21120, 326, 7088, 271, 37, 70941, 53763, 24271, 271, 4438, 7524, 374, 279, 2547, 3703, 369, 4785, 4814, 1980, 791, 27375, 315, 578, 9332, 14285, 4821, 369, 4785, 4814, 374, 5199, 11, 439, 1057, 35649, 7620, 323, 11084, 22972, 11, 1778, 439, 40708, 445, 7088, 323, 2579, 3177, 15419, 11, 527, 6319, 311, 1520, 8403, 11322, 872, 4785, 4814, 9021, 30820, 323, 21676, 382, 2664, 3703, 11, 279, 23542, 2547, 3703, 11, 279, 2547, 3703, 198, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:07:18 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:07:19 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.02, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:07:22 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 0, token usage: 0.03, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:07:24 TP0] Prefill batch. #new-seq: 1, #new-token: 487, #cached-token: 0, token usage: 0.05, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:07:27 TP0] Decode batch. #running-req: 1, #token: 6634, token usage: 0.06, cuda graph: True, gen throughput (token/s): 3.09, #queue-req: 0, 
[2025-10-13 08:07:28 TP0] Decode batch. #running-req: 1, #token: 6674, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.46, #queue-req: 0, 
[2025-10-13 08:07:29 TP0] Decode batch. #running-req: 1, #token: 6714, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.51, #queue-req: 0, 
[2025-10-13 08:07:29 TP0] Decode batch. #running-req: 1, #token: 6754, token usage: 0.06, cuda graph: True, gen throughput (token/s): 52.25, #queue-req: 0, 
[2025-10-13 08:07:30 TP0] Decode batch. #running-req: 1, #token: 6794, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.97, #queue-req: 0, 
[2025-10-13 08:07:31 TP0] Decode batch. #running-req: 1, #token: 6834, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.80, #queue-req: 0, 
[2025-10-13 08:07:32 TP0] Decode batch. #running-req: 1, #token: 6874, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.74, #queue-req: 0, 
[2025-10-13 08:07:32 TP0] Decode batch. #running-req: 1, #token: 6914, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.86, #queue-req: 0, 
[2025-10-13 08:07:33 TP0] Decode batch. #running-req: 1, #token: 6954, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.88, #queue-req: 0, 
[2025-10-13 08:07:34 TP0] Decode batch. #running-req: 1, #token: 6994, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.83, #queue-req: 0, 
[2025-10-13 08:07:35 TP0] Decode batch. #running-req: 1, #token: 7034, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.84, #queue-req: 0, 
[2025-10-13 08:07:35 TP0] Decode batch. #running-req: 1, #token: 7074, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.81, #queue-req: 0, 
[2025-10-13 08:07:36 TP0] Decode batch. #running-req: 1, #token: 7114, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.42, #queue-req: 0, 
[2025-10-13 08:07:37 TP0] Decode batch. #running-req: 1, #token: 7154, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.87, #queue-req: 0, 
[2025-10-13 08:07:38 TP0] Decode batch. #running-req: 1, #token: 7194, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.51, #queue-req: 0, 
[2025-10-13 08:07:39 TP0] Decode batch. #running-req: 1, #token: 7234, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.71, #queue-req: 0, 
[2025-10-13 08:07:39 TP0] Decode batch. #running-req: 1, #token: 7274, token usage: 0.06, cuda graph: True, gen throughput (token/s): 51.72, #queue-req: 0, 
[2025-10-13 08:07:40 TP0] Decode batch. #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.68, #queue-req: 0, 
[2025-10-13 08:07:40] Finish: obj=GenerateReqInput(rid='d41abf2378fa442eb6f29bd801a3024a', text=None, input_ids=[128000, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3788, 1129, 791, 47364, 2664, 2308, 916, 128012, 40, 2846, 12153, 311, 27100, 279, 3566, 477, 58228, 13335, 304, 1972, 7394, 13, 4452, 11, 358, 649, 1520, 499, 7068, 5117, 1772, 6848, 11, 21513, 11, 323, 10765, 4754, 30994, 3196, 389, 279, 14595, 8970, 315, 264, 3997, 7086, 330, 791, 9332, 14285, 4821, 1210, 1102, 10578, 1093, 264, 2626, 4461, 5552, 311, 2890, 11, 17479, 11, 477, 39890, 13, 5810, 527, 220, 605, 5117, 1772, 6848, 41891, 369, 26029, 1888, 12659, 1473, 14711, 14496, 3962, 23748, 1473, 16, 13, 3146, 3936, 68063, 330, 5479, 220, 20, 39195, 315, 27483, 40143, 369, 20277, 14203, 702, 256, 482, 3146, 44942, 68063, 27483, 15419, 11, 7720, 11, 23579, 11, 10034, 13654, 198, 256, 482, 3146, 54049, 1873, 68063, 20277, 14203, 11, 10034, 43448, 11, 17479, 51191, 271, 17, 13, 3146, 3936, 68063, 330, 791, 29950, 13002, 311, 68685, 367, 25, 35914, 19409, 1392, 1472, 14998, 311, 9934, 702, 256, 482, 3146, 44942, 68063, 35914, 22972, 11, 43685, 11, 17139, 8641, 11, 39890, 198, 256, 482, 3146, 54049, 1873, 68063, 3331, 39890, 50433, 11, 31493, 43448, 11, 8631, 59644, 3078, 271, 18, 13, 3146, 3936, 68063, 330, 10445, 29900, 14285, 39306, 374, 76104, 532, 369, 38895, 6401, 702, 256, 482, 3146, 44942, 68063, 14285, 13709, 11, 10723, 2890, 11, 39890, 11, 5912, 2512, 198, 256, 482, 3146, 54049, 1873, 68063, 38895, 2890, 28424, 11, 39890, 43448, 11, 8631, 29972, 271, 19, 13, 3146, 3936, 68063, 330, 14809, 3962, 12, 6919, 412, 27483, 66044, 369, 68427, 35011, 702, 256, 482, 3146, 44942, 68063, 3962, 29721, 412, 6378, 11, 16124, 13654, 11, 12823, 11, 17479, 198, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 19343, 20521, 388, 11, 23579, 271, 20, 13, 3146, 3936, 68063, 330, 791, 39195, 315, 362, 442, 1894, 9864, 25, 48833, 507, 8839, 369, 51568, 53495, 702, 256, 482, 3146, 44942, 68063, 362, 442, 1894, 9864, 11, 7720, 11, 7718, 32462, 11, 8631, 16337, 198, 256, 482, 3146, 54049, 1873, 68063, 362, 442, 1894, 9864, 33218, 11, 8631, 6373, 50433, 11, 7718, 5707, 3932, 271, 21, 13, 3146, 3936, 68063, 330, 4438, 311, 22991, 279, 10291, 27483, 369, 4718, 44240, 702, 256, 482, 3146, 44942, 68063, 22991, 279, 1314, 6378, 11, 6378, 4595, 11, 3966, 11, 39890, 198, 256, 482, 3146, 54049, 1873, 68063, 5629, 7394, 31493, 20521, 388, 11, 39890, 50433, 11, 7931, 449, 3230, 3966, 271, 22, 13, 3146, 3936, 68063, 330, 17513, 5241, 7922, 4718, 14285, 25, 48833, 27483, 323, 35914, 26788, 702, 256, 482, 3146, 44942, 68063, 16828, 5241, 11, 2547, 61987, 11, 6378, 10631, 11, 31493, 10631, 198, 256, 482, 3146, 54049, 1873, 68063, 16828, 5241, 50433, 11, 39890, 43448, 11, 31493, 5912, 82, 271, 23, 13, 3146, 3936, 68063, 330, 53463, 4718, 15695, 2957, 744, 25, 578, 7572, 315, 14285, 10852, 702, 256, 482, 3146, 44942, 68063, 15695, 2957, 1887, 7916, 11, 2547, 2512, 11, 39890, 11, 2890, 198, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 50433, 271, 24, 13, 3146, 3936, 68063, 330, 7778, 35914, 23748, 25, 4324, 4718, 21189, 68685, 367, 88391, 702, 256, 482, 3146, 44942, 68063, 5492, 31493, 11, 43685, 11, 32558, 11, 39890, 198, 256, 482, 3146, 54049, 1873, 68063, 5492, 39890, 50433, 11, 32558, 43448, 11, 43685, 33218, 271, 605, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 14285, 10852, 25, 2650, 27483, 362, 41027, 4718, 6401, 702, 262, 482, 3146, 44942, 68063, 14285, 2512, 11, 8198, 11, 6378, 7720, 11, 2890, 198, 262, 482, 3146, 54049, 1873, 68063, 6401, 323, 39890, 43448, 11, 8198, 45994, 10185, 7931, 271, 14711, 26029, 7252, 64813, 1473, 16, 13, 3146, 35581, 8483, 68063, 5560, 7526, 1093, 5195, 50070, 75865, 11, 46544, 78277, 11, 477, 362, 13638, 82, 311, 1505, 9959, 323, 1579, 19993, 67868, 21513, 627, 17, 13, 3146, 12452, 3959, 25712, 68063, 9842, 29722, 8999, 28887, 311, 5376, 4299, 43847, 7969, 627, 18, 13, 3146, 11830, 6074, 287, 68063, 6074, 311, 1023, 9959, 9908, 389, 701, 3997, 311, 2567, 3932, 17045, 323, 7417, 26029, 627, 19, 13, 3146, 34174, 9059, 68063, 30379, 701, 2262, 374, 39319, 11, 1664, 5621, 1874, 291, 11, 323, 5825, 907, 311, 279, 6742, 627, 20, 13, 3146, 18876, 7424, 18260, 7127, 68063, 7557, 2771, 701, 5117, 374, 6505, 21638, 369, 2731, 1217, 3217, 323, 26029, 7720, 627, 21, 13, 3146, 4198, 4210, 20796, 82, 68063, 5560, 1579, 22867, 5448, 323, 6946, 311, 18885, 279, 5403, 3217, 627, 22, 13, 3146, 27414, 57135, 68063, 10984, 61140, 13016, 311, 4430, 701, 5117, 8158, 389, 3674, 3772, 15771, 627, 23, 13, 3146, 31504, 28600, 68063, 7440, 380, 4501, 2713, 701, 5117, 449, 7878, 2262, 311, 8450, 311, 2778, 21787, 430, 701, 2816, 374, 4642, 382, 1383, 2768, 1521, 1888, 12659, 11, 499, 649, 5376, 279, 29736, 315, 701, 5117, 8158, 23862, 5190, 304, 2778, 3135, 323, 51647, 810, 17808, 9629, 13, 128001, 128011, 40, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 2355, 47530, 757, 1063, 13650, 922, 3816, 3177, 15419, 128012, 40914, 11, 1095, 596, 1893, 1063, 5117, 1772, 6848, 389, 2579, 3177, 15419, 41891, 369, 26029, 1888, 12659, 13, 16644, 430, 279, 3997, 374, 4461, 5552, 311, 2890, 323, 39890, 11, 1521, 13650, 1288, 5398, 1664, 449, 279, 7057, 382, 14711, 14496, 3962, 23748, 1473, 16, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 3816, 8828, 40143, 25, 2650, 1102, 21785, 323, 11699, 39195, 702, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 8198, 11, 7720, 11, 1268, 433, 4375] ... [24762, 4497, 2595, 21362, 1945, 220, 22, 25, 4487, 8138, 5333, 449, 11113, 6930, 67908, 2547, 42533, 287, 3135, 2403, 264, 18718, 4092, 11, 14393, 578, 9332, 14285, 4821, 596, 4785, 4814, 323, 13444, 3600, 36284, 414, 3600, 271, 10596, 323, 32833, 24327, 198, 776, 57970, 14711, 40708, 445, 7088, 271, 58, 4518, 4497, 2595, 14711, 16233, 8733, 43287, 271, 58, 4518, 4497, 2595, 2822, 35567, 11, 2360, 92132, 11, 2360, 423, 13197, 547, 11, 2360, 17072, 30246, 627, 9411, 271, 6161, 3177, 15419, 369, 4785, 4814, 4375, 555, 1701, 3230, 93959, 315, 2579, 3177, 311, 64855, 279, 6930, 323, 5662, 8834, 7917, 304, 279, 2547, 13, 1115, 3177, 95455, 279, 55042, 4298, 2949, 8834, 7917, 11, 26921, 1124, 311, 4984, 9967, 8834, 13, 9843, 6004, 11, 279, 2547, 649, 28168, 553, 323, 22472, 279, 8834, 810, 13750, 13, 6104, 3135, 1253, 13592, 505, 1732, 311, 1732, 11, 2579, 3177, 15419, 374, 11846, 311, 18885, 279, 2547, 753, 5933, 8834, 1481, 54444, 11618, 11, 3339, 433, 459, 33352, 3072, 369, 1884, 3411, 311, 22822, 872, 4785, 4814, 9045, 382, 358, 4265, 53485, 47706, 423, 461, 268, 323, 578, 9332, 14285, 4821, 13, 4815, 358, 5675, 264, 2860, 315, 220, 22, 15271, 304, 264, 2478, 22972, 13, 4815, 34, 13, 17882, 271, 21362, 1945, 220, 24, 25, 25525, 449, 69203, 7013, 323, 5933, 27649, 11, 53004, 3463, 3725, 2403, 264, 14071, 4092, 11, 14393, 13444, 323, 39890, 22100, 9959, 311, 578, 9332, 14285, 4821, 596, 2547, 42533, 287, 323, 4785, 4814, 3600, 13, 9725, 36212, 70559, 1129, 8465, 14, 15257, 64, 19, 13536, 24289, 23, 69, 22, 67, 24951, 6365, 4484, 25326, 21717, 67, 12676, 15, 696, 94909, 382, 11649, 2136, 382, 2675, 627, 1547, 895, 271, 1688, 578, 9332, 14285, 4821, 11, 1057, 68009, 374, 4382, 3686, 8147, 1473, 94909, 13, 61283, 13, 1472, 382, 1453, 1195, 325, 6261, 304, 264, 51639, 315, 73484, 367, 11, 1405, 6335, 2512, 20628, 35649, 11704, 13, 6914, 603, 8641, 499, 389, 264, 11879, 311, 77443, 12410, 323, 1664, 33851, 382, 58, 24762, 4497, 2595, 827, 3639, 656, 584, 656, 1980, 1688, 578, 9332, 14285, 4821, 11, 584, 48444, 304, 33508, 287, 279, 73484, 1113, 2410, 315, 2579, 3177, 15419, 13, 5751, 14713, 48448, 22972, 527, 6319, 311, 18885, 6930, 2890, 11, 8108, 37140, 11, 323, 12192, 8244, 1664, 33851, 13, 15166, 1139, 264, 1917, 1405, 8198, 20628, 1446, 56685, 11, 323, 3217, 279, 87435, 6372, 315, 1057, 12223, 2579, 3177, 15419, 16079, 13, 55664, 349, 701, 659, 41206, 14348, 449, 603, 11, 323, 1095, 279, 12164, 685, 2949, 499, 33505, 53657, 1109, 3596, 382, 58, 24762, 4497, 2595, 21362, 1945, 220, 806, 25, 4487, 8138, 5333, 449, 11113, 6930, 67908, 2547, 42533, 287, 3135, 2403, 264, 18718, 4092, 11, 14393, 578, 9332, 14285, 4821, 596, 4785, 4814, 323, 13444, 3600, 13, 9725, 36212, 70559, 1129, 8465, 14, 2839, 4313, 2495, 64, 21969, 966, 66, 9892, 4484, 1897, 15, 68, 2545, 67, 2946, 11032, 69, 696, 414, 3600, 271, 10596, 323, 32833, 24327, 198, 776, 57970, 14711, 40708, 445, 7088, 271, 58, 4518, 4497, 2595, 14711, 16233, 8733, 43287, 271, 58, 4518, 4497, 2595, 2822, 35567, 11, 2360, 92132, 11, 2360, 423, 13197, 547, 11, 2360, 17072, 30246, 627, 9411, 271, 6161, 3177, 15419, 369, 4785, 4814, 4375, 555, 1701, 3230, 93959, 315, 2579, 3177, 311, 64855, 279, 6930, 323, 5662, 8834, 7917, 304, 279, 2547, 13, 1115, 3177, 95455, 279, 55042, 4298, 2949, 8834, 7917, 11, 26921, 1124, 311, 4984, 9967, 8834, 13, 9843, 6004, 11, 279, 2547, 649, 28168, 553, 323, 22472, 279, 8834, 810, 13750, 13, 6104, 3135, 1253, 13592, 505, 1732, 311, 1732, 11, 2579, 3177, 15419, 374, 11846, 311, 18885, 279, 2547, 753, 5933, 8834, 1481, 54444, 11618, 11, 3339, 433, 459, 33352, 3072, 369, 1884, 3411, 311, 22822, 872, 4785, 4814, 9045, 382, 358, 4265, 53485, 47706, 423, 461, 268, 323, 578, 9332, 14285, 4821, 13, 4815, 358, 5675, 264, 2860, 315, 220, 22, 15271, 304, 264, 2478, 22972, 13, 4815, 34, 13, 17882, 271, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 17024, 1318, 271, 21362, 1945, 220, 1032, 25, 25525, 449, 69203, 7013, 323, 5933, 27649, 11, 53004, 3463, 3725, 2403, 264, 14071, 4092, 11, 14393, 13444, 323, 39890, 22100, 9959, 311, 578, 9332, 14285, 4821, 596, 2547, 42533, 287, 323, 4785, 4814, 3600, 13, 9725, 36212, 70559, 1129, 8465, 14, 15257, 64, 19, 13536, 24289, 23, 69, 22, 67, 24951, 6365, 4484, 25326, 21717, 67, 12676, 15, 696, 58, 21124, 2269, 1483, 50444, 2595, 78229, 17697, 26496, 271, 9, 256, 510, 23407, 14285, 4821, 933, 9, 256, 510, 10714, 933, 9, 256, 510, 20989, 82, 933, 9, 256, 510, 43, 12351, 445, 7088, 933, 9, 256, 510, 10254, 4779, 933, 9, 256, 510, 7280, 4800, 933, 9, 256, 510, 8906, 933, 9, 256, 510, 48932, 11216, 2595, 9, 256, 510, 23407, 14285, 4821, 933, 9, 256, 510, 10714, 933, 9, 256, 510, 20989, 82, 933, 9, 256, 510, 43, 12351, 445, 7088, 933, 9, 256, 510, 10254, 4779, 933, 9, 256, 510, 7280, 4800, 933, 9, 256, 510, 8906, 933, 9, 256, 510, 48932, 11216, 2595, 78229, 2175, 763, 19898, 271, 9, 256, 510, 16718, 12, 26519, 12, 23324, 24, 9725, 23774, 25, 21290, 3192, 24876, 15, 340, 9, 256, 510, 18248, 17, 12292, 85, 9265, 393, 74, 23361, 11, 21652, 220, 23, 33, 42451, 11, 2006, 220, 20911, 3487, 9725, 23774, 25, 21290, 3192, 24876, 15, 696, 1820, 2547, 3703, 11, 2547, 3703, 11, 15419, 11, 87942, 3703, 11, 5933, 2547, 27883, 11, 16434, 2547, 27883, 11, 6573, 87942, 3703, 11, 2547, 87942, 3703, 11, 2547, 27883, 3703, 11, 2547, 3703, 10368, 11, 279, 2547, 2308, 11, 21120, 326, 7088, 271, 37, 70941, 53763, 24271, 271, 4438, 7524, 374, 279, 2547, 3703, 369, 4785, 4814, 1980, 791, 27375, 315, 578, 9332, 14285, 4821, 369, 4785, 4814, 374, 5199, 11, 439, 1057, 35649, 7620, 323, 11084, 22972, 11, 1778, 439, 40708, 445, 7088, 323, 2579, 3177, 15419, 11, 527, 6319, 311, 1520, 8403, 11322, 872, 4785, 4814, 9021, 30820, 323, 21676, 382, 2664, 3703, 11, 279, 23542, 2547, 3703, 11, 279, 2547, 3703, 198, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\n\n### Blog Post Ideas for The Bay Body Bar\n\n1. **Title:** "How Red Light Therapy Boosts Weight Loss: Expert Insights from The Bay Body Bar"  \n   - **Keywords:** Red light therapy, weight loss, fat reduction, body contouring  \n   - **Audience:** individuals interested in non-invasive weight loss methods, health enthusiasts  \n\n2. **Title:** "Laser Lipo vs. Red Light Therapy: Which Is Best for Body Contouring?"  \n   - **Keywords:** Laser Lipo, red light therapy, body contouring, fat reduction  \n   - **Audience:** fitness enthusiasts, people considering non-invasive treatments  \n\n3. **Title:** "The Science Behind Red Light Therapy for Skin Health and Anti-Aging"  \n   - **Keywords:** Red light therapy, skin health, anti-aging, glowing skin  \n   - **Audience:** skincare enthusiasts, individuals with aging concerns  \n\n4. **Title:** "How Sauna Blanket Therapy Can Enhance Your Detox Routine"  \n   - **Keywords:** Sauna blanket, detox, relaxation, weight loss  \n   - **Audience:** wellness enthusiasts, detox seeke' ... 'er Recovery"  \n   - **Keywords:** Red light therapy, muscle recovery, fitness, post-workout care  \n   - **Audience:** athletes, gym-goers, fitness enthusiasts  \n\n10. **Title:** "Debunking Myths About Red Light Therapy: Separating Fact from Fiction"  \n   - **Keywords:** Red light therapy, myths, facts, wellness treatments  \n   - **Audience:** individuals new to red light therapy, wellness enthusiasts  \n\n### SEO Best Practices:\n- **Keyword Research:** Use tools to identify high-search-volume keywords like "red light therapy for weight loss."  \n- **Internal Linking:** Link to other relevant blog posts or service pages on your website.  \n- **Meta Descriptions:** Craft engaging meta descriptions to improve click-through rates.  \n- **Image Optimization:** Use high-quality images with alt text that includes your target keywords.  \n- **Mobile Optimization:** Ensure your blog is mobile-friendly for better user experience.  \n- **Social Sharing:** Share blog posts on social media to drive traffic and improve visibility.', 'output_ids': [128001], 'meta_info': {'id': 'd41abf2378fa442eb6f29bd801a3024a', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 6632, 'weight_version': 'default', 'completion_tokens': 682, 'cached_tokens': 1, 'e2e_latency': 21.898670434951782}}
[2025-10-13 08:07:40] Receive: obj=GenerateReqInput(rid='8a1f31356b124bca83b5aacfab3b2f4f', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 5045, 3931, 2891, 25, 8681, 0, 20817, 389, 279, 2262, 505, 3146, 791, 9332, 14285, 4821, 98319, 358, 4805, 7068, 220, 605, 5117, 1772, 6848, 41891, 311, 872, 3600, 11, 10877, 11, 323, 26029, 1888, 12659, 382, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 21785, 369, 16923, 25733, 25, 362, 19121, 13002, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 1268, 433, 4375, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 8173, 304, 2536, 3502, 78134, 4785, 4814, 11, 39890, 43448, 11, 1274, 45243, 2579, 3177, 15419, 271, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 39195, 315, 3816, 8828, 40143, 369, 28049, 6401, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 6930, 2890, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 13444, 50343, 271, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 43, 12351, 445, 7088, 6296, 13, 3816, 8828, 40143, 25, 16299, 374, 7252, 369, 14285, 2140, 414, 287, 7673, 2355, 256, 482, 3146, 44942, 68063, 40708, 326, 7088, 11, 2579, 3177, 15419, 11, 2547, 42533, 287, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 13126, 2536, 3502, 78134, 8834, 14278, 5528, 11, 17479, 43448, 11, 7931, 3411, 369, 2547, 42533, 287, 2671, 271, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 3816, 8828, 40143, 369, 28993, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 39890, 11, 37140, 14278, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 50433, 11, 5933, 15419, 28424, 271, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 13346, 30129, 25, 8976, 18591, 505, 578, 9332, 14285, 4821, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2450, 7493, 11, 4785, 4814, 11, 84610, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 44929, 922, 2579, 3177, 15419, 11, 7931, 3411, 369, 17033, 3135, 11, 39890, 43448, 271, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 4438, 311, 47912, 3816, 8828, 40143, 449, 4718, 70242, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 26308, 14348, 11, 16124, 13654, 11, 17479, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 19343, 20521, 388, 11, 23579, 8173, 304, 13654, 271, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 16233, 8733, 43541, 295, 40143, 369, 16828, 5241, 323, 68685, 367, 1, 2355, 256, 482, 3146, 44942, 68063, 16233, 8733, 39139, 11, 61987, 11, 43685, 11, 4785, 4814, 2355, 256, 482, 3146, 54049, 1873, 68063, 35914, 20521, 388, 11, 61987, 43448, 11, 7931, 3411, 369, 43685, 12823, 271, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 10480, 17375, 10522, 520, 578, 9332, 14285, 4821, 25, 2650, 2435, 34507, 28049, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 8572, 3547, 10522, 11, 6930, 39890, 11, 49592, 6930, 11, 7294, 12, 4210, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 13444, 50343, 11, 7931, 449, 3230, 6930, 10742, 271, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 763, 45864, 198, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 5045, 3931, 2891, 25, 19173, 11, 358, 4250, 58228, 13335, 304, 1972, 7394, 11, 719, 358, 649, 1520, 499, 1893, 5117, 1772, 6848, 11, 21513, 11, 323, 10877, 25103, 3146, 31039, 389, 279, 14595, 2262, 315, 264, 3997, 1093, 578, 9332, 14285, 4821, 98319, 902, 8111, 311, 5357, 389, 39890, 11, 2547, 22972, 11, 323, 52312, 1093, 2579, 3177, 15419, 13, 5810, 527, 220, 605, 41891, 5117, 1772, 13650, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 15583, 4718, 4923, 67003, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 80705, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 4923, 67003, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 39890, 50433, 19124, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 3816, 8828, 40143, 369, 16923, 25733, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 8834, 14278, 11, 2547, 42533, 287, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 7931, 3411, 369, 2536, 3502, 78134, 4785, 4814, 11, 2890, 66666, 7931, 19124, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 6296, 13, 350, 6073, 67354, 25, 16299, 374, 24327, 369, 4718, 28049, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 259, 6073, 28036, 11, 6930, 2890, 11, 30136, 5674, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 13126, 259, 6073, 477, 6930, 22972, 11, 39890, 43448, 19124, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 65184, 68427, 35011, 4740, 5664, 11934, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 16124, 13654, 11, 17479, 11, 1772, 29721, 412, 2512, 2355, 256, 482, 3146, 54049, 1873, 68063, 20277, 14203, 11, 19343, 20521, 388, 11, 17479, 43448, 19124, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 791, 10170, 315, 3816, 8828, 40143, 25, 2650, 1102, 34507, 82, 13201, 8703, 25003, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 71313, 11, 7294, 12, 4210, 11, 6930, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 8173, 304, 5933, 7294, 12, 4210, 10105, 19124, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 520, 5492, 25, 12838, 1102, 5664, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2162, 15419, 11, 32558, 39890, 11, 2853, 53421, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 32558, 43448, 11, 7931, 8173, 304, 520, 25389, 39890, 10105, 19124, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 6854, 3816, 8828, 40143, 53253, 763, 45864, 367, 30, 5810, 596, 3639, 279, 8483, 47559, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 37140, 11, 21249, 6784, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 43448, 11, 1274, 449, 21249, 6784, 19124, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 26781, 34871, 25, 37812, 477, 43754, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 7013, 6650, 11, 7013, 4814, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 25051, 7013, 4814, 11, 13444, 43448, 11, 39890, 50433, 19124, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 1951, 65, 3200, 287, 7874, 3092, 17323, 10180, 3816, 8828, 40143, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 51286, 11, 13363, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 502, 311, 2579, 3177, 15419, 11, 39890, 43448, 11, 37679, 1233, 19124, 14711, 220, 605, 13, 3146, 3936, 68063, 330, 6102, 318, 4954, 4718, 3816, 8828, 40143, 31900, 25, 26788, 369, 279, 7252, 18591, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 10631, 11, 1888, 3135, 11, 39890, 26329, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 2736, 1701, 477, 13126, 2579, 3177, 15419, 11, 39890, 43448, 19124, 45464, 14711, 26029, 7252, 64813, 311, 11359, 512, 16, 13, 3146, 35581, 8483, 68063, 5560, 7526, 1093, 5195, 50070, 75865, 11, 46544, 78277, 11, 477, 362, 13638, 82, 311, 10765, 1579, 67868, 11, 3428, 12, 79123, 21513, 13, 2355, 17, 13, 3146, 6720, 9469, 607, 56795, 68063, 26891, 389, 1317, 2442, 607, 21513, 1093, 330, 68244, 1220, 315, 2579, 3177, 15419, 369, 4785, 4814, 1, 311, 2218, 3230, 10877, 3966, 13, 2355, 18, 13, 3146, 2831, 18410, 68063, 9842, 39319, 11, 1664, 5621, 1874, 291, 11, 323, 23387, 2262, 430, 11503, 1217, 4860, 13, 2355, 19, 13, 3146, 11830, 6074, 287, 68063, 6074, 311, 1023, 5117, 8158, 477, 3600, 389, 701, 3997, 311, 7417, 26029, 323, 1217, 20392, 13, 2355, 20, 13, 3146, 12452, 3959, 25712, 68063, 24969, 29722, 8999, 28887, 311, 5376, 4299, 43847, 7969, 13, 2355, 21, 13, 3146, 1945, 58839, 68063, 5560, 1579, 22867, 5448, 449, 4902, 1495, 430, 5764, 701, 2218, 21513, 13, 2355, 22, 13, 3146, 18876, 58839, 68063, 30379, 701, 5117, 374, 6505, 22658, 369, 2731, 33407, 323, 1217, 3217, 13, 2355, 23, 13, 3146, 27414, 57135, 68063, 12037, 701, 5117, 8158, 389, 3674, 3772, 15771, 311, 6678, 9629, 323, 5376, 24035, 382, 1383, 2768, 1521, 1888, 12659, 323, 6968, 2262, 430, 5398, 82, 449, 701, 10877, 596, 12034, 11, 499, 3358, 7417, 701, 5117, 596, 26029, 5178, 323, 9504, 810, 17808, 9629, 13, 6914, 757, 1440, 422, 499, 1205, 1520, 449, 4477, 904, 315, 1521, 8158, 4999, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 5045, 3931, 2891, 25, 720, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:07:40 TP0] Prefill batch. #new-seq: 1, #new-token: 1853, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:07:43 TP0] Decode batch. #running-req: 1, #token: 1895, token usage: 0.02, cuda graph: True, gen throughput (token/s): 12.23, #queue-req: 0, 
[2025-10-13 08:07:44 TP0] Decode batch. #running-req: 1, #token: 1935, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.43, #queue-req: 0, 
[2025-10-13 08:07:45 TP0] Decode batch. #running-req: 1, #token: 1975, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.25, #queue-req: 0, 
[2025-10-13 08:07:46 TP0] Decode batch. #running-req: 1, #token: 2015, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.18, #queue-req: 0, 
[2025-10-13 08:07:46 TP0] Decode batch. #running-req: 1, #token: 2055, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.23, #queue-req: 0, 
[2025-10-13 08:07:47 TP0] Decode batch. #running-req: 1, #token: 2095, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.33, #queue-req: 0, 
[2025-10-13 08:07:48 TP0] Decode batch. #running-req: 1, #token: 2135, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.10, #queue-req: 0, 
[2025-10-13 08:07:49 TP0] Decode batch. #running-req: 1, #token: 2175, token usage: 0.02, cuda graph: True, gen throughput (token/s): 51.70, #queue-req: 0, 
[2025-10-13 08:07:49 TP0] Decode batch. #running-req: 1, #token: 2215, token usage: 0.02, cuda graph: True, gen throughput (token/s): 52.81, #queue-req: 0, 
[2025-10-13 08:07:50 TP0] Decode batch. #running-req: 1, #token: 2255, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.14, #queue-req: 0, 
[2025-10-13 08:07:50] Finish: obj=GenerateReqInput(rid='8a1f31356b124bca83b5aacfab3b2f4f', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 5045, 3931, 2891, 25, 8681, 0, 20817, 389, 279, 2262, 505, 3146, 791, 9332, 14285, 4821, 98319, 358, 4805, 7068, 220, 605, 5117, 1772, 6848, 41891, 311, 872, 3600, 11, 10877, 11, 323, 26029, 1888, 12659, 382, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 21785, 369, 16923, 25733, 25, 362, 19121, 13002, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 1268, 433, 4375, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 8173, 304, 2536, 3502, 78134, 4785, 4814, 11, 39890, 43448, 11, 1274, 45243, 2579, 3177, 15419, 271, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 39195, 315, 3816, 8828, 40143, 369, 28049, 6401, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 6930, 2890, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 13444, 50343, 271, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 43, 12351, 445, 7088, 6296, 13, 3816, 8828, 40143, 25, 16299, 374, 7252, 369, 14285, 2140, 414, 287, 7673, 2355, 256, 482, 3146, 44942, 68063, 40708, 326, 7088, 11, 2579, 3177, 15419, 11, 2547, 42533, 287, 11, 8834, 14278, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 13126, 2536, 3502, 78134, 8834, 14278, 5528, 11, 17479, 43448, 11, 7931, 3411, 369, 2547, 42533, 287, 2671, 271, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 791, 10170, 43474, 3816, 8828, 40143, 369, 28993, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 39890, 11, 37140, 14278, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 50433, 11, 5933, 15419, 28424, 271, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 13346, 30129, 25, 8976, 18591, 505, 578, 9332, 14285, 4821, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2450, 7493, 11, 4785, 4814, 11, 84610, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 44929, 922, 2579, 3177, 15419, 11, 7931, 3411, 369, 17033, 3135, 11, 39890, 43448, 271, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 4438, 311, 47912, 3816, 8828, 40143, 449, 4718, 70242, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 26308, 14348, 11, 16124, 13654, 11, 17479, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 19343, 20521, 388, 11, 23579, 8173, 304, 13654, 271, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 16233, 8733, 43541, 295, 40143, 369, 16828, 5241, 323, 68685, 367, 1, 2355, 256, 482, 3146, 44942, 68063, 16233, 8733, 39139, 11, 61987, 11, 43685, 11, 4785, 4814, 2355, 256, 482, 3146, 54049, 1873, 68063, 35914, 20521, 388, 11, 61987, 43448, 11, 7931, 3411, 369, 43685, 12823, 271, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 10480, 17375, 10522, 520, 578, 9332, 14285, 4821, 25, 2650, 2435, 34507, 28049, 61283, 1, 2355, 256, 482, 3146, 44942, 68063, 8572, 3547, 10522, 11, 6930, 39890, 11, 49592, 6930, 11, 7294, 12, 4210, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 13444, 50343, 11, 7931, 449, 3230, 6930, 10742, 271, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 763, 45864, 198, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 5045, 3931, 2891, 25, 19173, 11, 358, 4250, 58228, 13335, 304, 1972, 7394, 11, 719, 358, 649, 1520, 499, 1893, 5117, 1772, 6848, 11, 21513, 11, 323, 10877, 25103, 3146, 31039, 389, 279, 14595, 2262, 315, 264, 3997, 1093, 578, 9332, 14285, 4821, 98319, 902, 8111, 311, 5357, 389, 39890, 11, 2547, 22972, 11, 323, 52312, 1093, 2579, 3177, 15419, 13, 5810, 527, 220, 605, 41891, 5117, 1772, 13650, 1473, 45464, 14711, 220, 16, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 15583, 4718, 4923, 67003, 72162, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 80705, 11, 7294, 12, 4210, 11, 49592, 6930, 2355, 256, 482, 3146, 54049, 1873, 68063, 4923, 67003, 43448, 11, 7931, 449, 46905, 477, 30084, 10742, 11, 39890, 50433, 19124, 14711, 220, 17, 13, 3146, 3936, 68063, 330, 791, 7054, 220, 20, 39195, 315, 3816, 8828, 40143, 369, 16923, 25733, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 4785, 4814, 11, 8834, 14278, 11, 2547, 42533, 287, 2355, 256, 482, 3146, 54049, 1873, 68063, 36808, 43448, 11, 7931, 3411, 369, 2536, 3502, 78134, 4785, 4814, 11, 2890, 66666, 7931, 19124, 14711, 220, 18, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 6296, 13, 350, 6073, 67354, 25, 16299, 374, 24327, 369, 4718, 28049, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 259, 6073, 28036, 11, 6930, 2890, 11, 30136, 5674, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 13126, 259, 6073, 477, 6930, 22972, 11, 39890, 43448, 19124, 14711, 220, 19, 13, 3146, 3936, 68063, 330, 4438, 3816, 8828, 40143, 3053, 65184, 68427, 35011, 4740, 5664, 11934, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 16124, 13654, 11, 17479, 11, 1772, 29721, 412, 2512, 2355, 256, 482, 3146, 54049, 1873, 68063, 20277, 14203, 11, 19343, 20521, 388, 11, 17479, 43448, 19124, 14711, 220, 20, 13, 3146, 3936, 68063, 330, 791, 10170, 315, 3816, 8828, 40143, 25, 2650, 1102, 34507, 82, 13201, 8703, 25003, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 71313, 11, 7294, 12, 4210, 11, 6930, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 28049, 2512, 43448, 11, 7931, 8173, 304, 5933, 7294, 12, 4210, 10105, 19124, 14711, 220, 21, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 520, 5492, 25, 12838, 1102, 5664, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 2162, 15419, 11, 32558, 39890, 11, 2853, 53421, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 32558, 43448, 11, 7931, 8173, 304, 520, 25389, 39890, 10105, 19124, 14711, 220, 22, 13, 3146, 3936, 68063, 330, 6854, 3816, 8828, 40143, 53253, 763, 45864, 367, 30, 5810, 596, 3639, 279, 8483, 47559, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 37140, 11, 21249, 6784, 11, 2890, 7720, 2355, 256, 482, 3146, 54049, 1873, 68063, 6401, 66666, 7931, 11, 39890, 43448, 11, 1274, 449, 21249, 6784, 19124, 14711, 220, 23, 13, 3146, 3936, 68063, 330, 6161, 8828, 40143, 369, 26781, 34871, 25, 37812, 477, 43754, 7673, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 7013, 6650, 11, 7013, 4814, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 25051, 7013, 4814, 11, 13444, 43448, 11, 39890, 50433, 19124, 14711, 220, 24, 13, 3146, 3936, 68063, 330, 1951, 65, 3200, 287, 7874, 3092, 17323, 10180, 3816, 8828, 40143, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 51286, 11, 13363, 11, 39890, 22972, 2355, 256, 482, 3146, 54049, 1873, 68063, 62525, 502, 311, 2579, 3177, 15419, 11, 39890, 43448, 11, 37679, 1233, 19124, 14711, 220, 605, 13, 3146, 3936, 68063, 330, 6102, 318, 4954, 4718, 3816, 8828, 40143, 31900, 25, 26788, 369, 279, 7252, 18591, 1, 2355, 256, 482, 3146, 44942, 68063, 3816, 3177, 15419, 11, 10631, 11, 1888, 3135, 11, 39890, 26329, 2355, 256, 482, 3146, 54049, 1873, 68063, 9029, 2736, 1701, 477, 13126, 2579, 3177, 15419, 11, 39890, 43448, 19124, 45464, 14711, 26029, 7252, 64813, 311, 11359, 512, 16, 13, 3146, 35581, 8483, 68063, 5560, 7526, 1093, 5195, 50070, 75865, 11, 46544, 78277, 11, 477, 362, 13638, 82, 311, 10765, 1579, 67868, 11, 3428, 12, 79123, 21513, 13, 2355, 17, 13, 3146, 6720, 9469, 607, 56795, 68063, 26891, 389, 1317, 2442, 607, 21513, 1093, 330, 68244, 1220, 315, 2579, 3177, 15419, 369, 4785, 4814, 1, 311, 2218, 3230, 10877, 3966, 13, 2355, 18, 13, 3146, 2831, 18410, 68063, 9842, 39319, 11, 1664, 5621, 1874, 291, 11, 323, 23387, 2262, 430, 11503, 1217, 4860, 13, 2355, 19, 13, 3146, 11830, 6074, 287, 68063, 6074, 311, 1023, 5117, 8158, 477, 3600, 389, 701, 3997, 311, 7417, 26029, 323, 1217, 20392, 13, 2355, 20, 13, 3146, 12452, 3959, 25712, 68063, 24969, 29722, 8999, 28887, 311, 5376, 4299, 43847, 7969, 13, 2355, 21, 13, 3146, 1945, 58839, 68063, 5560, 1579, 22867, 5448, 449, 4902, 1495, 430, 5764, 701, 2218, 21513, 13, 2355, 22, 13, 3146, 18876, 58839, 68063, 30379, 701, 5117, 374, 6505, 22658, 369, 2731, 33407, 323, 1217, 3217, 13, 2355, 23, 13, 3146, 27414, 57135, 68063, 12037, 701, 5117, 8158, 389, 3674, 3772, 15771, 311, 6678, 9629, 323, 5376, 24035, 382, 1383, 2768, 1521, 1888, 12659, 323, 6968, 2262, 430, 5398, 82, 449, 701, 10877, 596, 12034, 11, 499, 3358, 7417, 701, 5117, 596, 26029, 5178, 323, 9504, 810, 17808, 9629, 13, 6914, 757, 1440, 422, 499, 1205, 1520, 449, 4477, 904, 315, 1521, 8158, 4999, 6584, 25, 358, 1390, 499, 311, 58228, 420, 3997, 323, 7068, 220, 605, 2204, 5117, 1772, 13650, 11, 21513, 11, 323, 889, 279, 10877, 690, 387, 369, 1521, 5117, 8158, 13, 2355, 19999, 304, 4059, 1888, 12659, 369, 26029, 11, 5195, 41425, 11, 323, 3794, 1948, 3135, 323, 2778, 28849, 1022, 315, 1521, 8158, 13, 2355, 1820, 3997, 374, 3146, 2485, 1129, 791, 47364, 2664, 2308, 916, 1035, 5045, 3931, 2891, 25, 720, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "<think>\nAlright, so the user is working on creating blog posts for The Bay Body Bar. They asked for 10 different topics with keywords and audience info. The assistant provided two sets of topics focusing on red light therapy, which makes sense given the website's services. \n\nLooking at the chat history, the user repeated the same request, so maybe they were after more ideas or perhaps had a connection issue. The assistant clarified that they can't scrape but can help based on typical content. The topics provided are well-structured and cover various angles of red light therapy, which is the core service of The Bay Body Bar.\n\nNow, to come up with follow-up questions, I need to think about what the user might want next. They might want to improve their blog's SEO further or dive deeper into specific topics. So, potential follow-ups could be about optimizing individual blog posts, creating internal links, or targeting a broader audience. They might also want to know how to present these topics effectively or att" ... 'ntent presentation strategies, audience expansion ideas, and ways to internally link the posts. These are natural next steps after generating topics. The questions should be concise and directly related to the current discussion without repeating what\'s already covered.\n\nSo, the follow-up questions would focus on optimizing the generated topics, engaging the audience better, expanding the audience, and internal linking strategies. Each question should be clear and guide the conversation towards enhancing the blog\'s effectiveness based on the provided topics.\n</think>\n\n```json\n{\n  "follow_ups": [\n    "What are the best SEO practices to optimize these blog posts for higher search engine rankings?",\n    "Can you suggest some internal linking strategies to connect these blog posts with other relevant content on the website?",\n    "How can we make these blog posts more engaging to attract a wider audience?",\n    "Are there any additional keywords or long-tail phrases we should consider for these topics?"\n  ]\n}\n```', 'output_ids': [524, 9884, 20389, 29, 128012, 128013, 198, 72586, 11, 779, 279, 1217, 374, 3318, 389, 6968, 5117, 8158, 369, 578, 9332, 14285, 4821, 13, 2435, 4691, 369, 220, 605, 2204, 13650, 449, 21513, 323, 10877, 3630, 13, 578, 18328, 3984, 1403, 7437, 315, 13650, 21760, 389, 2579, 3177, 15419, 11, 902, 3727, 5647, 2728, 279, 3997, 596, 3600, 13, 4815, 23274, 520, 279, 6369, 3925, 11, 279, 1217, 11763, 279, 1890, 1715, 11, 779, 7344, 814, 1051, 1306, 810, 6848, 477, 8530, 1047, 264, 3717, 4360, 13, 578, 18328, 65876, 430, 814, 649, 956, 58228, 719, 649, 1520, 3196, 389, 14595, 2262, 13, 578, 13650, 3984, 527, 1664, 12, 52243, 323, 3504, 5370, 27030, 315, 2579, 3177, 15419, 11, 902, 374, 279, 6332, 2532, 315, 578, 9332, 14285, 4821, 382, 7184, 11, 311, 2586, 709, 449, 1833, 5352, 4860, 11, 358, 1205, 311, 1781, 922, 1148, 279, 1217, 2643, 1390, 1828, 13, 2435, 2643, 1390, 311, 7417, 872, 5117, 596, 26029, 4726, 477, 30963, 19662, 1139, 3230, 13650, 13, 2100, 11, 4754, 1833, 27859, 1436, 387, 922, 74142, 3927, 5117, 8158, 11, 6968, 5419, 7902, 11, 477, 25103, 264, 27927, 10877, 13, 2435, 2643, 1101, 1390, 311, 1440, 1268, 311, 3118, 1521, 13650, 13750, 477, 9504, 810, 9629, 382, 40, 1288, 2980, 4860, 430, 2610, 369, 26029, 26329, 10631, 11, 2262, 15864, 15174, 11, 10877, 14800, 6848, 11, 323, 5627, 311, 34167, 2723, 279, 8158, 13, 4314, 527, 5933, 1828, 7504, 1306, 24038, 13650, 13, 578, 4860, 1288, 387, 64694, 323, 6089, 5552, 311, 279, 1510, 10430, 2085, 40916, 1148, 596, 2736, 9960, 382, 4516, 11, 279, 1833, 5352, 4860, 1053, 5357, 389, 74142, 279, 8066, 13650, 11, 23387, 279, 10877, 2731, 11, 24050, 279, 10877, 11, 323, 5419, 31799, 15174, 13, 9062, 3488, 1288, 387, 2867, 323, 8641, 279, 10652, 7119, 47594, 279, 5117, 596, 27375, 3196, 389, 279, 3984, 13650, 627, 128014, 271, 74694, 2285, 198, 517, 220, 330, 19070, 8401, 82, 794, 2330, 262, 330, 3923, 527, 279, 1888, 26029, 12659, 311, 30536, 1521, 5117, 8158, 369, 5190, 2778, 4817, 33407, 36818, 262, 330, 6854, 499, 4284, 1063, 5419, 31799, 15174, 311, 4667, 1521, 5117, 8158, 449, 1023, 9959, 2262, 389, 279, 3997, 36818, 262, 330, 4438, 649, 584, 1304, 1521, 5117, 8158, 810, 23387, 311, 9504, 264, 22622, 10877, 36818, 262, 330, 11787, 1070, 904, 5217, 21513, 477, 1317, 2442, 607, 32847, 584, 1288, 2980, 369, 1521, 13650, 48469, 220, 5243, 534, 74694, 128001], 'meta_info': {'id': '8a1f31356b124bca83b5aacfab3b2f4f', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 1855, 'weight_version': 'default', 'completion_tokens': 403, 'cached_tokens': 2, 'e2e_latency': 10.005752563476562}}
[2025-10-13 08:07:50] INFO:     127.0.0.1:47530 - "POST /v1/chat/completions HTTP/1.1" 200 OK
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:22:36] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=41633368, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:22:36] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:22:36] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/start_sglang_deepseek_tp4_compiled.sh: line 23: 2560989 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name deepseek-r1-distill-70b-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:22:59] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=555802456, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:22:59] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:22:59] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:23:07 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:23:07 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:23:08 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:23:08 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:23:08 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:23:08 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:23:09 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:23:09 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:23:09 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 08:23:09 TP0] Init torch distributed begin.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:23:09 TP0] sglang is using nccl==2.27.3
[2025-10-13 08:23:10 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:23:10 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:23:10 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:23:10 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:23:10 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 08:23:10 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:23:10 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:23:10 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:23:10 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:23:11 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:02<00:19,  2.39s/it]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:04<00:14,  2.09s/it]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:06<00:13,  2.18s/it]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:08<00:10,  2.15s/it]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:09<00:07,  1.76s/it]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:12<00:06,  2.25s/it]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:15<00:04,  2.30s/it]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:17<00:02,  2.30s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:20<00:00,  2.56s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:20<00:00,  2.31s/it]

[2025-10-13 08:23:32 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=0.34 GB, mem usage=22.77 GB.
[2025-10-13 08:23:34 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 08:23:34 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:23:34 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:23:34 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:23:34 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:23:34] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_deepseek_tp4_compiled.sh: line 23: 2563525 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name deepseek-r1-distill-70b-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:23:58] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=834930584, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:23:58] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:23:58] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:05 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:06 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:06 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:06 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:06 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:07 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:07 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:07 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:07 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 08:24:07 TP0] Init torch distributed begin.
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:24:08 TP0] sglang is using nccl==2.27.3
[2025-10-13 08:24:08 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:24:08 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:24:08 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:24:08 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:24:08 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 08:24:08 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:08 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:08 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:08 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:09 TP0] Load weight begin. avail mem=9.77 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:02<00:22,  2.75s/it]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:03<00:11,  1.63s/it]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:07<00:15,  2.51s/it]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:09<00:11,  2.37s/it]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:09<00:06,  1.64s/it]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:10<00:03,  1.26s/it]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:10<00:02,  1.05s/it]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:11<00:00,  1.10it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:11<00:00,  1.25it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:11<00:00,  1.33s/it]

[2025-10-13 08:24:22 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=0.34 GB, mem usage=9.43 GB.
[2025-10-13 08:24:22 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 08:24:22 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:24:22 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:24:22 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:24:22 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:24:22] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_deepseek_tp4_compiled.sh: line 23: 2565080 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name deepseek-r1-distill-70b-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:24:40] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=1056861159, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:40] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:40] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:51 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:51 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:51 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:24:51 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:52 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:52 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:52 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 08:24:52 TP0] Init torch distributed begin.
[2025-10-13 08:24:52 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:24:52 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:24:52 TP0] sglang is using nccl==2.27.3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:24:53 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:24:53 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:24:53 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:24:53 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:24:53 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 08:24:53 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:53 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:53 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:53 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:24:54 TP0] Load weight begin. avail mem=9.77 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:05,  1.52it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:03,  1.93it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.84it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:02,  1.81it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:04<00:04,  1.13s/it]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:06<00:04,  1.39s/it]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:07<00:02,  1.21s/it]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:07<00:01,  1.03s/it]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:08<00:00,  1.13it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:08<00:00,  1.08it/s]

[2025-10-13 08:25:03 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=0.34 GB, mem usage=9.43 GB.
[2025-10-13 08:25:03 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 08:25:03 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:03 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:03 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:03 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:03] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_deepseek_tp4_compiled.sh: line 23: 2566543 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name deepseek-r1-distill-70b-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:25:21] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=203863708, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:25:21] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:25:21] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:25:31 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:25:31 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:25:31 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:25:31 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:25:32 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:25:32 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:25:32 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:25:32 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 08:25:32 TP0] Init torch distributed begin.
[2025-10-13 08:25:32 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:25:33 TP0] sglang is using nccl==2.27.3
[2025-10-13 08:25:34 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:25:34 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:25:34 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:25:34 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:25:34 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 08:25:34 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:25:34 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:25:34 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:25:34 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:25:34 TP0] Load weight begin. avail mem=9.77 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.78it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:03,  1.93it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:02,  2.06it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:02,  1.77it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:02<00:01,  2.36it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.07it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:03<00:01,  1.84it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  1.72it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.65it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.82it/s]

[2025-10-13 08:25:40 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=0.34 GB, mem usage=9.43 GB.
[2025-10-13 08:25:40 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 08:25:40 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:40 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:40 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:40 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:25:40] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_deepseek_tp4_compiled.sh: line 23: 2567924 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name deepseek-r1-distill-70b-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:25:57] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=891997974, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:25:57] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:25:58] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:06 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:07 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:07 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:07 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:07 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:07 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:07 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 08:26:07 TP0] Init torch distributed begin.
[2025-10-13 08:26:07 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:08 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:26:09 TP0] sglang is using nccl==2.27.3
[2025-10-13 08:26:09 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:26:09 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:26:09 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:26:09 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:26:09 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 08:26:09 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:09 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:09 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:09 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:10 TP0] Load weight begin. avail mem=9.77 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:05,  1.56it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:03,  1.87it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.91it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:02,  1.81it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:02<00:01,  2.40it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.20it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:03<00:01,  1.95it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  1.77it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.64it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.83it/s]

[2025-10-13 08:26:16 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=0.34 GB, mem usage=9.43 GB.
[2025-10-13 08:26:17 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 08:26:17 TP0] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:26:17 TP2] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:26:17 TP1] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:26:17 TP3] Scheduler hit an exception: Traceback (most recent call last):
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 2934, in run_scheduler_process
    scheduler = Scheduler(
                ^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/scheduler.py", line 446, in __init__
    self.tp_worker = TpModelWorker(
                     ^^^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/managers/tp_worker.py", line 95, in __init__
    self.model_runner = ModelRunner(
                        ^^^^^^^^^^^^
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 299, in __init__
    self.initialize(min_per_gpu_memory)
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 435, in initialize
    self.init_memory_pool(
  File "/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/sglang/srt/model_executor/model_runner.py", line 1520, in init_memory_pool
    raise RuntimeError(
RuntimeError: Not enough memory. Please try to increase --mem-fraction-static. Current value: self.server_args.mem_fraction_static=0.8

[2025-10-13 08:26:17] Received sigquit from a child process. It usually means the child failed.
/home/ivan/sglang/start_sglang_deepseek_tp4_compiled.sh: line 23: 2569298 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name deepseek-r1-distill-70b-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
[2025-10-13 08:26:36] server_args=ServerArgs(model_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_path='/home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, modelopt_quant=None, context_length=24576, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8001, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization='awq_marlin', quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=2048, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=4, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=604980130, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=True, log_level='info', log_level_http=None, log_requests=True, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='deepseek-r1-distill-70b-awq', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=80, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, sm_group_num=3)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:36] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:37] Using default HuggingFace chat template with detected content format: string
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:44 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:45 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:45 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:45 TP1] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-13 08:26:45 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:45 TP3] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:45 TP0] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2025-10-13 08:26:45 TP0] Attention backend not explicitly specified. Use flashinfer backend by default.
[2025-10-13 08:26:45 TP0] Init torch distributed begin.
[2025-10-13 08:26:46 TP2] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:26:47 TP0] sglang is using nccl==2.27.3
[2025-10-13 08:26:47 TP1] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:26:47 TP2] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:26:47 TP3] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2025-10-13 08:26:47 TP0] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[2025-10-13 08:26:47 TP0] Init torch distributed ends. mem usage=0.19 GB
[2025-10-13 08:26:47 TP3] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:47 TP0] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:47 TP1] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:47 TP2] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
[2025-10-13 08:26:48 TP0] Load weight begin. avail mem=23.11 GB
Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.88it/s]
Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:00<00:03,  2.04it/s]
Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.99it/s]
Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:02,  1.81it/s]
Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:02<00:01,  2.28it/s]
Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:02<00:01,  2.11it/s]
Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:03<00:01,  1.91it/s]
Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  1.80it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.75it/s]
Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  1.89it/s]

[2025-10-13 08:26:53 TP0] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=13.69 GB, mem usage=9.43 GB.
[2025-10-13 08:26:54 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-13 08:26:54 TP3] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:26:54 TP2] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:26:54 TP0] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:26:54 TP0] Memory pool end. avail mem=4.08 GB
[2025-10-13 08:26:54 TP1] KV Cache is allocated. #tokens: 118830, K size: 4.53 GB, V size: 4.53 GB
[2025-10-13 08:26:54 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=3.64 GB
[2025-10-13 08:26:54 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80]
  0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   0%|          | 0/14 [00:00<?, ?it/s]Capturing batches (bs=80 avail_mem=3.60 GB):   7%|▋         | 1/14 [00:01<00:16,  1.26s/it]Capturing batches (bs=72 avail_mem=3.48 GB):   7%|▋         | 1/14 [00:01<00:16,  1.26s/it]Capturing batches (bs=72 avail_mem=3.48 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.15it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  14%|█▍        | 2/14 [00:01<00:10,  1.15it/s]Capturing batches (bs=64 avail_mem=3.46 GB):  21%|██▏       | 3/14 [00:02<00:06,  1.66it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  21%|██▏       | 3/14 [00:02<00:06,  1.66it/s]Capturing batches (bs=56 avail_mem=3.45 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.14it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  29%|██▊       | 4/14 [00:02<00:04,  2.14it/s]Capturing batches (bs=48 avail_mem=3.42 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.60it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  36%|███▌      | 5/14 [00:02<00:03,  2.60it/s]Capturing batches (bs=40 avail_mem=3.41 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.03it/s]Capturing batches (bs=32 avail_mem=3.38 GB):  43%|████▎     | 6/14 [00:02<00:02,  3.03it/s]/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
/home/ivan/sglang/sglang-env/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:1575: UserWarning: Dynamo detected a call to a `functools.lru_cache`-wrapped function. Dynamo ignores the cache wrapper and directly traces the wrapped function. Silent incorrectness is only a *potential* risk, not something we have observed. Enable TORCH_LOGS="+dynamo" for a DEBUG stack trace.
  torch._dynamo.utils.warn_once(msg)
Capturing batches (bs=32 avail_mem=3.38 GB):  50%|█████     | 7/14 [00:56<02:04, 17.78s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  50%|█████     | 7/14 [00:56<02:04, 17.78s/it]Capturing batches (bs=24 avail_mem=3.37 GB):  57%|█████▋    | 8/14 [01:14<01:46, 17.82s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  57%|█████▋    | 8/14 [01:14<01:46, 17.82s/it]Capturing batches (bs=16 avail_mem=3.34 GB):  64%|██████▍   | 9/14 [01:35<01:33, 18.72s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  64%|██████▍   | 9/14 [01:35<01:33, 18.72s/it]Capturing batches (bs=12 avail_mem=3.33 GB):  71%|███████▏  | 10/14 [01:56<01:18, 19.59s/it]Capturing batches (bs=8 avail_mem=3.31 GB):  71%|███████▏  | 10/14 [01:56<01:18, 19.59s/it] Capturing batches (bs=8 avail_mem=3.31 GB):  79%|███████▊  | 11/14 [02:23<01:05, 21.72s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  79%|███████▊  | 11/14 [02:23<01:05, 21.72s/it]Capturing batches (bs=4 avail_mem=3.30 GB):  86%|████████▌ | 12/14 [02:50<00:46, 23.28s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  86%|████████▌ | 12/14 [02:50<00:46, 23.28s/it]Capturing batches (bs=2 avail_mem=3.27 GB):  93%|█████████▎| 13/14 [03:16<00:24, 24.16s/it]Capturing batches (bs=1 avail_mem=3.26 GB):  93%|█████████▎| 13/14 [03:16<00:24, 24.16s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:42<00:00, 24.65s/it]Capturing batches (bs=1 avail_mem=3.26 GB): 100%|██████████| 14/14 [03:42<00:00, 15.86s/it]
[2025-10-13 08:30:37 TP0] Capture cuda graph end. Time elapsed: 223.50 s. mem usage=0.41 GB. avail mem=3.23 GB.
[2025-10-13 08:30:38 TP0] max_total_num_tokens=118830, chunked_prefill_size=2048, max_prefill_tokens=16384, max_running_requests=2475, context_len=24576, available_gpu_mem=3.23 GB
[2025-10-13 08:30:38] INFO:     Started server process [2570657]
[2025-10-13 08:30:38] INFO:     Waiting for application startup.
[2025-10-13 08:30:38] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 08:30:38] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0}
[2025-10-13 08:30:38] INFO:     Application startup complete.
[2025-10-13 08:30:38] INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
[2025-10-13 08:30:39] INFO:     127.0.0.1:48208 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-13 08:30:39] Receive: obj=GenerateReqInput(rid='4df364b6066244d398fc77ef82f42d12', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:30:39 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:30:40] Finish: obj=GenerateReqInput(rid='4df364b6066244d398fc77ef82f42d12', text='The capital city of France is', input_ids=None, input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 0, 'max_new_tokens': 8}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=False, stream=False, log_metrics=True, return_hidden_states=False, modalities=None, session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': ' Paris. It is known for its iconic', 'output_ids': [6864, 3363, 315, 9822, 374, 12366, 13, 1102, 374, 3967, 369, 1202, 27373], 'meta_info': {'id': '4df364b6066244d398fc77ef82f42d12', 'finish_reason': {'type': 'length', 'length': 8}, 'prompt_tokens': 7, 'weight_version': 'default', 'completion_tokens': 8, 'cached_tokens': 0, 'e2e_latency': 1.0990967750549316}}
[2025-10-13 08:30:40] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-13 08:30:40] The server is fired up and ready to roll!
[2025-10-13 08:49:34] INFO:     127.0.0.1:49832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 08:49:34] Receive: obj=GenerateReqInput(rid='49ae469256d648c9bc971387d5fd23a1', text=None, input_ids=[128000, 1502, 9805, 1473, 128011, 19182, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 30, 128012, 9906, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 30, 128001, 128011, 40, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 30, 128012, 40914, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 13, 128001, 128011, 3923, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 30, 128012, 4599, 38663, 279, 45478, 4732, 369, 264, 6683, 16239, 5361, 1579, 58574, 71503, 11, 264, 8147, 14266, 11, 323, 3544, 15055, 315, 22813, 11, 1070, 527, 3892, 9547, 311, 2980, 13, 6914, 596, 1464, 1523, 279, 3685, 5178, 323, 279, 5536, 315, 5370, 36770, 1473, 14711, 5464, 12499, 512, 12, 3146, 21, 87, 34661, 10860, 55, 220, 15500, 15, 71503, 96618, 4314, 527, 8147, 11761, 41327, 71503, 449, 220, 1187, 5494, 315, 19718, 1428, 1855, 11, 13171, 315, 11850, 26192, 15592, 990, 33785, 627, 12, 3146, 58805, 19613, 93944, 220, 5958, 37, 18, 14266, 96618, 362, 220, 845, 24357, 11, 220, 843, 61904, 3622, 41327, 14266, 449, 264, 2385, 9042, 4732, 315, 220, 18, 13, 22, 52405, 323, 264, 7916, 9042, 4732, 315, 220, 19, 13, 16, 52405, 13, 1102, 596, 1664, 87229, 1639, 369, 18646, 5361, 71503, 323, 11850, 14266, 50962, 9256, 627, 12, 3146, 8358, 5494, 44860, 19, 22813, 520, 220, 9588, 15, 37594, 96618, 69489, 315, 5044, 311, 6106, 430, 701, 990, 33785, 527, 539, 5044, 50962, 627, 12, 3146, 1600, 71730, 220, 23, 13, 15, 13, 18, 84, 38, 323, 36060, 220, 1187, 13, 2371, 80128, 18270, 96618, 29125, 279, 54696, 304, 264, 4200, 1534, 4676, 11621, 1063, 32115, 11, 719, 6617, 9950, 651, 42314, 1093, 19844, 71730, 527, 5115, 11297, 382, 14711, 56360, 323, 15248, 408, 512, 12, 3146, 8750, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 96618, 1115, 374, 4461, 34440, 369, 5178, 389, 6617, 23501, 12035, 627, 12, 3146, 5109, 30531, 2005, 46745, 96618, 46402, 11, 279, 46745, 690, 617, 264, 17832, 5536, 389, 45478, 4732, 7389, 433, 9221, 264, 88938, 304, 11850, 1217, 7540, 382, 14711, 21304, 21829, 811, 512, 16, 13, 3146, 2409, 14, 27131, 25464, 4026, 334, 512, 256, 482, 3146, 2409, 25464, 4026, 96618, 25464, 4026, 6276, 2167, 1579, 68775, 3175, 10758, 1990, 71503, 11, 18189, 40370, 323, 18899, 15374, 13, 1115, 649, 387, 24629, 369, 4332, 4967, 323, 45478, 11, 1405, 828, 3966, 311, 387, 6222, 6288, 1990, 71503, 627, 256, 482, 3146, 27131, 25464, 4026, 96618, 432, 989, 287, 389, 91790, 34125, 369, 10758, 1990, 71503, 11, 902, 1253, 19678, 1063, 40370, 323, 8108, 8244, 64862, 382, 17, 13, 3146, 2409, 14, 27131, 5768, 9988, 7286, 50991, 393, 17, 47, 46837, 334, 512, 256, 482, 3146, 2409, 5768, 9988, 46837, 96618, 12362, 279, 2167, 23501, 393, 17, 47, 12050, 505, 279, 3984, 2723, 649, 12207, 7417, 14734, 4791, 79697, 10758, 1990, 71503, 11, 18189, 32115, 323, 40370, 627, 256, 482, 3146, 27131, 5768, 9988, 46837, 96618, 1472, 1053, 17631, 389, 5410, 12050, 11, 902, 2643, 539, 387, 439, 34440, 369, 23501, 4791, 12279, 6459, 10758, 382, 18, 13, 3146, 1959, 21321, 14171, 334, 512, 256, 482, 3146, 1959, 21321, 291, 14171, 96618, 6193, 21321, 287, 23501, 5044, 649, 3493, 264, 8275, 5178, 7916, 11, 8104, 422, 701, 54696, 374, 5044, 50962, 13, 4452, 11, 279, 20192, 2643, 387, 32873, 323, 2586, 520, 279, 2853, 315, 5190, 2410, 15652, 323, 4754, 20334, 4819, 382, 19, 13, 3146, 15335, 72955, 334, 512, 256, 482, 3146, 99341, 7572, 72955, 96618, 74540, 279, 2410, 4017, 320, 51, 10510, 8, 649, 2187, 279, 71503, 311, 1629, 520, 5190, 34873, 369, 5129, 18852, 11, 13893, 18899, 5178, 13, 4452, 11, 420, 690, 5376, 2410, 15652, 323, 8798, 14091, 49686, 627, 256, 482, 3146, 3760, 7572, 72955, 96618, 29125, 520, 1670, 2410, 13693, 690, 6106, 20334, 323, 58219, 719, 1253, 4017, 279, 7340, 5178, 382, 20, 13, 3146, 33, 548, 19757, 6296, 13, 19844, 71730, 18270, 334, 512, 256, 482, 3146, 33, 548, 19757, 96618, 29125, 6089, 389, 12743, 9501, 690, 22472, 279, 4200, 2065, 32115, 11, 13893, 6522, 311, 2731, 5178, 627, 256, 482, 3146, 1600, 71730, 18270, 96618, 29125, 304, 264, 18270, 11621, 1063, 32115, 719, 6209, 7720, 1093, 31398, 11, 8831, 6373, 11, 323, 25152, 13, 18766, 9950, 651, 42314, 527, 5115, 11297, 11, 779, 279, 5178, 4295, 2643, 539, 387, 439, 5199, 439, 304, 279, 3347, 382, 14711, 32121, 763, 2251, 15535, 512, 22818, 279, 2385, 6683, 323, 26619, 23669, 5110, 320, 4291, 25464, 4026, 11, 1491, 9988, 12050, 11, 81294, 291, 5044, 11, 323, 7319, 2410, 13693, 705, 499, 649, 1755, 1633, 1579, 45478, 64862, 13, 4452, 11, 4839, 5219, 6904, 389, 279, 3230, 1646, 323, 54696, 13, 1789, 3187, 11, 449, 264, 1664, 12, 99968, 45478, 15660, 11, 499, 2643, 11322, 11758, 311, 9214, 315, 304, 5006, 824, 2132, 389, 1778, 264, 1579, 13368, 6642, 382, 14711, 89520, 512, 12, 5560, 25464, 4026, 422, 3284, 311, 35608, 10758, 4732, 1990, 71503, 627, 12, 21829, 1701, 279, 1491, 9988, 2167, 23501, 393, 17, 47, 12050, 311, 18885, 10758, 15374, 627, 12, 6193, 21321, 287, 5044, 649, 3493, 264, 8275, 7916, 719, 8891, 369, 20334, 4819, 627, 12, 28295, 2410, 13693, 311, 8335, 5178, 323, 2410, 15652, 627, 12, 1442, 279, 4200, 2065, 32115, 374, 5199, 11, 2980, 4401, 389, 12743, 9501, 369, 7340, 5178, 382, 644, 12399, 11, 279, 6683, 499, 7633, 706, 279, 4754, 369, 1633, 1579, 45478, 25753, 11, 5423, 449, 279, 1314, 82278, 13, 27866, 2204, 33483, 323, 16967, 1972, 31184, 5178, 690, 3041, 499, 279, 1455, 13687, 26793, 13, 128001, 128011, 40, 1120, 743, 499, 709, 389, 430, 6642, 11, 719, 449, 328, 3910, 526, 358, 1436, 1193, 1005, 220, 19, 87, 15500, 15, 82, 627, 2355, 2181, 27096, 856, 9825, 2754, 505, 220, 717, 13, 1484, 311, 220, 1927, 30694, 2754, 0, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:49:34 TP0] Prefill batch. #new-seq: 1, #new-token: 1792, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:49:38 TP0] Decode batch. #running-req: 1, #token: 1826, token usage: 0.02, cuda graph: True, gen throughput (token/s): 0.04, #queue-req: 0, 
[2025-10-13 08:49:40 TP0] Decode batch. #running-req: 1, #token: 1866, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.11, #queue-req: 0, 
[2025-10-13 08:49:41 TP0] Decode batch. #running-req: 1, #token: 1906, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.26, #queue-req: 0, 
[2025-10-13 08:49:43 TP0] Decode batch. #running-req: 1, #token: 1946, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.15, #queue-req: 0, 
[2025-10-13 08:49:44 TP0] Decode batch. #running-req: 1, #token: 1986, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.15, #queue-req: 0, 
[2025-10-13 08:49:46 TP0] Decode batch. #running-req: 1, #token: 2026, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.11, #queue-req: 0, 
[2025-10-13 08:49:47 TP0] Decode batch. #running-req: 1, #token: 2066, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.21, #queue-req: 0, 
[2025-10-13 08:49:49 TP0] Decode batch. #running-req: 1, #token: 2106, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.16, #queue-req: 0, 
[2025-10-13 08:49:50 TP0] Decode batch. #running-req: 1, #token: 2146, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.06, #queue-req: 0, 
[2025-10-13 08:49:52 TP0] Decode batch. #running-req: 1, #token: 2186, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.04, #queue-req: 0, 
[2025-10-13 08:49:53 TP0] Decode batch. #running-req: 1, #token: 2226, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.03, #queue-req: 0, 
[2025-10-13 08:49:55 TP0] Decode batch. #running-req: 1, #token: 2266, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.05, #queue-req: 0, 
[2025-10-13 08:49:56 TP0] Decode batch. #running-req: 1, #token: 2306, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.01, #queue-req: 0, 
[2025-10-13 08:49:58 TP0] Decode batch. #running-req: 1, #token: 2346, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.96, #queue-req: 0, 
[2025-10-13 08:49:59 TP0] Decode batch. #running-req: 1, #token: 2386, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.00, #queue-req: 0, 
[2025-10-13 08:50:01 TP0] Decode batch. #running-req: 1, #token: 2426, token usage: 0.02, cuda graph: True, gen throughput (token/s): 27.06, #queue-req: 0, 
[2025-10-13 08:50:02 TP0] Decode batch. #running-req: 1, #token: 2466, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.91, #queue-req: 0, 
[2025-10-13 08:50:04 TP0] Decode batch. #running-req: 1, #token: 2506, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.96, #queue-req: 0, 
[2025-10-13 08:50:05 TP0] Decode batch. #running-req: 1, #token: 2546, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.94, #queue-req: 0, 
[2025-10-13 08:50:07 TP0] Decode batch. #running-req: 1, #token: 2586, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.79, #queue-req: 0, 
[2025-10-13 08:50:08 TP0] Decode batch. #running-req: 1, #token: 2626, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.87, #queue-req: 0, 
[2025-10-13 08:50:10 TP0] Decode batch. #running-req: 1, #token: 2666, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.77, #queue-req: 0, 
[2025-10-13 08:50:11 TP0] Decode batch. #running-req: 1, #token: 2706, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.83, #queue-req: 0, 
[2025-10-13 08:50:13 TP0] Decode batch. #running-req: 1, #token: 2746, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.85, #queue-req: 0, 
[2025-10-13 08:50:14 TP0] Decode batch. #running-req: 1, #token: 2786, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.81, #queue-req: 0, 
[2025-10-13 08:50:16 TP0] Decode batch. #running-req: 1, #token: 2826, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.83, #queue-req: 0, 
[2025-10-13 08:50:17 TP0] Decode batch. #running-req: 1, #token: 2866, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.81, #queue-req: 0, 
[2025-10-13 08:50:19 TP0] Decode batch. #running-req: 1, #token: 2906, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.79, #queue-req: 0, 
[2025-10-13 08:50:20 TP0] Decode batch. #running-req: 1, #token: 2946, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.76, #queue-req: 0, 
[2025-10-13 08:50:21 TP0] Decode batch. #running-req: 1, #token: 2986, token usage: 0.03, cuda graph: True, gen throughput (token/s): 26.75, #queue-req: 0, 
[2025-10-13 08:50:23 TP0] Decode batch. #running-req: 1, #token: 3026, token usage: 0.03, cuda graph: True, gen throughput (token/s): 26.82, #queue-req: 0, 
[2025-10-13 08:50:24 TP0] Decode batch. #running-req: 1, #token: 3066, token usage: 0.03, cuda graph: True, gen throughput (token/s): 26.72, #queue-req: 0, 
[2025-10-13 08:50:26 TP0] Decode batch. #running-req: 1, #token: 3106, token usage: 0.03, cuda graph: True, gen throughput (token/s): 26.78, #queue-req: 0, 
[2025-10-13 08:50:27 TP0] Decode batch. #running-req: 1, #token: 3146, token usage: 0.03, cuda graph: True, gen throughput (token/s): 26.69, #queue-req: 0, 
[2025-10-13 08:50:29 TP0] Decode batch. #running-req: 1, #token: 3186, token usage: 0.03, cuda graph: True, gen throughput (token/s): 26.79, #queue-req: 0, 
[2025-10-13 08:50:29] Finish: obj=GenerateReqInput(rid='49ae469256d648c9bc971387d5fd23a1', text=None, input_ids=[128000, 1502, 9805, 1473, 128011, 19182, 1070, 24385, 41460, 15110, 4912, 2650, 13835, 656, 258, 30, 128012, 9906, 0, 358, 2846, 31301, 439, 10825, 11, 9901, 499, 13, 2650, 922, 499, 30, 2650, 596, 701, 1938, 2133, 30, 128001, 128011, 40, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 30, 128012, 40914, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 13, 128001, 128011, 3923, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701, 12934, 5571, 62179, 64078, 14, 13506, 13, 4370, 13, 868, 2320, 17, 79, 198, 1959, 21321, 1871, 5380, 15335, 13693, 5380, 33, 548, 9501, 4619, 315, 19844, 71730, 18270, 30, 128012, 4599, 38663, 279, 45478, 4732, 369, 264, 6683, 16239, 5361, 1579, 58574, 71503, 11, 264, 8147, 14266, 11, 323, 3544, 15055, 315, 22813, 11, 1070, 527, 3892, 9547, 311, 2980, 13, 6914, 596, 1464, 1523, 279, 3685, 5178, 323, 279, 5536, 315, 5370, 36770, 1473, 14711, 5464, 12499, 512, 12, 3146, 21, 87, 34661, 10860, 55, 220, 15500, 15, 71503, 96618, 4314, 527, 8147, 11761, 41327, 71503, 449, 220, 1187, 5494, 315, 19718, 1428, 1855, 11, 13171, 315, 11850, 26192, 15592, 990, 33785, 627, 12, 3146, 58805, 19613, 93944, 220, 5958, 37, 18, 14266, 96618, 362, 220, 845, 24357, 11, 220, 843, 61904, 3622, 41327, 14266, 449, 264, 2385, 9042, 4732, 315, 220, 18, 13, 22, 52405, 323, 264, 7916, 9042, 4732, 315, 220, 19, 13, 16, 52405, 13, 1102, 596, 1664, 87229, 1639, 369, 18646, 5361, 71503, 323, 11850, 14266, 50962, 9256, 627, 12, 3146, 8358, 5494, 44860, 19, 22813, 520, 220, 9588, 15, 37594, 96618, 69489, 315, 5044, 311, 6106, 430, 701, 990, 33785, 527, 539, 5044, 50962, 627, 12, 3146, 1600, 71730, 220, 23, 13, 15, 13, 18, 84, 38, 323, 36060, 220, 1187, 13, 2371, 80128, 18270, 96618, 29125, 279, 54696, 304, 264, 4200, 1534, 4676, 11621, 1063, 32115, 11, 719, 6617, 9950, 651, 42314, 1093, 19844, 71730, 527, 5115, 11297, 382, 14711, 56360, 323, 15248, 408, 512, 12, 3146, 8750, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 96618, 1115, 374, 4461, 34440, 369, 5178, 389, 6617, 23501, 12035, 627, 12, 3146, 5109, 30531, 2005, 46745, 96618, 46402, 11, 279, 46745, 690, 617, 264, 17832, 5536, 389, 45478, 4732, 7389, 433, 9221, 264, 88938, 304, 11850, 1217, 7540, 382, 14711, 21304, 21829, 811, 512, 16, 13, 3146, 2409, 14, 27131, 25464, 4026, 334, 512, 256, 482, 3146, 2409, 25464, 4026, 96618, 25464, 4026, 6276, 2167, 1579, 68775, 3175, 10758, 1990, 71503, 11, 18189, 40370, 323, 18899, 15374, 13, 1115, 649, 387, 24629, 369, 4332, 4967, 323, 45478, 11, 1405, 828, 3966, 311, 387, 6222, 6288, 1990, 71503, 627, 256, 482, 3146, 27131, 25464, 4026, 96618, 432, 989, 287, 389, 91790, 34125, 369, 10758, 1990, 71503, 11, 902, 1253, 19678, 1063, 40370, 323, 8108, 8244, 64862, 382, 17, 13, 3146, 2409, 14, 27131, 5768, 9988, 7286, 50991, 393, 17, 47, 46837, 334, 512, 256, 482, 3146, 2409, 5768, 9988, 46837, 96618, 12362, 279, 2167, 23501, 393, 17, 47, 12050, 505, 279, 3984, 2723, 649, 12207, 7417, 14734, 4791, 79697, 10758, 1990, 71503, 11, 18189, 32115, 323, 40370, 627, 256, 482, 3146, 27131, 5768, 9988, 46837, 96618, 1472, 1053, 17631, 389, 5410, 12050, 11, 902, 2643, 539, 387, 439, 34440, 369, 23501, 4791, 12279, 6459, 10758, 382, 18, 13, 3146, 1959, 21321, 14171, 334, 512, 256, 482, 3146, 1959, 21321, 291, 14171, 96618, 6193, 21321, 287, 23501, 5044, 649, 3493, 264, 8275, 5178, 7916, 11, 8104, 422, 701, 54696, 374, 5044, 50962, 13, 4452, 11, 279, 20192, 2643, 387, 32873, 323, 2586, 520, 279, 2853, 315, 5190, 2410, 15652, 323, 4754, 20334, 4819, 382, 19, 13, 3146, 15335, 72955, 334, 512, 256, 482, 3146, 99341, 7572, 72955, 96618, 74540, 279, 2410, 4017, 320, 51, 10510, 8, 649, 2187, 279, 71503, 311, 1629, 520, 5190, 34873, 369, 5129, 18852, 11, 13893, 18899, 5178, 13, 4452, 11, 420, 690, 5376, 2410, 15652, 323, 8798, 14091, 49686, 627, 256, 482, 3146, 3760, 7572, 72955, 96618, 29125, 520, 1670, 2410, 13693, 690, 6106, 20334, 323, 58219, 719, 1253, 4017, 279, 7340, 5178, 382, 20, 13, 3146, 33, 548, 19757, 6296, 13, 19844, 71730, 18270, 334, 512, 256, 482, 3146, 33, 548, 19757, 96618, 29125, 6089, 389, 12743, 9501, 690, 22472, 279, 4200, 2065, 32115, 11, 13893, 6522, 311, 2731, 5178, 627, 256, 482, 3146, 1600, 71730, 18270, 96618, 29125, 304, 264, 18270, 11621, 1063, 32115, 719, 6209, 7720, 1093, 31398, 11, 8831, 6373, 11, 323, 25152, 13, 18766, 9950, 651, 42314, 527, 5115, 11297, 11, 779, 279, 5178, 4295, 2643, 539, 387, 439, 5199, 439, 304, 279, 3347, 382, 14711, 32121, 763, 2251, 15535, 512, 22818, 279, 2385, 6683, 323, 26619, 23669, 5110, 320, 4291, 25464, 4026, 11, 1491, 9988, 12050, 11, 81294, 291, 5044, 11, 323, 7319, 2410, 13693, 705, 499, 649, 1755, 1633, 1579, 45478, 64862, 13, 4452, 11, 4839, 5219, 6904, 389, 279, 3230, 1646, 323, 54696, 13, 1789, 3187, 11, 449, 264, 1664, 12, 99968, 45478, 15660, 11, 499, 2643, 11322, 11758, 311, 9214, 315, 304, 5006, 824, 2132, 389, 1778, 264, 1579, 13368, 6642, 382, 14711, 89520, 512, 12, 5560, 25464, 4026, 422, 3284, 311, 35608, 10758, 4732, 1990, 71503, 627, 12, 21829, 1701, 279, 1491, 9988, 2167, 23501, 393, 17, 47, 12050, 311, 18885, 10758, 15374, 627, 12, 6193, 21321, 287, 5044, 649, 3493, 264, 8275, 7916, 719, 8891, 369, 20334, 4819, 627, 12, 28295, 2410, 13693, 311, 8335, 5178, 323, 2410, 15652, 627, 12, 1442, 279, 4200, 2065, 32115, 374, 5199, 11, 2980, 4401, 389, 12743, 9501, 369, 7340, 5178, 382, 644, 12399, 11, 279, 6683, 499, 7633, 706, 279, 4754, 369, 1633, 1579, 45478, 25753, 11, 5423, 449, 279, 1314, 82278, 13, 27866, 2204, 33483, 323, 16967, 1972, 31184, 5178, 690, 3041, 499, 279, 1455, 13687, 26793, 13, 128001, 128011, 40, 1120, 743, 499, 709, 389, 430, 6642, 11, 719, 449, 328, 3910, 526, 358, 1436, 1193, 1005, 220, 19, 87, 15500, 15, 82, 627, 2355, 2181, 27096, 856, 9825, 2754, 505, 220, 717, 13, 1484, 311, 220, 1927, 30694, 2754, 0, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "<think>\nAlright, so I've got this setup with 6x NVIDIA RTX 3090s, an AMD EPYC 73F3 CPU, 512GB of DDR4-3200 RAM, running ESXi 8.0.3uG with an Ubuntu 24.04 LTS VM, using TabbyAPI/ExLLAMAv3 as the backend and Open-webui as the frontend. I'm trying to optimize inference speed here.\n\nFirst, I noticed that when I tried using SGLang, I could only utilize 4 out of 6 GPUs. Still, that boosted my token generation speed from 12.99 tokens per second to 36 tokens per second. That's a significant increase, but ideally, I want to harness all 6 GPUs. It seems like SGLang might not be fully compatible or optimized for all 6 GPUs yet.\n\nNow, considering the hardware setup:\n\n1. **6x RTX 3090 GPUs**: These are powerful, but I wonder if the system is maximizing their potential. Maybe there's a bottleneck somewhere preventing all 6 from being utilized effectively.\n\n2. **EPYC 73F3 CPU**: With 16 cores and 32 threads, this CPU should handle multi-threaded workloads efficiently. But I need to ensure that the CPU isn't becoming a bottl" ... 'ersus through a VM to determine if the removal of virtualization overhead yields significant benefits.\n\n6. **System Monitoring and Maintenance**:\n   - **Benchmark and Monitor**: Continuously benchmark each optimization step and monitor system temperatures, stability, and power consumption.\n   - **Maintain System Balance**: Ensure that all components are working in harmony, avoiding overclocking one part without considering others to prevent suboptimal performance or instability.\n\n7. **Community and Documentation**:\n   - **Consult Resources**: Regularly check community forums, official documentation, and best practices for updates and solutions to common issues.\n\n8. **Future-Proofing**:\n   - **Stay Updated**: Keep abreast of the latest advancements in GPU technology and optimizations to continually improve system performance and efficiency.\n\nBy following this plan, you can systematically enhance the performance of your setup, ensuring all 6 GPUs are fully utilized and achieving optimal token generation speeds.', 'output_ids': [128001], 'meta_info': {'id': '49ae469256d648c9bc971387d5fd23a1', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 1793, 'weight_version': 'default', 'completion_tokens': 1395, 'cached_tokens': 1, 'e2e_latency': 54.68422722816467}}
[2025-10-13 08:50:29] Receive: obj=GenerateReqInput(rid='82d628107b854c41a5065f0264ea8a79', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 358, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 5380, 5045, 3931, 2891, 25, 23371, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 627, 6584, 25, 3639, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701] ... [71503, 627, 256, 482, 3146, 27131, 25464, 4026, 96618, 432, 989, 287, 389, 91790, 34125, 369, 10758, 1990, 71503, 11, 902, 1253, 19678, 1063, 40370, 323, 8108, 8244, 64862, 382, 17, 13, 3146, 2409, 14, 27131, 5768, 9988, 7286, 50991, 393, 17, 47, 46837, 334, 512, 256, 482, 3146, 2409, 5768, 9988, 46837, 96618, 12362, 279, 2167, 23501, 393, 17, 47, 12050, 505, 279, 3984, 2723, 649, 12207, 7417, 14734, 4791, 79697, 10758, 1990, 71503, 11, 18189, 32115, 323, 40370, 627, 256, 482, 3146, 27131, 5768, 9988, 46837, 96618, 1472, 1053, 17631, 389, 5410, 12050, 11, 902, 2643, 539, 387, 439, 34440, 369, 23501, 4791, 12279, 6459, 10758, 382, 18, 13, 3146, 1959, 21321, 14171, 334, 512, 256, 482, 3146, 1959, 21321, 291, 14171, 96618, 6193, 21321, 287, 23501, 5044, 649, 3493, 264, 8275, 5178, 7916, 11, 8104, 422, 701, 54696, 374, 5044, 50962, 13, 4452, 11, 279, 20192, 2643, 387, 32873, 323, 2586, 520, 279, 2853, 315, 5190, 2410, 15652, 323, 4754, 20334, 4819, 382, 19, 13, 3146, 15335, 72955, 334, 512, 256, 482, 3146, 99341, 7572, 72955, 96618, 74540, 279, 2410, 4017, 320, 51, 10510, 8, 649, 2187, 279, 71503, 311, 1629, 520, 5190, 34873, 369, 5129, 18852, 11, 13893, 18899, 5178, 13, 4452, 11, 420, 690, 5376, 2410, 15652, 323, 8798, 14091, 49686, 627, 256, 482, 3146, 3760, 7572, 72955, 96618, 29125, 520, 1670, 2410, 13693, 690, 6106, 20334, 323, 58219, 719, 1253, 4017, 279, 7340, 5178, 382, 20, 13, 3146, 33, 548, 19757, 6296, 13, 19844, 71730, 18270, 334, 512, 256, 482, 3146, 33, 548, 19757, 96618, 29125, 6089, 389, 12743, 9501, 690, 22472, 279, 4200, 2065, 32115, 11, 13893, 6522, 311, 2731, 5178, 627, 256, 482, 3146, 1600, 71730, 18270, 96618, 29125, 304, 264, 18270, 11621, 1063, 32115, 719, 6209, 7720, 1093, 31398, 11, 8831, 6373, 11, 323, 25152, 13, 18766, 9950, 651, 42314, 527, 5115, 11297, 11, 779, 279, 5178, 4295, 2643, 539, 387, 439, 5199, 439, 304, 279, 3347, 382, 14711, 32121, 763, 2251, 15535, 512, 22818, 279, 2385, 6683, 323, 26619, 23669, 5110, 320, 4291, 25464, 4026, 11, 1491, 9988, 12050, 11, 81294, 291, 5044, 11, 323, 7319, 2410, 13693, 705, 499, 649, 1755, 1633, 1579, 45478, 64862, 13, 4452, 11, 4839, 5219, 6904, 389, 279, 3230, 1646, 323, 54696, 13, 1789, 3187, 11, 449, 264, 1664, 12, 99968, 45478, 15660, 11, 499, 2643, 11322, 11758, 311, 9214, 315, 304, 5006, 824, 2132, 389, 1778, 264, 1579, 13368, 6642, 382, 14711, 89520, 512, 12, 5560, 25464, 4026, 422, 3284, 311, 35608, 10758, 4732, 1990, 71503, 627, 12, 21829, 1701, 279, 1491, 9988, 2167, 23501, 393, 17, 47, 12050, 311, 18885, 10758, 15374, 627, 12, 6193, 21321, 287, 5044, 649, 3493, 264, 8275, 7916, 719, 8891, 369, 20334, 4819, 627, 12, 28295, 2410, 13693, 311, 8335, 5178, 323, 2410, 15652, 627, 12, 1442, 279, 4200, 2065, 32115, 374, 5199, 11, 2980, 4401, 389, 12743, 9501, 369, 7340, 5178, 382, 644, 12399, 11, 279, 6683, 499, 7633, 706, 279, 4754, 369, 1633, 1579, 45478, 25753, 11, 5423, 449, 279, 1314, 82278, 13, 27866, 2204, 33483, 323, 16967, 1972, 31184, 5178, 690, 3041, 499, 279, 1455, 13687, 26793, 627, 6584, 25, 358, 1120, 743, 499, 709, 389, 430, 6642, 11, 719, 449, 328, 3910, 526, 358, 1436, 1193, 1005, 220, 19, 87, 15500, 15, 82, 627, 2355, 2181, 27096, 856, 9825, 2754, 505, 220, 717, 13, 1484, 311, 220, 1927, 30694, 2754, 4999, 5045, 3931, 2891, 25, 2057, 30536, 279, 6642, 369, 7340, 5178, 11, 1618, 374, 264, 34030, 3197, 315, 1957, 1473, 16, 13, 3146, 50991, 10377, 2065, 334, 512, 256, 482, 3146, 34976, 65056, 328, 3910, 526, 29108, 811, 96618, 31001, 3249, 328, 3910, 526, 649, 1193, 29166, 220, 19, 704, 315, 220, 21, 71503, 13, 4343, 369, 25780, 4819, 477, 6683, 5110, 430, 2643, 387, 65951, 23501, 10648, 627, 256, 482, 3146, 4387, 4476, 35185, 96618, 30379, 682, 12050, 11, 30778, 11, 323, 3241, 527, 709, 4791, 18920, 13, 20556, 4029, 25907, 323, 4033, 9904, 369, 4754, 10105, 477, 990, 277, 3171, 382, 17, 13, 3146, 37426, 4026, 58839, 334, 512, 256, 482, 3146, 11331, 25464, 4026, 96618, 10377, 553, 25464, 4026, 311, 18885, 1579, 30699, 10758, 1990, 71503, 11, 18189, 828, 8481, 40370, 323, 18899, 8244, 5178, 382, 18, 13, 3146, 11603, 58839, 334, 512, 256, 482, 3146, 64080, 5768, 9988, 46837, 96618, 21829, 1701, 1491, 9988, 7286, 50991, 393, 17, 47, 12050, 311, 7417, 23501, 4791, 12279, 6459, 10758, 11, 13893, 47594, 5178, 382, 19, 13, 3146, 67962, 6193, 21321, 287, 334, 512, 256, 482, 3146, 1959, 21321, 14171, 96618, 45152, 279, 7720, 315, 81294, 287, 5044, 11, 1418, 16967, 369, 904, 8389, 25949, 389, 1887, 20334, 627, 256, 482, 3146, 39716, 7572, 72955, 96618, 50470, 2410, 13693, 311, 2187, 71503, 311, 14816, 520, 5190, 59539, 11, 44463, 5178, 20192, 2403, 7319, 2410, 15652, 323, 8798, 6373, 382, 20, 13, 3146, 34126, 2065, 6296, 13, 61892, 19757, 334, 512, 256, 482, 3146, 83445, 61892, 19757, 96618, 24702, 279, 5178, 315, 4401, 279, 6642, 389, 12743, 9501, 19579, 1555, 264, 18270, 311, 8417, 422, 279, 17065, 315, 4200, 2065, 32115, 36508, 5199, 7720, 382, 21, 13, 3146, 2374, 51803, 323, 39306, 334, 512, 256, 482, 3146, 86071, 323, 24423, 96618, 20915, 66735, 29531, 1855, 26329, 3094, 323, 8891, 1887, 20472, 11, 20334, 11, 323, 2410, 15652, 627, 256, 482, 3146, 67834, 467, 744, 31946, 96618, 30379, 430, 682, 6956, 527, 3318, 304, 26348, 11, 31526, 81294, 287, 832, 961, 2085, 13126, 3885, 311, 5471, 1207, 2985, 2931, 5178, 477, 56399, 382, 22, 13, 3146, 34868, 323, 45565, 334, 512, 256, 482, 3146, 60855, 16607, 96618, 29900, 398, 1817, 4029, 25907, 11, 4033, 9904, 11, 323, 1888, 12659, 369, 9013, 323, 10105, 311, 4279, 4819, 382, 23, 13, 3146, 25184, 12, 32176, 287, 334, 512, 256, 482, 3146, 39202, 16459, 96618, 13969, 67441, 561, 315, 279, 5652, 83787, 304, 23501, 5557, 323, 82278, 311, 35611, 7417, 1887, 5178, 323, 15374, 382, 1383, 2768, 420, 3197, 11, 499, 649, 60826, 18885, 279, 5178, 315, 701, 6642, 11, 23391, 682, 220, 21, 71503, 527, 7373, 34716, 323, 32145, 23669, 4037, 9659, 25753, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:50:29 TP0] Prefill batch. #new-seq: 1, #new-token: 2048, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:50:30 TP0] Prefill batch. #new-seq: 1, #new-token: 365, #cached-token: 0, token usage: 0.02, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:50:34 TP0] Decode batch. #running-req: 1, #token: 2452, token usage: 0.02, cuda graph: True, gen throughput (token/s): 7.46, #queue-req: 0, 
[2025-10-13 08:50:36 TP0] Decode batch. #running-req: 1, #token: 2492, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.87, #queue-req: 0, 
[2025-10-13 08:50:37 TP0] Decode batch. #running-req: 1, #token: 2532, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.75, #queue-req: 0, 
[2025-10-13 08:50:39 TP0] Decode batch. #running-req: 1, #token: 2572, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.95, #queue-req: 0, 
[2025-10-13 08:50:40 TP0] Decode batch. #running-req: 1, #token: 2612, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.73, #queue-req: 0, 
[2025-10-13 08:50:42 TP0] Decode batch. #running-req: 1, #token: 2652, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.85, #queue-req: 0, 
[2025-10-13 08:50:43 TP0] Decode batch. #running-req: 1, #token: 2692, token usage: 0.02, cuda graph: True, gen throughput (token/s): 26.76, #queue-req: 0, 
[2025-10-13 08:50:44] INFO:     127.0.0.1:34846 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 08:50:44] Receive: obj=GenerateReqInput(rid='3418f886a7534437a0bc252f55243603', text=None, input_ids=[128000, 1502, 9805, 1473, 128011, 12978, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 13, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:50:44 TP0] Prefill batch. #new-seq: 1, #new-token: 27, #cached-token: 5, token usage: 0.02, #running-req: 1, #queue-req: 0, 
[2025-10-13 08:50:45 TP0] Decode batch. #running-req: 2, #token: 2779, token usage: 0.02, cuda graph: True, gen throughput (token/s): 33.51, #queue-req: 0, 
[2025-10-13 08:50:46 TP0] Decode batch. #running-req: 2, #token: 2859, token usage: 0.02, cuda graph: True, gen throughput (token/s): 53.53, #queue-req: 0, 
[2025-10-13 08:50:48 TP0] Decode batch. #running-req: 2, #token: 2939, token usage: 0.02, cuda graph: True, gen throughput (token/s): 54.52, #queue-req: 0, 
[2025-10-13 08:50:49 TP0] Decode batch. #running-req: 2, #token: 3019, token usage: 0.03, cuda graph: True, gen throughput (token/s): 53.48, #queue-req: 0, 
[2025-10-13 08:50:50] Finish: obj=GenerateReqInput(rid='82d628107b854c41a5065f0264ea8a79', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 358, 4344, 4560, 311, 1518, 1268, 5043, 279, 4037, 9659, 374, 11, 649, 499, 10536, 704, 264, 220, 16, 2199, 2077, 555, 6140, 30, 4056, 1041, 15, 11460, 5380, 5045, 3931, 2891, 25, 23371, 11, 358, 4265, 387, 6380, 311, 1520, 449, 430, 13, 5810, 596, 264, 10015, 24131, 315, 279, 3925, 315, 21075, 11478, 11, 902, 1288, 387, 2212, 220, 1041, 15, 11460, 1473, 9470, 16895, 22107, 320, 15836, 8, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 13, 578, 4751, 330, 472, 16895, 11478, 1, 574, 1176, 78718, 555, 3842, 45040, 304, 220, 6280, 21, 11, 994, 568, 5762, 279, 1176, 14584, 10017, 389, 279, 3917, 520, 49197, 33780, 9304, 13, 578, 4216, 2919, 315, 15592, 1051, 10409, 449, 54508, 11, 449, 83407, 1093, 74626, 386, 52541, 52997, 430, 12933, 1053, 53120, 3823, 11478, 304, 1120, 264, 2478, 11026, 382, 11458, 11, 279, 2926, 54508, 574, 5246, 90761, 555, 279, 8903, 315, 279, 3465, 13, 578, 220, 4468, 15, 82, 323, 220, 3753, 15, 82, 5602, 264, 5199, 75017, 304, 15592, 3495, 11, 264, 4261, 3967, 439, 279, 330, 15836, 12688, 1210, 1115, 574, 4245, 311, 264, 10824, 315, 9547, 11, 2737, 279, 9669, 315, 4216, 19002, 323, 279, 17250, 315, 15840, 12933, 311, 3619, 5933, 4221, 382, 791, 220, 2550, 15, 82, 13160, 264, 13353, 1486, 369, 15592, 11, 449, 279, 11599, 315, 279, 7757, 323, 279, 7859, 18539, 315, 3544, 30525, 13, 1115, 6197, 311, 264, 6541, 304, 5357, 7119, 5780, 6975, 11, 264, 27084, 315, 15592, 430, 18065, 4967, 26249, 389, 828, 311, 1304, 20492, 477, 11429, 13, 3861, 315, 279, 1401, 42445, 82, 304, 5780, 6975, 574, 279, 4500, 315, 30828, 14488, 11, 902, 527, 62653, 1306, 279, 3823, 8271, 323, 649, 4048, 505, 828, 304, 264, 1648, 430, 8776, 26249, 4250, 382, 791, 220, 1049, 15, 82, 5602, 4726, 31003, 304, 15592, 11, 449, 279, 4500, 315, 5655, 6975, 12823, 430, 5535, 30828, 14488, 311, 387, 16572, 389, 1524, 8294, 30525, 13, 1115, 6197, 311, 5199, 18637, 304, 264, 7029, 2134, 315, 15592, 8522, 11, 505, 2217, 323, 8982, 18324, 311, 5933, 4221, 8863, 323, 74706, 382, 644, 3293, 1667, 11, 15592, 706, 3719, 15098, 18751, 1139, 1057, 7446, 6439, 11, 505, 4200, 57619, 1093, 76074, 323, 50712, 311, 659, 60811, 9515, 323, 28782, 26249, 389, 17265, 3600, 13, 4452, 11, 420, 18052, 706, 1101, 9408, 3062, 31308, 323, 59529, 4860, 11, 1778, 439, 279, 4754, 369, 15592, 311, 52875, 349, 6484, 93334, 477, 311, 387, 1511, 369, 39270, 10096, 382, 4054, 315, 279, 1401, 11774, 13176, 15592, 3432, 374, 279, 1205, 369, 10552, 481, 15592, 11, 902, 19813, 311, 279, 5845, 315, 15592, 6067, 311, 10552, 872, 11429, 304, 264, 1648, 430, 12966, 649, 3619, 13, 1115, 374, 8104, 3062, 304, 5789, 1093, 18985, 323, 9337, 12437, 11, 1405, 279, 45525, 527, 1579, 323, 279, 11429, 1903, 555, 15592, 6067, 649, 617, 5199, 16296, 382, 14364, 8815, 374, 279, 1205, 311, 6106, 430, 15592, 374, 8040, 323, 1511, 304, 264, 1648, 430, 374, 31308, 323, 8647, 13, 1115, 5764, 28118, 4819, 1093, 15837, 304, 15592, 26249, 11, 902, 649, 12446, 994, 279, 828, 1511, 311, 5542, 279, 26249, 374, 539, 18740, 315, 279, 7187, 439, 264, 4459, 13, 1102, 1101, 5764, 23391, 430, 15592, 374, 1511, 304, 264, 1648, 430, 43666, 12625, 323, 3823, 3268, 382, 20397, 1521, 11774, 11, 279, 3938, 315, 15592, 5992, 10107, 13, 91958, 304, 5789, 1093, 31228, 25213, 323, 21143, 442, 41969, 15009, 3412, 279, 11471, 315, 1524, 810, 8147, 15592, 6067, 11, 1418, 279, 7859, 18539, 315, 828, 323, 25213, 2410, 374, 3339, 15592, 810, 15987, 1109, 3596, 1603, 382, 644, 17102, 11, 15592, 706, 2586, 264, 1317, 1648, 2533, 1202, 54529, 304, 279, 5209, 12, 508, 339, 9478, 11, 323, 433, 9731, 311, 38680, 520, 264, 11295, 18338, 13, 6104, 1070, 527, 7995, 11774, 311, 387, 20669, 11, 279, 4754, 7720, 315, 15592, 527, 23205, 11, 323, 433, 374, 4461, 311, 1514, 459, 15098, 3062, 3560, 304, 1057, 6439, 304, 279, 1667, 311, 2586, 13, 1666, 584, 3136, 311, 2274, 323, 10739, 15592, 6067, 11, 433, 374, 7718, 430, 584, 656, 779, 304, 264, 1648, 430, 374, 31308, 11, 8647, 11, 323, 29408, 11, 779, 430, 279, 7720, 315, 15592, 649, 387, 6222, 555, 682, 627, 6584, 25, 3639, 1053, 499, 1755, 45478, 4732, 1053, 387, 389, 220, 21, 87, 15500, 15, 82, 11, 384, 3368, 66, 220, 5958, 69, 18, 11, 220, 8358, 9701, 294, 3696, 19, 12, 9588, 15, 11, 1560, 11927, 220, 23, 13, 15, 13, 18, 84, 38, 11, 4401, 459, 36060, 220, 1187, 13, 2371, 80128, 18270, 11, 15490, 1729, 7227, 14, 849, 4178, 1428, 10066, 18, 19713, 11, 5377, 30531, 2005, 46745, 627, 68037, 1053, 2997, 13, 3161, 14, 30096, 32540, 2125, 198, 4291, 14, 30096, 1491, 9988, 2167, 43694, 281, 17, 79, 12050, 3970, 1618, 25, 3788, 1129, 5316, 916, 73505, 6902, 38744, 2427, 5701] ... [71503, 627, 256, 482, 3146, 27131, 25464, 4026, 96618, 432, 989, 287, 389, 91790, 34125, 369, 10758, 1990, 71503, 11, 902, 1253, 19678, 1063, 40370, 323, 8108, 8244, 64862, 382, 17, 13, 3146, 2409, 14, 27131, 5768, 9988, 7286, 50991, 393, 17, 47, 46837, 334, 512, 256, 482, 3146, 2409, 5768, 9988, 46837, 96618, 12362, 279, 2167, 23501, 393, 17, 47, 12050, 505, 279, 3984, 2723, 649, 12207, 7417, 14734, 4791, 79697, 10758, 1990, 71503, 11, 18189, 32115, 323, 40370, 627, 256, 482, 3146, 27131, 5768, 9988, 46837, 96618, 1472, 1053, 17631, 389, 5410, 12050, 11, 902, 2643, 539, 387, 439, 34440, 369, 23501, 4791, 12279, 6459, 10758, 382, 18, 13, 3146, 1959, 21321, 14171, 334, 512, 256, 482, 3146, 1959, 21321, 291, 14171, 96618, 6193, 21321, 287, 23501, 5044, 649, 3493, 264, 8275, 5178, 7916, 11, 8104, 422, 701, 54696, 374, 5044, 50962, 13, 4452, 11, 279, 20192, 2643, 387, 32873, 323, 2586, 520, 279, 2853, 315, 5190, 2410, 15652, 323, 4754, 20334, 4819, 382, 19, 13, 3146, 15335, 72955, 334, 512, 256, 482, 3146, 99341, 7572, 72955, 96618, 74540, 279, 2410, 4017, 320, 51, 10510, 8, 649, 2187, 279, 71503, 311, 1629, 520, 5190, 34873, 369, 5129, 18852, 11, 13893, 18899, 5178, 13, 4452, 11, 420, 690, 5376, 2410, 15652, 323, 8798, 14091, 49686, 627, 256, 482, 3146, 3760, 7572, 72955, 96618, 29125, 520, 1670, 2410, 13693, 690, 6106, 20334, 323, 58219, 719, 1253, 4017, 279, 7340, 5178, 382, 20, 13, 3146, 33, 548, 19757, 6296, 13, 19844, 71730, 18270, 334, 512, 256, 482, 3146, 33, 548, 19757, 96618, 29125, 6089, 389, 12743, 9501, 690, 22472, 279, 4200, 2065, 32115, 11, 13893, 6522, 311, 2731, 5178, 627, 256, 482, 3146, 1600, 71730, 18270, 96618, 29125, 304, 264, 18270, 11621, 1063, 32115, 719, 6209, 7720, 1093, 31398, 11, 8831, 6373, 11, 323, 25152, 13, 18766, 9950, 651, 42314, 527, 5115, 11297, 11, 779, 279, 5178, 4295, 2643, 539, 387, 439, 5199, 439, 304, 279, 3347, 382, 14711, 32121, 763, 2251, 15535, 512, 22818, 279, 2385, 6683, 323, 26619, 23669, 5110, 320, 4291, 25464, 4026, 11, 1491, 9988, 12050, 11, 81294, 291, 5044, 11, 323, 7319, 2410, 13693, 705, 499, 649, 1755, 1633, 1579, 45478, 64862, 13, 4452, 11, 4839, 5219, 6904, 389, 279, 3230, 1646, 323, 54696, 13, 1789, 3187, 11, 449, 264, 1664, 12, 99968, 45478, 15660, 11, 499, 2643, 11322, 11758, 311, 9214, 315, 304, 5006, 824, 2132, 389, 1778, 264, 1579, 13368, 6642, 382, 14711, 89520, 512, 12, 5560, 25464, 4026, 422, 3284, 311, 35608, 10758, 4732, 1990, 71503, 627, 12, 21829, 1701, 279, 1491, 9988, 2167, 23501, 393, 17, 47, 12050, 311, 18885, 10758, 15374, 627, 12, 6193, 21321, 287, 5044, 649, 3493, 264, 8275, 7916, 719, 8891, 369, 20334, 4819, 627, 12, 28295, 2410, 13693, 311, 8335, 5178, 323, 2410, 15652, 627, 12, 1442, 279, 4200, 2065, 32115, 374, 5199, 11, 2980, 4401, 389, 12743, 9501, 369, 7340, 5178, 382, 644, 12399, 11, 279, 6683, 499, 7633, 706, 279, 4754, 369, 1633, 1579, 45478, 25753, 11, 5423, 449, 279, 1314, 82278, 13, 27866, 2204, 33483, 323, 16967, 1972, 31184, 5178, 690, 3041, 499, 279, 1455, 13687, 26793, 627, 6584, 25, 358, 1120, 743, 499, 709, 389, 430, 6642, 11, 719, 449, 328, 3910, 526, 358, 1436, 1193, 1005, 220, 19, 87, 15500, 15, 82, 627, 2355, 2181, 27096, 856, 9825, 2754, 505, 220, 717, 13, 1484, 311, 220, 1927, 30694, 2754, 4999, 5045, 3931, 2891, 25, 2057, 30536, 279, 6642, 369, 7340, 5178, 11, 1618, 374, 264, 34030, 3197, 315, 1957, 1473, 16, 13, 3146, 50991, 10377, 2065, 334, 512, 256, 482, 3146, 34976, 65056, 328, 3910, 526, 29108, 811, 96618, 31001, 3249, 328, 3910, 526, 649, 1193, 29166, 220, 19, 704, 315, 220, 21, 71503, 13, 4343, 369, 25780, 4819, 477, 6683, 5110, 430, 2643, 387, 65951, 23501, 10648, 627, 256, 482, 3146, 4387, 4476, 35185, 96618, 30379, 682, 12050, 11, 30778, 11, 323, 3241, 527, 709, 4791, 18920, 13, 20556, 4029, 25907, 323, 4033, 9904, 369, 4754, 10105, 477, 990, 277, 3171, 382, 17, 13, 3146, 37426, 4026, 58839, 334, 512, 256, 482, 3146, 11331, 25464, 4026, 96618, 10377, 553, 25464, 4026, 311, 18885, 1579, 30699, 10758, 1990, 71503, 11, 18189, 828, 8481, 40370, 323, 18899, 8244, 5178, 382, 18, 13, 3146, 11603, 58839, 334, 512, 256, 482, 3146, 64080, 5768, 9988, 46837, 96618, 21829, 1701, 1491, 9988, 7286, 50991, 393, 17, 47, 12050, 311, 7417, 23501, 4791, 12279, 6459, 10758, 11, 13893, 47594, 5178, 382, 19, 13, 3146, 67962, 6193, 21321, 287, 334, 512, 256, 482, 3146, 1959, 21321, 14171, 96618, 45152, 279, 7720, 315, 81294, 287, 5044, 11, 1418, 16967, 369, 904, 8389, 25949, 389, 1887, 20334, 627, 256, 482, 3146, 39716, 7572, 72955, 96618, 50470, 2410, 13693, 311, 2187, 71503, 311, 14816, 520, 5190, 59539, 11, 44463, 5178, 20192, 2403, 7319, 2410, 15652, 323, 8798, 6373, 382, 20, 13, 3146, 34126, 2065, 6296, 13, 61892, 19757, 334, 512, 256, 482, 3146, 83445, 61892, 19757, 96618, 24702, 279, 5178, 315, 4401, 279, 6642, 389, 12743, 9501, 19579, 1555, 264, 18270, 311, 8417, 422, 279, 17065, 315, 4200, 2065, 32115, 36508, 5199, 7720, 382, 21, 13, 3146, 2374, 51803, 323, 39306, 334, 512, 256, 482, 3146, 86071, 323, 24423, 96618, 20915, 66735, 29531, 1855, 26329, 3094, 323, 8891, 1887, 20472, 11, 20334, 11, 323, 2410, 15652, 627, 256, 482, 3146, 67834, 467, 744, 31946, 96618, 30379, 430, 682, 6956, 527, 3318, 304, 26348, 11, 31526, 81294, 287, 832, 961, 2085, 13126, 3885, 311, 5471, 1207, 2985, 2931, 5178, 477, 56399, 382, 22, 13, 3146, 34868, 323, 45565, 334, 512, 256, 482, 3146, 60855, 16607, 96618, 29900, 398, 1817, 4029, 25907, 11, 4033, 9904, 11, 323, 1888, 12659, 369, 9013, 323, 10105, 311, 4279, 4819, 382, 23, 13, 3146, 25184, 12, 32176, 287, 334, 512, 256, 482, 3146, 39202, 16459, 96618, 13969, 67441, 561, 315, 279, 5652, 83787, 304, 23501, 5557, 323, 82278, 311, 35611, 7417, 1887, 5178, 323, 15374, 382, 1383, 2768, 420, 3197, 11, 499, 649, 60826, 18885, 279, 5178, 315, 701, 6642, 11, 23391, 682, 220, 21, 71503, 527, 7373, 34716, 323, 32145, 23669, 4037, 9659, 25753, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': "<think>\nOkay, I need to come up with 3-5 follow-up questions that the user might naturally ask next, based on the chat history. The user is setting up a high-performance machine learning environment with 6x3090 GPUs and specific software configurations. They noticed that using SGLang increased their token speed from 12.99 to 36. So, the previous discussion centered around optimizing this setup, including hardware, drivers, virtualization, and software tweaks.\n\nLooking at the assistant's response, it outlined a plan of action including investigating SGLang limitations, optimizing GPU utilization, enabling NVLink, modded drivers, and considering bare metal setup. The user's last message indicated that SGLang could only use 4 out of 6 GPUs, which the assistant addressed by suggesting steps to utilize all 6 GPUs.\n\nNow, as a user, they might want more details on these optimizations. They might ask about specific steps to utilize all GPUs, the trade-offs of moving to bare metal, deeper dives into why SGLang is limi" ... 'Us.\n2. Inquire about the trade-offs between using bare metal versus the VM setup they\'re currently on.\n3. Request an explanation of why SGLang is limiting GPU usage and whether it\'s a common issue.\n4. Ask which specific modded drivers are recommended and how to install them.\n5. Maybe questions about specific benchmarks or expected performance increases after optimizations.\n\nI should frame these as straightforward, concise questions from the user\'s perspective, ensuring they are directly related to the discussed topics without repeating previous points.\n</think>\n\n{\n  "follow_ups": [\n    "What specific steps can I take to modify SGLang and utilize all 6 GPUs?",\n    "What are the trade-offs of switching from ESXi VM to a bare-metal setup?",\n    "Can you explain why SGLang is limiting me to 4 GPUs and how common this issue is?",\n    "Which modded drivers do you recommend for this setup and how should I install them?",\n    "What specific benchmarks should I expect after implementing all these optimizations?"\n  ]\n}', 'output_ids': [524, 9884, 20389, 29, 128012, 128013, 198, 33413, 11, 358, 1205, 311, 2586, 709, 449, 220, 18, 12, 20, 1833, 5352, 4860, 430, 279, 1217, 2643, 18182, 2610, 1828, 11, 3196, 389, 279, 6369, 3925, 13, 578, 1217, 374, 6376, 709, 264, 1579, 58574, 5780, 6975, 4676, 449, 220, 21, 87, 15500, 15, 71503, 323, 3230, 3241, 33483, 13, 2435, 14000, 430, 1701, 328, 3910, 526, 7319, 872, 4037, 4732, 505, 220, 717, 13, 1484, 311, 220, 1927, 13, 2100, 11, 279, 3766, 10430, 31288, 2212, 74142, 420, 6642, 11, 2737, 12035, 11, 12050, 11, 4200, 2065, 11, 323, 3241, 63437, 382, 23274, 520, 279, 18328, 596, 2077, 11, 433, 33740, 264, 3197, 315, 1957, 2737, 24834, 328, 3910, 526, 9669, 11, 74142, 23501, 50549, 11, 28462, 25464, 4026, 11, 1491, 9988, 12050, 11, 323, 13126, 12743, 9501, 6642, 13, 578, 1217, 596, 1566, 1984, 16717, 430, 328, 3910, 526, 1436, 1193, 1005, 220, 19, 704, 315, 220, 21, 71503, 11, 902, 279, 18328, 20669, 555, 23377, 7504, 311, 29166, 682, 220, 21, 71503, 382, 7184, 11, 439, 264, 1217, 11, 814, 2643, 1390, 810, 3649, 389, 1521, 82278, 13, 2435, 2643, 2610, 922, 3230, 7504, 311, 29166, 682, 71503, 11, 279, 6696, 65039, 315, 7366, 311, 12743, 9501, 11, 19662, 94521, 1139, 3249, 328, 3910, 526, 374, 33994, 23501, 10648, 11, 477, 902, 3230, 12050, 311, 1005, 382, 4516, 3284, 1833, 5352, 4860, 1436, 387, 1473, 16, 13, 21069, 369, 3230, 7504, 311, 5719, 328, 3910, 526, 311, 1005, 682, 220, 21, 71503, 627, 17, 13, 763, 999, 922, 279, 6696, 65039, 1990, 1701, 12743, 9501, 19579, 279, 18270, 6642, 814, 2351, 5131, 389, 627, 18, 13, 6274, 459, 16540, 315, 3249, 328, 3910, 526, 374, 33994, 23501, 10648, 323, 3508, 433, 596, 264, 4279, 4360, 627, 19, 13, 21069, 902, 3230, 1491, 9988, 12050, 527, 11349, 323, 1268, 311, 4685, 1124, 627, 20, 13, 10926, 4860, 922, 3230, 63119, 477, 3685, 5178, 12992, 1306, 82278, 382, 40, 1288, 4124, 1521, 439, 31439, 11, 64694, 4860, 505, 279, 1217, 596, 13356, 11, 23391, 814, 527, 6089, 5552, 311, 279, 14407, 13650, 2085, 40916, 3766, 3585, 627, 128014, 271, 517, 220, 330, 19070, 8401, 82, 794, 2330, 262, 330, 3923, 3230, 7504, 649, 358, 1935, 311, 5719, 328, 3910, 526, 323, 29166, 682, 220, 21, 71503, 36818, 262, 330, 3923, 527, 279, 6696, 65039, 315, 28865, 505, 19844, 71730, 18270, 311, 264, 12743, 89932, 6642, 36818, 262, 330, 6854, 499, 10552, 3249, 328, 3910, 526, 374, 33994, 757, 311, 220, 19, 71503, 323, 1268, 4279, 420, 4360, 374, 36818, 262, 330, 23956, 1491, 9988, 12050, 656, 499, 7079, 369, 420, 6642, 323, 1268, 1288, 358, 4685, 1124, 36818, 262, 330, 3923, 3230, 63119, 1288, 358, 1755, 1306, 25976, 682, 1521, 82278, 48469, 220, 5243, 92, 128001], 'meta_info': {'id': '82d628107b854c41a5065f0264ea8a79', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 2414, 'weight_version': 'default', 'completion_tokens': 461, 'cached_tokens': 1, 'e2e_latency': 21.144517183303833}}
[2025-10-13 08:50:50] INFO:     127.0.0.1:48052 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 08:50:51 TP0] Decode batch. #running-req: 1, #token: 208, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.47, #queue-req: 0, 
[2025-10-13 08:50:52 TP0] Decode batch. #running-req: 1, #token: 248, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.12, #queue-req: 0, 
[2025-10-13 08:50:54 TP0] Decode batch. #running-req: 1, #token: 288, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.29, #queue-req: 0, 
[2025-10-13 08:50:55 TP0] Decode batch. #running-req: 1, #token: 328, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.16, #queue-req: 0, 
[2025-10-13 08:50:57 TP0] Decode batch. #running-req: 1, #token: 368, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.24, #queue-req: 0, 
[2025-10-13 08:50:58 TP0] Decode batch. #running-req: 1, #token: 408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.06, #queue-req: 0, 
[2025-10-13 08:51:00 TP0] Decode batch. #running-req: 1, #token: 448, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.18, #queue-req: 0, 
[2025-10-13 08:51:01 TP0] Decode batch. #running-req: 1, #token: 488, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.18, #queue-req: 0, 
[2025-10-13 08:51:03 TP0] Decode batch. #running-req: 1, #token: 528, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.12, #queue-req: 0, 
[2025-10-13 08:51:04 TP0] Decode batch. #running-req: 1, #token: 568, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.17, #queue-req: 0, 
[2025-10-13 08:51:06 TP0] Decode batch. #running-req: 1, #token: 608, token usage: 0.01, cuda graph: True, gen throughput (token/s): 27.21, #queue-req: 0, 
[2025-10-13 08:51:07 TP0] Decode batch. #running-req: 1, #token: 648, token usage: 0.01, cuda graph: True, gen throughput (token/s): 27.01, #queue-req: 0, 
[2025-10-13 08:51:09 TP0] Decode batch. #running-req: 1, #token: 688, token usage: 0.01, cuda graph: True, gen throughput (token/s): 27.09, #queue-req: 0, 
[2025-10-13 08:51:10 TP0] Decode batch. #running-req: 1, #token: 728, token usage: 0.01, cuda graph: True, gen throughput (token/s): 27.06, #queue-req: 0, 
[2025-10-13 08:51:11 TP0] Decode batch. #running-req: 1, #token: 768, token usage: 0.01, cuda graph: True, gen throughput (token/s): 27.07, #queue-req: 0, 
[2025-10-13 08:51:12] Finish: obj=GenerateReqInput(rid='3418f886a7534437a0bc252f55243603', text=None, input_ids=[128000, 1502, 9805, 1473, 128011, 12978, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 13, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=True, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\nOkay, so I need to figure out which word fits in the blank: "The tourists got lost in the ______ of the city." The options I have are labyrinth, jungle, ocean, kitchen, and desert. Hmm, let\'s break this down step by step.\n\nFirst, I\'ll look at the sentence structure. It\'s saying the tourists got lost in the ______ of the city. So, the blank should be filled with a word that describes a part of the city where someone could easily get lost.\n\nLooking at the options, I\'ll evaluate each one to see which makes the most sense.\n\n1. **Labyrinth**: I know a labyrinth is a complex maze-like structure. If someone is in a labyrinth, it\'s easy to imagine getting lost because of the confusing paths and multiple turns. So, saying the tourists got lost in the labyrinth of the city makes sense if the city has a very confusing layout with narrow streets and many intersections.\n\n2. **Jungle**: A jungle is a dense area with lots of vegetation. While you can get lost in a jungle, the phrase "jungle of the city" is a bit con' ... '" could work metaphorically, but "labyrinth" is more precise in describing a place where navigation is difficult.\n\nIs there any chance the other options fit better? Let me think again. "Jungle" is sometimes used to describe a city metaphorically, like the concrete jungle, which means a busy and confusing city environment. So, could "jungle" be another option? It depends on whether "jungle of the city" is a common enough metaphor.\n\nWait, the phrase "concrete jungle" is a well-known metaphor, but the way the sentence is worded, "jungle of the city" sounds a bit awkward. Whereas "labyrinth of the city" flows naturally and is a direct metaphor for a maze-like structure, which fits well.\n\nSo, considering all that, the best fit is labyrinth. It directly implies a maze-like environment where getting lost is easy, fitting the context of tourists losing their way in a complex city layout.\n</think>\n\nThe correct word to fill in the blank is **labyrinth**. \n\n**Answer:** The tourists got lost in the labyrinth of the city.', 'output_ids': [128001], 'meta_info': {'id': '3418f886a7534437a0bc252f55243603', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 32, 'weight_version': 'default', 'completion_tokens': 740, 'cached_tokens': 5, 'e2e_latency': 27.488179445266724}}
[2025-10-13 08:51:12] Receive: obj=GenerateReqInput(rid='7aac1e410dcf4dbdb663f6562bdf0c4a', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 11736, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 627, 5045, 3931, 2891, 25, 578, 4495, 3492, 311, 5266, 304, 279, 10321, 374, 3146, 75, 62630, 334, 13, 4815, 334, 16533, 68063, 578, 32753, 2751, 5675, 304, 279, 99810, 315, 279, 3363, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:51:12 TP0] Prefill batch. #new-seq: 1, #new-token: 64, #cached-token: 200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:51:13 TP0] Decode batch. #running-req: 1, #token: 300, token usage: 0.00, cuda graph: True, gen throughput (token/s): 25.48, #queue-req: 0, 
[2025-10-13 08:51:15 TP0] Decode batch. #running-req: 1, #token: 340, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.11, #queue-req: 0, 
[2025-10-13 08:51:16 TP0] Decode batch. #running-req: 1, #token: 380, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.19, #queue-req: 0, 
[2025-10-13 08:51:17 TP0] Decode batch. #running-req: 1, #token: 420, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.07, #queue-req: 0, 
[2025-10-13 08:51:19 TP0] Decode batch. #running-req: 1, #token: 460, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.16, #queue-req: 0, 
[2025-10-13 08:51:20 TP0] Decode batch. #running-req: 1, #token: 500, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.12, #queue-req: 0, 
[2025-10-13 08:51:22 TP0] Decode batch. #running-req: 1, #token: 540, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.07, #queue-req: 0, 
[2025-10-13 08:51:23 TP0] Decode batch. #running-req: 1, #token: 580, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.19, #queue-req: 0, 
[2025-10-13 08:51:25 TP0] Decode batch. #running-req: 1, #token: 620, token usage: 0.01, cuda graph: True, gen throughput (token/s): 27.12, #queue-req: 0, 
[2025-10-13 08:51:25] Finish: obj=GenerateReqInput(rid='7aac1e410dcf4dbdb663f6562bdf0c4a', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 50, 3884, 220, 18, 12, 20, 9959, 1833, 5352, 4860, 477, 52032, 430, 279, 1217, 2643, 18182, 2610, 1828, 304, 420, 10652, 439, 264, 3146, 882, 98319, 3196, 389, 279, 6369, 3925, 11, 311, 1520, 3136, 477, 92792, 279, 10430, 627, 14711, 48528, 512, 12, 9842, 682, 1833, 5352, 4860, 505, 279, 1217, 753, 1486, 315, 1684, 11, 15910, 311, 279, 18328, 627, 12, 7557, 4860, 64694, 11, 2867, 11, 323, 6089, 5552, 311, 279, 14407, 8712, 1161, 4390, 12, 8442, 4284, 1833, 27859, 430, 1304, 5647, 2728, 279, 6369, 2262, 323, 656, 539, 13454, 1148, 574, 2736, 9960, 627, 12, 1442, 279, 10652, 374, 1633, 2875, 477, 539, 3230, 11, 4284, 810, 4689, 320, 8248, 9959, 8, 1833, 27859, 279, 1217, 2643, 2610, 627, 12, 5560, 279, 10652, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 6075, 2011, 387, 264, 4823, 1358, 315, 9246, 11, 912, 5066, 1495, 477, 37666, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 19070, 8401, 82, 794, 4482, 14924, 220, 16, 32111, 330, 14924, 220, 17, 32111, 330, 14924, 220, 18, 30, 1365, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 11736, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 627, 5045, 3931, 2891, 25, 578, 4495, 3492, 311, 5266, 304, 279, 10321, 374, 3146, 75, 62630, 334, 13, 4815, 334, 16533, 68063, 578, 32753, 2751, 5675, 304, 279, 99810, 315, 279, 3363, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\nAlright, let me try to figure out how to approach this. The user is asking for follow-up questions based on the provided chat history. I need to come up with 3-5 questions that the user might naturally ask next.\n\nLooking at the chat history, the user first asked for help studying vocabulary with a fill-in-the-blank sentence. The assistant provided a sentence using the word "labyrinth." So the initial interaction was about a specific vocabulary word.\n\nNow, to suggest follow-ups, I should think about what the user might want next. Maybe they want more examples to better understand the word\'s usage. They might not remember the exact meaning, so a question about that makes sense. Also, they could be interested in other similar words for expansion of their vocabulary. Maybe also how "labyrinth" is used in different contexts or whether it\'s used in common expressions.\n\nI should avoid repeating what\'s already been covered, so no more example sentences unless it\'s asking for additional ones. It\'s also important to keep the questions concise and directly related to the topic, which is vocabulary learning.\n\nSo, possible questions could be about definitions, synonyms, example sentences in different contexts, or usage in expressions. Each of these would help deepen the user\'s understanding beyond just one example.\n\nPutting it all together, the follow-up questions should cover these areas without being repetitive.\n</think>\n\n```json\n{\n  "follow_ups": [\n    "Can you provide another example sentence using a different vocabulary word?",\n    "What does the word \'labyrinth\' mean?",\n    "Could you explain how \'labyrinth\' is used in different contexts?",\n    "Are there any synonyms for \'labyrinth\' I should know?",\n    "Can you create another fill-in-the-blank sentence for me to try?"\n  ]\n}\n```', 'output_ids': [524, 9884, 20389, 29, 128012, 128013, 198, 72586, 11, 1095, 757, 1456, 311, 7216, 704, 1268, 311, 5603, 420, 13, 578, 1217, 374, 10371, 369, 1833, 5352, 4860, 3196, 389, 279, 3984, 6369, 3925, 13, 358, 1205, 311, 2586, 709, 449, 220, 18, 12, 20, 4860, 430, 279, 1217, 2643, 18182, 2610, 1828, 382, 23274, 520, 279, 6369, 3925, 11, 279, 1217, 1176, 4691, 369, 1520, 21630, 36018, 449, 264, 5266, 3502, 10826, 16677, 1201, 11914, 13, 578, 18328, 3984, 264, 11914, 1701, 279, 3492, 330, 75, 62630, 1210, 2100, 279, 2926, 16628, 574, 922, 264, 3230, 36018, 3492, 382, 7184, 11, 311, 4284, 1833, 27859, 11, 358, 1288, 1781, 922, 1148, 279, 1217, 2643, 1390, 1828, 13, 10926, 814, 1390, 810, 10507, 311, 2731, 3619, 279, 3492, 596, 10648, 13, 2435, 2643, 539, 6227, 279, 4839, 7438, 11, 779, 264, 3488, 922, 430, 3727, 5647, 13, 7429, 11, 814, 1436, 387, 8173, 304, 1023, 4528, 4339, 369, 14800, 315, 872, 36018, 13, 10926, 1101, 1268, 330, 75, 62630, 1, 374, 1511, 304, 2204, 38697, 477, 3508, 433, 596, 1511, 304, 4279, 24282, 382, 40, 1288, 5766, 40916, 1148, 596, 2736, 1027, 9960, 11, 779, 912, 810, 3187, 23719, 7389, 433, 596, 10371, 369, 5217, 6305, 13, 1102, 596, 1101, 3062, 311, 2567, 279, 4860, 64694, 323, 6089, 5552, 311, 279, 8712, 11, 902, 374, 36018, 6975, 382, 4516, 11, 3284, 4860, 1436, 387, 922, 17931, 11, 86506, 11, 3187, 23719, 304, 2204, 38697, 11, 477, 10648, 304, 24282, 13, 9062, 315, 1521, 1053, 1520, 92792, 279, 1217, 596, 8830, 7953, 1120, 832, 3187, 382, 99004, 433, 682, 3871, 11, 279, 1833, 5352, 4860, 1288, 3504, 1521, 5789, 2085, 1694, 59177, 627, 128014, 271, 74694, 2285, 198, 517, 220, 330, 19070, 8401, 82, 794, 2330, 262, 330, 6854, 499, 3493, 2500, 3187, 11914, 1701, 264, 2204, 36018, 3492, 36818, 262, 330, 3923, 1587, 279, 3492, 364, 75, 62630, 6, 3152, 36818, 262, 330, 13191, 499, 10552, 1268, 364, 75, 62630, 6, 374, 1511, 304, 2204, 38697, 36818, 262, 330, 11787, 1070, 904, 86506, 369, 364, 75, 62630, 6, 358, 1288, 1440, 36818, 262, 330, 6854, 499, 1893, 2500, 5266, 3502, 10826, 16677, 1201, 11914, 369, 757, 311, 1456, 48469, 220, 5243, 534, 74694, 128001], 'meta_info': {'id': '7aac1e410dcf4dbdb663f6562bdf0c4a', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 264, 'weight_version': 'default', 'completion_tokens': 369, 'cached_tokens': 200, 'e2e_latency': 13.668073654174805}}
[2025-10-13 08:51:25] INFO:     127.0.0.1:42330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 08:51:25] Receive: obj=GenerateReqInput(rid='88cb0053ca654e1e9a90745e1c7e65bd', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 32215, 264, 64694, 11, 220, 18, 12, 20, 3492, 2316, 449, 459, 43465, 29385, 4954, 279, 6369, 3925, 627, 14711, 48528, 512, 12, 578, 2316, 1288, 9539, 4097, 279, 1925, 7057, 477, 3917, 315, 279, 10652, 627, 12, 5560, 100166, 430, 18885, 8830, 315, 279, 8712, 11, 719, 5766, 55331, 15785, 477, 3361, 37666, 627, 12, 9842, 279, 2316, 304, 279, 6369, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 32499, 27406, 13708, 927, 27639, 28697, 26, 2567, 433, 2867, 323, 4382, 627, 12, 4718, 4553, 2077, 2011, 6824, 21742, 315, 279, 4823, 1665, 11, 2085, 904, 62469, 477, 72126, 1495, 627, 12, 578, 2612, 2011, 387, 264, 3254, 11, 7257, 4823, 1665, 11, 2085, 904, 51594, 2082, 70255, 477, 1023, 43669, 15853, 1495, 627, 12, 30379, 912, 7669, 1697, 1495, 11, 33449, 811, 11, 477, 41941, 16599, 68, 477, 1833, 279, 7257, 4823, 2612, 11, 439, 420, 690, 5353, 2167, 23115, 8060, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 2150, 794, 330, 22479, 64694, 2316, 1618, 1, 457, 14711, 26379, 512, 12, 314, 330, 2150, 794, 330, 9468, 241, 231, 12937, 8152, 50730, 1, 1173, 12, 314, 330, 2150, 794, 330, 9468, 235, 103, 24118, 39520, 32013, 26371, 1, 1173, 12, 314, 330, 2150, 794, 330, 35212, 3294, 315, 10948, 45910, 1, 1173, 12, 314, 330, 2150, 794, 330, 25732, 5664, 5761, 1968, 26788, 1, 1173, 12, 314, 330, 2150, 794, 330, 9470, 16895, 22107, 304, 39435, 1, 1173, 12, 314, 330, 2150, 794, 330, 9468, 236, 106, 8519, 4140, 11050, 73137, 1, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 11736, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 627, 5045, 3931, 2891, 25, 578, 4495, 3492, 311, 5266, 304, 279, 10321, 374, 3146, 75, 62630, 334, 13, 4815, 334, 16533, 68063, 578, 32753, 2751, 5675, 304, 279, 99810, 315, 279, 3363, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': 1000, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:51:25 TP0] Prefill batch. #new-seq: 1, #new-token: 335, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:51:27 TP0] Decode batch. #running-req: 1, #token: 367, token usage: 0.00, cuda graph: True, gen throughput (token/s): 20.12, #queue-req: 0, 
[2025-10-13 08:51:28 TP0] Decode batch. #running-req: 1, #token: 407, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.03, #queue-req: 0, 
[2025-10-13 08:51:30 TP0] Decode batch. #running-req: 1, #token: 447, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.34, #queue-req: 0, 
[2025-10-13 08:51:31 TP0] Decode batch. #running-req: 1, #token: 487, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.00, #queue-req: 0, 
[2025-10-13 08:51:33 TP0] Decode batch. #running-req: 1, #token: 527, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.24, #queue-req: 0, 
[2025-10-13 08:51:34 TP0] Decode batch. #running-req: 1, #token: 567, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.14, #queue-req: 0, 
[2025-10-13 08:51:35] Finish: obj=GenerateReqInput(rid='88cb0053ca654e1e9a90745e1c7e65bd', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 32215, 264, 64694, 11, 220, 18, 12, 20, 3492, 2316, 449, 459, 43465, 29385, 4954, 279, 6369, 3925, 627, 14711, 48528, 512, 12, 578, 2316, 1288, 9539, 4097, 279, 1925, 7057, 477, 3917, 315, 279, 10652, 627, 12, 5560, 100166, 430, 18885, 8830, 315, 279, 8712, 11, 719, 5766, 55331, 15785, 477, 3361, 37666, 627, 12, 9842, 279, 2316, 304, 279, 6369, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 627, 12, 32499, 27406, 13708, 927, 27639, 28697, 26, 2567, 433, 2867, 323, 4382, 627, 12, 4718, 4553, 2077, 2011, 6824, 21742, 315, 279, 4823, 1665, 11, 2085, 904, 62469, 477, 72126, 1495, 627, 12, 578, 2612, 2011, 387, 264, 3254, 11, 7257, 4823, 1665, 11, 2085, 904, 51594, 2082, 70255, 477, 1023, 43669, 15853, 1495, 627, 12, 30379, 912, 7669, 1697, 1495, 11, 33449, 811, 11, 477, 41941, 16599, 68, 477, 1833, 279, 7257, 4823, 2612, 11, 439, 420, 690, 5353, 2167, 23115, 8060, 627, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 2150, 794, 330, 22479, 64694, 2316, 1618, 1, 457, 14711, 26379, 512, 12, 314, 330, 2150, 794, 330, 9468, 241, 231, 12937, 8152, 50730, 1, 1173, 12, 314, 330, 2150, 794, 330, 9468, 235, 103, 24118, 39520, 32013, 26371, 1, 1173, 12, 314, 330, 2150, 794, 330, 35212, 3294, 315, 10948, 45910, 1, 1173, 12, 314, 330, 2150, 794, 330, 25732, 5664, 5761, 1968, 26788, 1, 1173, 12, 314, 330, 2150, 794, 330, 9470, 16895, 22107, 304, 39435, 1, 1173, 12, 314, 330, 2150, 794, 330, 9468, 236, 106, 8519, 4140, 11050, 73137, 1, 457, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 11736, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 627, 5045, 3931, 2891, 25, 578, 4495, 3492, 311, 5266, 304, 279, 10321, 374, 3146, 75, 62630, 334, 13, 4815, 334, 16533, 68063, 578, 32753, 2751, 5675, 304, 279, 99810, 315, 279, 3363, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': 1000, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\nAlright, let me break down how I approach creating the title for this chat history.\n\nFirst, I read through the conversation to understand the main theme. The user is asking for help with studying vocabulary by creating fill-in-the-blank sentences. The assistant responds with a sentence using the word "labyrinth." So, the focus is clearly on vocabulary learning, specifically using fill-in-the-blank exercises.\n\nNext, I consider concise titles. Possible options could be "Vocabulary Fill-in-the-blank" or "Learning New Words." However, the guidelines suggest using an emoji that enhances understanding. The emoji chosen needs to directly relate to the main theme. \n\nIn this case, the emoji "📚" represents books or studying, which fits perfectly with vocabulary learning. It\'s simple and widely recognized, making it effective without being too creative. \n\nPutting it all together, the title needs to be 3-5 words. "Vocabulary Fill-in-the-blank Practice" is concise, captures the essence, and the emoji reinforces the educational aspect. It\'s clear, accurate, and meets all the specified guidelines without any unnecessary embellishments.\n</think>\n\n{ "title": "📚 Vocabulary Fill-in-the-blank" }', 'output_ids': [524, 9884, 20389, 29, 128012, 128013, 198, 72586, 11, 1095, 757, 1464, 1523, 1268, 358, 5603, 6968, 279, 2316, 369, 420, 6369, 3925, 382, 5451, 11, 358, 1373, 1555, 279, 10652, 311, 3619, 279, 1925, 7057, 13, 578, 1217, 374, 10371, 369, 1520, 449, 21630, 36018, 555, 6968, 5266, 3502, 10826, 16677, 1201, 23719, 13, 578, 18328, 31680, 449, 264, 11914, 1701, 279, 3492, 330, 75, 62630, 1210, 2100, 11, 279, 5357, 374, 9539, 389, 36018, 6975, 11, 11951, 1701, 5266, 3502, 10826, 16677, 1201, 23783, 382, 5971, 11, 358, 2980, 64694, 15671, 13, 37207, 2671, 1436, 387, 330, 53, 44627, 22748, 3502, 10826, 16677, 1201, 1, 477, 330, 48567, 1561, 28730, 1210, 4452, 11, 279, 17959, 4284, 1701, 459, 43465, 430, 57924, 8830, 13, 578, 43465, 12146, 3966, 311, 6089, 29243, 311, 279, 1925, 7057, 13, 4815, 644, 420, 1162, 11, 279, 43465, 330, 9468, 241, 248, 1, 11105, 6603, 477, 21630, 11, 902, 18809, 14268, 449, 36018, 6975, 13, 1102, 596, 4382, 323, 13882, 15324, 11, 3339, 433, 7524, 2085, 1694, 2288, 11782, 13, 4815, 99004, 433, 682, 3871, 11, 279, 2316, 3966, 311, 387, 220, 18, 12, 20, 4339, 13, 330, 53, 44627, 22748, 3502, 10826, 16677, 1201, 28082, 1, 374, 64694, 11, 41255, 279, 28591, 11, 323, 279, 43465, 95593, 279, 16627, 13189, 13, 1102, 596, 2867, 11, 13687, 11, 323, 20628, 682, 279, 5300, 17959, 2085, 904, 26225, 72514, 100053, 627, 128014, 271, 90, 330, 2150, 794, 330, 9468, 241, 248, 99272, 22748, 3502, 10826, 16677, 1201, 1, 335, 128001], 'meta_info': {'id': '88cb0053ca654e1e9a90745e1c7e65bd', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 340, 'weight_version': 'default', 'completion_tokens': 250, 'cached_tokens': 5, 'e2e_latency': 9.691670179367065}}
[2025-10-13 08:51:35] INFO:     127.0.0.1:55146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2025-10-13 08:51:35] Receive: obj=GenerateReqInput(rid='590b0fd3c6ce48cc9a59a8287f25cbaa', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 32215, 220, 16, 12, 18, 7353, 9681, 22824, 4954, 279, 1925, 22100, 315, 279, 6369, 3925, 11, 3235, 449, 220, 16, 12, 18, 810, 3230, 1207, 16816, 9681, 382, 14711, 48528, 512, 12, 5256, 449, 1579, 11852, 31576, 320, 68, 1326, 13, 10170, 11, 12053, 11, 46969, 11, 17979, 11, 35979, 11, 8184, 11, 6401, 11, 13482, 11, 23334, 11, 11930, 340, 12, 21829, 2737, 9959, 1207, 9184, 38985, 60721, 422, 814, 527, 16917, 15609, 6957, 279, 10652, 198, 12, 1442, 2262, 374, 2288, 2875, 320, 1752, 1109, 220, 18, 6743, 8, 477, 2288, 17226, 11, 1005, 1193, 4482, 15777, 7171, 12, 5560, 279, 6369, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 198, 12, 32499, 27406, 13708, 927, 76041, 271, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 14412, 794, 4482, 4681, 16, 498, 330, 4681, 17, 498, 330, 4681, 18, 1365, 557, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 11736, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 627, 5045, 3931, 2891, 25, 578, 4495, 3492, 311, 5266, 304, 279, 10321, 374, 3146, 75, 62630, 334, 13, 4815, 334, 16533, 68063, 578, 32753, 2751, 5675, 304, 279, 99810, 315, 279, 3363, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False)
[2025-10-13 08:51:35 TP0] Prefill batch. #new-seq: 1, #new-token: 219, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-13 08:51:36 TP0] Decode batch. #running-req: 1, #token: 242, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.90, #queue-req: 0, 
[2025-10-13 08:51:37 TP0] Decode batch. #running-req: 1, #token: 282, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.35, #queue-req: 0, 
[2025-10-13 08:51:39 TP0] Decode batch. #running-req: 1, #token: 322, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.12, #queue-req: 0, 
[2025-10-13 08:51:40 TP0] Decode batch. #running-req: 1, #token: 362, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.06, #queue-req: 0, 
[2025-10-13 08:51:42 TP0] Decode batch. #running-req: 1, #token: 402, token usage: 0.00, cuda graph: True, gen throughput (token/s): 26.95, #queue-req: 0, 
[2025-10-13 08:51:43 TP0] Decode batch. #running-req: 1, #token: 442, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.07, #queue-req: 0, 
[2025-10-13 08:51:45] Finish: obj=GenerateReqInput(rid='590b0fd3c6ce48cc9a59a8287f25cbaa', text=None, input_ids=[128000, 128011, 14711, 5546, 512, 32215, 220, 16, 12, 18, 7353, 9681, 22824, 4954, 279, 1925, 22100, 315, 279, 6369, 3925, 11, 3235, 449, 220, 16, 12, 18, 810, 3230, 1207, 16816, 9681, 382, 14711, 48528, 512, 12, 5256, 449, 1579, 11852, 31576, 320, 68, 1326, 13, 10170, 11, 12053, 11, 46969, 11, 17979, 11, 35979, 11, 8184, 11, 6401, 11, 13482, 11, 23334, 11, 11930, 340, 12, 21829, 2737, 9959, 1207, 9184, 38985, 60721, 422, 814, 527, 16917, 15609, 6957, 279, 10652, 198, 12, 1442, 2262, 374, 2288, 2875, 320, 1752, 1109, 220, 18, 6743, 8, 477, 2288, 17226, 11, 1005, 1193, 4482, 15777, 7171, 12, 5560, 279, 6369, 596, 6156, 4221, 26, 1670, 311, 6498, 422, 2814, 50923, 198, 12, 32499, 27406, 13708, 927, 76041, 271, 14711, 9442, 512, 5483, 3645, 25, 314, 330, 14412, 794, 4482, 4681, 16, 498, 330, 4681, 17, 498, 330, 4681, 18, 1365, 557, 14711, 13149, 11346, 512, 27, 9884, 20389, 397, 6584, 25, 11736, 757, 4007, 36018, 25, 3350, 264, 11914, 369, 757, 311, 5266, 304, 279, 10321, 11, 323, 358, 3358, 1456, 311, 3820, 279, 4495, 3072, 627, 5045, 3931, 2891, 25, 578, 4495, 3492, 311, 5266, 304, 279, 10321, 374, 3146, 75, 62630, 334, 13, 4815, 334, 16533, 68063, 578, 32753, 2751, 5675, 304, 279, 99810, 315, 279, 3363, 627, 524, 9884, 20389, 29, 128012], input_embeds=None, image_data=None, video_data=None, audio_data=None, sampling_params={'temperature': 1.0, 'max_new_tokens': None, 'min_new_tokens': 0, 'stop': None, 'stop_token_ids': None, 'top_p': 1.0, 'top_k': 50, 'min_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0, 'repetition_penalty': 1.0, 'regex': None, 'ebnf': None, 'n': 1, 'no_stop_trim': False, 'ignore_eos': False, 'skip_special_tokens': True, 'logit_bias': None}, return_logprob=False, logprob_start_len=-1, top_logprobs_num=0, token_ids_logprob=None, return_text_in_logprobs=True, stream=False, log_metrics=True, return_hidden_states=False, modalities=[], session_params=None, lora_path=None, lora_id=None, custom_logit_processor=None, bootstrap_host=None, bootstrap_port=None, bootstrap_room=None, bootstrap_pair_key=None, data_parallel_rank=None, background=False, conversation_id=None, priority=None, extra_key=None, no_logs=False, custom_labels=None, return_bytes=False), out={'text': '<think>\nOkay, so the user has provided a chat history where they asked for help studying vocabulary. The assistant then gave a sentence with the word "labyrinth." I need to generate tags for this interaction. \n\nFirst, I\'ll look at the main themes. The user is specifically working on vocabulary, which falls under language learning. So, "Education" is a clear primary tag. There\'s nothing else indicating other domains like science or arts, so "Education" seems sufficient at the broad level.\n\nNow, considering subtopics, vocabulary building is a key aspect here. The user is using fill-in-the-blank exercises, which is a common method in language education. Therefore, "Language Learning" and "Vocabulary Building" are appropriate subtopic tags. \n\nI don\'t see enough content to add more tags, and the chat is pretty focused, so I\'ll stick with these. Also, since the language used is English, I don\'t need to adjust for multilingual considerations. \n\nPutting it all together, the main tag is "Education," and the subtopics are "Language Learning" and "Vocabulary Building."\n</think>\n\n```json\n{\n  "tags": ["Education", "Language Learning", "Vocabulary Building"]\n}\n```', 'output_ids': [524, 9884, 20389, 29, 128012, 128013, 198, 33413, 11, 779, 279, 1217, 706, 3984, 264, 6369, 3925, 1405, 814, 4691, 369, 1520, 21630, 36018, 13, 578, 18328, 1243, 6688, 264, 11914, 449, 279, 3492, 330, 75, 62630, 1210, 358, 1205, 311, 7068, 9681, 369, 420, 16628, 13, 4815, 5451, 11, 358, 3358, 1427, 520, 279, 1925, 22100, 13, 578, 1217, 374, 11951, 3318, 389, 36018, 11, 902, 17503, 1234, 4221, 6975, 13, 2100, 11, 330, 40234, 1, 374, 264, 2867, 6156, 4877, 13, 2684, 596, 4400, 775, 19392, 1023, 31576, 1093, 8198, 477, 19071, 11, 779, 330, 40234, 1, 5084, 14343, 520, 279, 7353, 2237, 382, 7184, 11, 13126, 1207, 56252, 11, 36018, 4857, 374, 264, 1401, 13189, 1618, 13, 578, 1217, 374, 1701, 5266, 3502, 10826, 16677, 1201, 23783, 11, 902, 374, 264, 4279, 1749, 304, 4221, 6873, 13, 15636, 11, 330, 14126, 21579, 1, 323, 330, 53, 44627, 17283, 1, 527, 8475, 1207, 16816, 9681, 13, 4815, 40, 1541, 956, 1518, 3403, 2262, 311, 923, 810, 9681, 11, 323, 279, 6369, 374, 5128, 10968, 11, 779, 358, 3358, 9396, 449, 1521, 13, 7429, 11, 2533, 279, 4221, 1511, 374, 6498, 11, 358, 1541, 956, 1205, 311, 7652, 369, 2814, 50923, 38864, 13, 4815, 99004, 433, 682, 3871, 11, 279, 1925, 4877, 374, 330, 40234, 1359, 323, 279, 1207, 56252, 527, 330, 14126, 21579, 1, 323, 330, 53, 44627, 17283, 10246, 128014, 271, 74694, 2285, 198, 517, 220, 330, 14412, 794, 4482, 40234, 498, 330, 14126, 21579, 498, 330, 53, 44627, 17283, 7171, 534, 74694, 128001], 'meta_info': {'id': '590b0fd3c6ce48cc9a59a8287f25cbaa', 'finish_reason': {'type': 'stop', 'matched': 128001}, 'prompt_tokens': 225, 'weight_version': 'default', 'completion_tokens': 252, 'cached_tokens': 6, 'e2e_latency': 9.61434006690979}}
[2025-10-13 08:51:45] INFO:     127.0.0.1:40244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
/home/ivan/sglang/start_sglang_deepseek_tp4_compiled.sh: line 23: 2570657 Killed                  /home/ivan/sglang/sglang-env/bin/python -m sglang.launch_server --model-path /home/ivan/models/DeepSeek-R1-Distill-Llama-70B-AWQ --host 0.0.0.0 --port 8001 --tp 4 --context-length 24576 --quantization awq_marlin --served-model-name deepseek-r1-distill-70b-awq --mem-fraction-static 0.80 --log-level info --log-requests --enable-torch-compile --sleep-on-idle
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
